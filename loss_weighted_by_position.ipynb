{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Loss_weighted_by_position.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "naPvFSlyIWkn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e8c203cd-9c93-43d7-f7ef-2b0e11c9bbf6"
      },
      "source": [
        "!pip install spektral -q\n",
        "!pip install keras-swa -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 102kB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 4.9MB/s \n",
            "\u001b[?25h  Building wheel for keras-swa (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Mxt5hdxmWF"
      },
      "source": [
        "from google.colab import drive\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06n0j5WDKkWV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6682de0b-2ceb-40b8-9a82-a6f4e8b22ce9"
      },
      "source": [
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks/OpenVaccine\"\n",
        "\n",
        "!cp '/content/drive/My Drive/Colab Notebooks/OpenVaccine/inputs.zip' \"./inputs.zip\"\n",
        "\n",
        "!ls\n",
        "\n",
        "!unzip -q inputs.zip\n",
        "\n",
        "!rm -r outputs\n",
        "!mkdir -p outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "'cat notebook.ipynb'\t\t\t\t    inputs.zip\n",
            "'Copy of cat_notebook best.ipynb'\t\t    oof.zip\n",
            "'Copy of Copy of cat_notebook best.ipynb'\t   'OpenVaccine GRU + LSTM'\n",
            "'Copy of Copy of Copy of cat_notebook best.ipynb'   outputs\n",
            "'Copy of wavenet-gru-baseline_0.2063.ipynb'\t    vaccine\n",
            " hidden_outputs\t\t\t\t\t    VAE.ipynb\n",
            " inputs\n",
            "drive  inputs.zip  sample_data\n",
            "rm: cannot remove 'outputs': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsZ6d8ACLVuf"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from spektral.layers import GraphConv, GraphConvSkip, GatedGraphConv, GraphAttention, ARMAConv,ChebConv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import tensorflow.keras.layers as L\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import plotly.express as px\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
        "from sklearn.cluster import KMeans\n",
        "import os\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_oMHqV0G5gX"
      },
      "source": [
        "train_json_path = \"inputs/train.json\"\n",
        "test_json_path = \"inputs/test.json\"\n",
        "sample_sub_path = \"inputs/sample_submission.csv\"\n",
        "aug_data_path = \"inputs/aug_data1.csv\"\n",
        "output_path = \"inputs/outputs/\"\n",
        "bpps_path = \"inputs/bpps\"\n",
        "\n",
        "train = pd.read_json(train_json_path, lines=True)\n",
        "test = pd.read_json(test_json_path, lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoWEdkHUIKax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "377d6eb8-304a-4ac9-bc67-e5672e3050c7"
      },
      "source": [
        "# additional features\n",
        "\n",
        "def read_bpps_sum(df):\n",
        "    bpps_arr = []\n",
        "    for mol_id in tqdm(df.id.to_list()):\n",
        "        bpps_arr.append(np.load(f\"{bpps_path}/{mol_id}.npy\").max(axis=1))\n",
        "    return bpps_arr\n",
        "\n",
        "def read_bpps_max(df):\n",
        "    bpps_arr = []\n",
        "    for mol_id in tqdm(df.id.to_list()):\n",
        "        bpps_arr.append(np.load(f\"{bpps_path}/{mol_id}.npy\").sum(axis=1))\n",
        "    return bpps_arr\n",
        "\n",
        "def read_bpps_nb(df):\n",
        "    # normalized non-zero number\n",
        "    # from https://www.kaggle.com/symyksr/openvaccine-deepergcn \n",
        "    print(df.shape)\n",
        "    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n",
        "    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n",
        "    bpps_arr = []\n",
        "    for mol_id in tqdm(df.id.to_list()):\n",
        "        bpps = np.load(f\"{bpps_path}/{mol_id}.npy\")\n",
        "        bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n",
        "        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n",
        "        bpps_arr.append(bpps_nb)\n",
        "    return bpps_arr \n",
        "\n",
        "train['bpps_sum'] = read_bpps_sum(train)\n",
        "test['bpps_sum'] = read_bpps_sum(test)\n",
        "train['bpps_max'] = read_bpps_max(train)\n",
        "test['bpps_max'] = read_bpps_max(test)\n",
        "train['bpps_nb'] = read_bpps_nb(train)\n",
        "test['bpps_nb'] = read_bpps_nb(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:02<00:00, 912.28it/s]\n",
            "100%|██████████| 3634/3634 [00:02<00:00, 1699.87it/s]\n",
            "100%|██████████| 2400/2400 [00:00<00:00, 3210.38it/s]\n",
            "100%|██████████| 3634/3634 [00:01<00:00, 3106.35it/s]\n",
            " 12%|█▏        | 283/2400 [00:00<00:00, 2826.03it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2400, 21)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:00<00:00, 2788.38it/s]\n",
            "  7%|▋         | 260/3634 [00:00<00:01, 2598.45it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(3634, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 3634/3634 [00:01<00:00, 2728.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e6q9BguIjXi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fbea62eb-5ca1-495f-cd6b-a9bc7e91c042"
      },
      "source": [
        "aug_df = pd.read_csv(aug_data_path)\n",
        "display(aug_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sequence</th>\n",
              "      <th>structure</th>\n",
              "      <th>log_gamma</th>\n",
              "      <th>score</th>\n",
              "      <th>cnt</th>\n",
              "      <th>predicted_loop_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id_fff546103</td>\n",
              "      <td>GGAAAGCUAGGACGUGGGAGCGUAGCUCUCCACACGGGUACGCCAA...</td>\n",
              "      <td>.....((((((((((((((((...)))).)))).((((((((((.....</td>\n",
              "      <td>2</td>\n",
              "      <td>0.981885</td>\n",
              "      <td>3</td>\n",
              "      <td>EEEEESSSSSSSSSSSSSSSSHHHSSSSBSSSSMSSSSSSSSSSHH...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id_18ff9d670</td>\n",
              "      <td>GGAAAGAGCUCGUGAGAAGAAUCUAGUACAUGCAUACGCUACAUCU...</td>\n",
              "      <td>.....(((.((.......((..((((.....((....)).....))...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.887485</td>\n",
              "      <td>5</td>\n",
              "      <td>EEEEESSSISSIIIIIIISSIISSSSIIIIISSHHHHSSIIIIISS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id_177cd630b</td>\n",
              "      <td>GGAAAGAAGUAGCACGGUCCUAAGGUUACUGUAGCUAUGUCCAGCG...</td>\n",
              "      <td>(....((.((((((((((((...))..))))).))))).))(((.(...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.923722</td>\n",
              "      <td>3</td>\n",
              "      <td>SMMMMSSISSSSSSSSSSSSHHHSSBBSSSSSBSSSSSISSSSSIS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id_17a9ad5b7</td>\n",
              "      <td>GGAAAACACUGCAAAAGUCAACGAAGAAGUUGACUAAGAAGUGAUC...</td>\n",
              "      <td>......((((((...(((((((......))))))).....((((((...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.977602</td>\n",
              "      <td>3</td>\n",
              "      <td>EEEEEESSSSSSMMMSSSSSSSHHHHHHSSSSSSSMMMMMSSSSSS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id_17ab91518</td>\n",
              "      <td>GGAAAACGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCG...</td>\n",
              "      <td>......(((((((((((((((((((((((((((((....)))))))...</td>\n",
              "      <td>2</td>\n",
              "      <td>0.982851</td>\n",
              "      <td>3</td>\n",
              "      <td>EEEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSHHHHSSSSSSS...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  ...                                predicted_loop_type\n",
              "0  id_fff546103  ...  EEEEESSSSSSSSSSSSSSSSHHHSSSSBSSSSMSSSSSSSSSSHH...\n",
              "1  id_18ff9d670  ...  EEEEESSSISSIIIIIIISSIISSSSIIIIISSHHHHSSIIIIISS...\n",
              "2  id_177cd630b  ...  SMMMMSSISSSSSSSSSSSSHHHSSBBSSSSSBSSSSSISSSSSIS...\n",
              "3  id_17a9ad5b7  ...  EEEEEESSSSSSMMMSSSSSSSHHHHHHSSSSSSSMMMMMSSSSSS...\n",
              "4  id_17ab91518  ...  EEEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSHHHHSSSSSSS...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Oa-RJdHUK8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ab0bda4-2742-43a7-bff2-798d264f68f1"
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "def allocate_gpu_memory(gpu_number=0):\n",
        "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "    if physical_devices:\n",
        "        try:\n",
        "            print(\"Found {} GPU(s)\".format(len(physical_devices)))\n",
        "            tf.config.set_visible_devices(physical_devices[gpu_number], 'GPU')\n",
        "            tf.config.experimental.set_memory_growth(physical_devices[gpu_number], True)\n",
        "            print(\"#{} GPU memory is allocated\".format(gpu_number))\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "    else:\n",
        "        print(\"Not enough GPU hardware devices available\")\n",
        "allocate_gpu_memory()\n",
        "\n",
        "Ver='GRU_LSTM1'\n",
        "debug = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1 GPU(s)\n",
            "#0 GPU memory is allocated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_UDNxb7MBzO"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead) \n",
        "    but it must be broadcastable for addition.\n",
        "  \n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable \n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "    Returns:\n",
        "     output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, \n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights\n",
        "    \n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'depth': self.depth,\n",
        "            'wq': self.wq,\n",
        "            'qk': self.wk,\n",
        "            'wv': self.wv,\n",
        "            'dense': self.dense,\n",
        "        })\n",
        "        \n",
        "        return config\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "      return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.rate = rate\n",
        "        \n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        #mask made None\n",
        "        attn_output, _ = self.mha(x, x, x, None)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'num_heads': self.num_heads,\n",
        "            'rate': self.rate,\n",
        "            'd_model': self.d_model,\n",
        "            'num_heads': self.num_heads,\n",
        "            'dropout1': self.dropout1,\n",
        "            'dropout2': self.dropout2,\n",
        "            'layernorm1': self.layernorm1,\n",
        "            'layernorm2': self.layernorm2,\n",
        "            'mha': self.mha,\n",
        "            'ffn': self.ffn,\n",
        "        })\n",
        "        return config\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtgfCfF3HZZk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "0d9b608a-c730-484c-b70b-602012a96005"
      },
      "source": [
        "token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n",
        "sequence_token2int = {x:i for i, x in enumerate('AGUC')}\n",
        "structure_token2int = {\n",
        "    '.': 0,\n",
        "    '(': 1,\n",
        "    ')': 2,\n",
        "}\n",
        "loop_token2int = {x:i for i, x in enumerate('SMIBHEX')}\n",
        "token2int_map = {\n",
        "    \"sequence\": sequence_token2int,\n",
        "    \"structure\": structure_token2int,\n",
        "    \"predicted_loop_type\": loop_token2int\n",
        "}\n",
        "\n",
        "\n",
        "pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
        "pred_cols_2 = ['reactivity_error', 'deg_error_Mg_pH10', 'deg_error_pH10',\n",
        "       'deg_error_Mg_50C', 'deg_error_50C']\n",
        "\n",
        "def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n",
        "    base_fea = np.transpose(\n",
        "        np.array(\n",
        "            df[cols]\n",
        "            .applymap(lambda seq: [token2int[x] for x in seq])\n",
        "            .values\n",
        "            .tolist()\n",
        "        ),\n",
        "        (0, 2, 1)\n",
        "    )\n",
        "\n",
        "    base_fea_ = np.transpose(\n",
        "        np.array([\n",
        "            df[col]\n",
        "            .apply(lambda seq: [token2int_map[col][x] for x in seq])\n",
        "            .values\n",
        "            .tolist()\n",
        "            for col in cols\n",
        "        ]),\n",
        "        (1, 2, 0)\n",
        "    )\n",
        "\n",
        "    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n",
        "    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n",
        "    # bpps_nb_fea = np.array(df['bpps_nb'].to_list())[:,:,np.newaxis]\n",
        "\n",
        "    # This kind of helps...\n",
        "    # _, position_fea = np.mgrid[0:bpps_nb_fea.shape[0]:1, 0:bpps_nb_fea.shape[1]:1]/(bpps_nb_fea.shape[1]-1)\n",
        "    position_fea = np.zeros((base_fea_.shape[0], base_fea_.shape[1], 1))\n",
        "    for seqpos in range(position_fea.shape[1]):\n",
        "      position_fea[:,seqpos,0] = 1 if seqpos <= 8 else 0\n",
        "    \n",
        "    ohe_1 = tf.keras.utils.to_categorical(base_fea_[:,:,0], 4)\n",
        "    ohe_2 = tf.keras.utils.to_categorical(base_fea_[:,:,1], 3)\n",
        "    ohe_3 = tf.keras.utils.to_categorical(base_fea_[:,:,2], 7)\n",
        "    \n",
        "    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea], 2), np.concatenate([ohe_1, ohe_2, ohe_3], axis=2)\n",
        "\n",
        "def rmse(y_actual, y_pred):\n",
        "    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n",
        "    return K.sqrt(mse)\n",
        "\n",
        "def mcrmse(y_actual, y_pred, num_scored=len(pred_cols)):\n",
        "    score = 0\n",
        "    for i in range(num_scored):\n",
        "        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) / num_scored\n",
        "    return score\n",
        "\n",
        "pos_mean_errs = [0.16569523, 0.31918474, 0.2423365 , 0.18850689, 0.15248323,\n",
        "       0.13064822, 0.109808  , 0.10030268, 0.10738042, 0.1086974 ,\n",
        "       0.11067812, 0.11281216, 0.13217267, 0.14510697, 0.14849753,\n",
        "       0.13327258, 0.1357438 , 0.13440289, 0.13023155, 0.13379187,\n",
        "       0.13629936, 0.12690499, 0.12328645, 0.13145057, 0.12742558,\n",
        "       0.11889836, 0.11708341, 0.12485095, 0.12485695, 0.12092339,\n",
        "       0.11301588, 0.10417391, 0.11182073, 0.12986295, 0.14624775,\n",
        "       0.1288749 , 0.13702195, 0.12557423, 0.11809574, 0.11681401,\n",
        "       0.10809299, 0.11329438, 0.11905934, 0.10801768, 0.12010753,\n",
        "       0.10573638, 0.09388344, 0.09732526, 0.09348288, 0.08953104,\n",
        "       0.08785295, 0.09074078, 0.11804569, 0.10242562, 0.09927281,\n",
        "       0.10599232, 0.09749185, 0.0864934 , 0.09399159, 0.08767603,\n",
        "       0.07716206, 0.07637734, 0.07398977, 0.08018929, 0.07567211,\n",
        "       0.08506333, 0.08731176, 0.07006043]\n",
        "\n",
        "pos_weights = np.array(pos_mean_errs) * 4\n",
        "print(pos_weights)\n",
        "\n",
        "def mcrmse_weighted_by_pos(y_actual, y_pred):\n",
        "  score = 0\n",
        "  pre_length = 5\n",
        "  for i in range(5):\n",
        "    prefix_s = tf.keras.losses.mean_squared_error(y_actual[:, :pre_length, i], y_pred[:, :pre_length, i]) * pre_length\n",
        "    body_s = tf.keras.losses.mean_squared_error(y_actual[:, pre_length:, i], y_pred[:, pre_length:, i]) * (68 - pre_length) \n",
        "    score += K.sqrt((prefix_s + body_s * 4)/68) / 5.0\n",
        "  return score\n",
        "\n",
        "def gru_layer(hidden_dim, dropout):\n",
        "    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True,\n",
        "                                  kernel_initializer = 'orthogonal'),\n",
        "                                  merge_mode=\"concat\",)\n",
        "\n",
        "def lstm_layer(hidden_dim, dropout):\n",
        "    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True,\n",
        "                                  kernel_initializer = 'orthogonal'),\n",
        "                                  merge_mode=\"concat\",)\n",
        "\n",
        "def build_base_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=256, type=0,):\n",
        "    def wave_layer(x):\n",
        "      def wave_block(x, filters, kernel_size, n):\n",
        "        dilation_rates = [2 ** i for i in range(n)]\n",
        "        x = tf.keras.layers.Conv1D(filters = filters, \n",
        "                                    kernel_size = 1,\n",
        "                                    padding = 'same')(x)\n",
        "        res_x = x\n",
        "        for dilation_rate in dilation_rates:\n",
        "            tanh_out = tf.keras.layers.Conv1D(filters = filters,\n",
        "                              kernel_size = kernel_size,\n",
        "                              padding = 'same', \n",
        "                              activation = 'tanh', \n",
        "                              dilation_rate = dilation_rate)(x)\n",
        "            sigm_out = tf.keras.layers.Conv1D(filters = filters,\n",
        "                              kernel_size = kernel_size,\n",
        "                              padding = 'same',\n",
        "                              activation = 'sigmoid', \n",
        "                              dilation_rate = dilation_rate)(x)\n",
        "            x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n",
        "            x = tf.keras.layers.Conv1D(filters = filters,\n",
        "                        kernel_size = 1,\n",
        "                        padding = 'same')(x)\n",
        "            res_x = tf.keras.layers.Add()([res_x, x])\n",
        "        return res_x\n",
        "\n",
        "      x = wave_block(x, 32, 3, 8)\n",
        "      #x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Dropout(0.1)(x)\n",
        "      #x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "      x = wave_block(x, 64, 3, 4)\n",
        "      #x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Dropout(0.1)(x)\n",
        "      #x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "      x = wave_block(x, 128, 3, 1)\n",
        "      #x = tf.keras.layers.BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Dropout(0.1)(x)\n",
        "      #x = tf.keras.layers.Activation(\"relu\")(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "    def conv_layer(row, col, x):\n",
        "        conv = tf.keras.layers.Conv1D(hidden_dim * 2, 5,\n",
        "                          padding='same',\n",
        "                          activation='tanh',\n",
        "                          input_shape=(row, col))(x)\n",
        "        \n",
        "        # trans = EncoderLayer(conv.shape[2], 8, 64, rate=0.6)(conv)\n",
        "        \n",
        "        gcn_1 = GraphConv(\n",
        "            graph_channels,\n",
        "            activation='tanh',\n",
        "        )([conv, As_in[:,:,:,1]])\n",
        "\n",
        "        gcn_2 = ChebConv(\n",
        "            graph_channels,\n",
        "            activation='tanh',\n",
        "        )([conv, As_in[:,:,:,1]])\n",
        "        \n",
        "        gcn_3 = ARMAConv(\n",
        "                    graph_channels,\n",
        "                    activation='tanh',\n",
        "                )([conv, As_in[:,:,:,1]])\n",
        "\n",
        "        gcn_4 = GraphConv(\n",
        "            graph_channels,\n",
        "            activation='tanh',\n",
        "        )([conv, As_in[:,:,:,0]])\n",
        "    \n",
        "        gcn_1 = tf.keras.layers.Concatenate()([gcn_1, gcn_2, gcn_3])\n",
        "        gcn_1 = tf.keras.layers.Conv1D(3*graph_channels, 5,\n",
        "                                  padding='same',\n",
        "                                  activation='tanh',\n",
        "                                  input_shape=(row, col))(gcn_1)\n",
        "\n",
        "\n",
        "        conv = tf.keras.layers.Concatenate()([x, conv,gcn_1, gcn_2, gcn_3])\n",
        "        conv = tf.keras.layers.Activation(\"relu\")(conv)\n",
        "        conv = tf.keras.layers.SpatialDropout1D(0.1)(conv)\n",
        "\n",
        "        return conv\n",
        "\n",
        "        \n",
        "    graph_channels = 80 * 1\n",
        "    \n",
        "    inputs = L.Input(shape=(seq_len, 5))\n",
        "    ohe_inputs = L.Input(shape=(seq_len, 14))\n",
        "    As_in = tf.keras.layers.Input((seq_len, seq_len, 2))\n",
        "    \n",
        "    # hidden_As = Dense(2)(tf.reshape(L.concatenate([As_in, pair_probs], axis=2), (-1, seq_len, seq_len, 2)))\n",
        "\n",
        "    # split categorical and numerical features and concatenate them later.\n",
        "    categorical_feat_dim = 3\n",
        "    categorical_fea = ohe_inputs\n",
        "    numerical_fea = inputs[:, :, 3:]\n",
        "\n",
        "    # embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea)\n",
        "    # reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
        "    \n",
        "    reshaped = L.concatenate([categorical_fea, numerical_fea], axis=2)\n",
        "    \n",
        "    # reshaped = L.concatenate([reshaped, trans], axis=2)\n",
        "\n",
        "    # reshaped = tf.keras.layers.Conv1D(256, 5, padding='same', activation=tf.keras.activations.swish)(reshaped)\n",
        "    \n",
        "\n",
        "    if type == 0:\n",
        "        reshaped = L.concatenate([\n",
        "                    conv_layer(reshaped.shape[1], reshaped.shape[2], reshaped),\n",
        "                    wave_layer(reshaped)\n",
        "                  ], axis=2)\n",
        "        hidden_1 = gru_layer(hidden_dim, dropout)(reshaped)\n",
        "\n",
        "        hidden_2 = L.concatenate([\n",
        "                              conv_layer(hidden_1.shape[1], hidden_1.shape[2], hidden_1),\n",
        "                              wave_layer(hidden_1)\n",
        "                           ], axis=2)\n",
        "\n",
        "        hidden_3 = gru_layer(hidden_dim, dropout)(hidden_2)\n",
        "    elif type == 1:\n",
        "        reshaped = L.concatenate([\n",
        "              conv_layer(reshaped.shape[1], reshaped.shape[2], reshaped),\n",
        "              wave_layer(reshaped)\n",
        "            ], axis=2)\n",
        "        hidden_1 = lstm_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden_2 = L.concatenate([\n",
        "                      conv_layer(hidden_1.shape[1], hidden_1.shape[2], hidden_1),\n",
        "                      wave_layer(hidden_1)\n",
        "                    ], axis=2)\n",
        "        hidden_3 = gru_layer(hidden_dim, dropout)(hidden_2)\n",
        "\n",
        "        # hidden_3 = L.Add()([hidden_1, hidden_3])\n",
        "    elif type == 2:\n",
        "        reshaped = conv_layer(reshaped.shape[1], reshaped.shape[2], reshaped)\n",
        "        hidden_1 = gru_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden_2 = conv_layer(hidden_1.shape[1], hidden_1.shape[2], hidden_1)\n",
        "        hidden_3 = lstm_layer(hidden_dim, dropout)(hidden_2)\n",
        "        # hidden_3 = L.Add()([hidden_1, hidden_3])\n",
        "    elif type == 3:\n",
        "        reshaped = L.concatenate([\n",
        "              conv_layer(reshaped.shape[1], reshaped.shape[2], reshaped),\n",
        "              wave_layer(reshaped)\n",
        "            ], axis=2)\n",
        "        hidden_1 = lstm_layer(hidden_dim, dropout)(reshaped)\n",
        "        hidden_2 = L.concatenate([\n",
        "                      conv_layer(hidden_1.shape[1], hidden_1.shape[2], hidden_1),\n",
        "                      wave_layer(hidden_1)\n",
        "                    ], axis=2)\n",
        "        hidden_3 = lstm_layer(hidden_dim, dropout)(hidden_2)\n",
        "        # hidden_3 = L.Add()([hidden_1, hidden_3])\n",
        "\n",
        "\n",
        "    base_model = tf.keras.Model(inputs=[inputs, ohe_inputs, As_in], outputs=hidden_3)\n",
        "    return base_model\n",
        "\n",
        "\n",
        "def build_model(base_model, seq_len=107, pred_len=68, type=0, for_pretrain=False):\n",
        "    inputs = L.Input(shape=(seq_len, 5))\n",
        "    ohe_inputs = L.Input(shape=(seq_len, 14))\n",
        "    As_in = tf.keras.layers.Input((seq_len, seq_len, 2))\n",
        "\n",
        "\n",
        "    if for_pretrain:\n",
        "      truncated = base_model([\n",
        "                              L.SpatialDropout1D(0.3)(inputs), \n",
        "                              L.SpatialDropout1D(0.3)(ohe_inputs),\n",
        "                              As_in], )\n",
        "\n",
        "      out = L.Dense(14, activation='sigmoid', name=\"targets\")(truncated)\n",
        "\n",
        "      out = - tf.reduce_mean(14 * ohe_inputs * tf.math.log(out + 1e-4) + (1 - ohe_inputs) * tf.math.log(1 - out + 1e-4))\n",
        "\n",
        "      model = tf.keras.Model(inputs=[inputs, ohe_inputs, As_in], outputs=out)\n",
        "      model.compile(tf.keras.optimizers.Adam(), \n",
        "                    loss=lambda t, y: y)\n",
        "    else:\n",
        "      truncated = base_model([inputs, ohe_inputs, As_in])[:, :pred_len]\n",
        "      truncated = L.concatenate([truncated, inputs[:,:pred_len, -5:]], axis=2)\n",
        "      out = L.Dense(5, activation='linear', name=\"targets\")(truncated)\n",
        "      clf_out = L.Dense(5, activation='sigmoid', name=\"outlier\")(truncated)\n",
        "      \n",
        "      model = tf.keras.Model(inputs=[inputs, ohe_inputs, As_in], outputs=[out])\n",
        "      model.compile(tf.keras.optimizers.Adam(), \n",
        "                    loss=[mcrmse_weighted_by_pos],\n",
        "                    metrics=[mcrmse],\n",
        "                    loss_weights=[1, 0])\n",
        "    return model\n",
        "\n",
        "base = build_base_model()\n",
        "model = build_model(base)\n",
        "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.66278092 1.27673896 0.969346   0.75402756 0.60993292 0.52259288\n",
            " 0.439232   0.40121072 0.42952168 0.4347896  0.44271248 0.45124864\n",
            " 0.52869068 0.58042788 0.59399012 0.53309032 0.5429752  0.53761156\n",
            " 0.5209262  0.53516748 0.54519744 0.50761996 0.4931458  0.52580228\n",
            " 0.50970232 0.47559344 0.46833364 0.4994038  0.4994278  0.48369356\n",
            " 0.45206352 0.41669564 0.44728292 0.5194518  0.584991   0.5154996\n",
            " 0.5480878  0.50229692 0.47238296 0.46725604 0.43237196 0.45317752\n",
            " 0.47623736 0.43207072 0.48043012 0.42294552 0.37553376 0.38930104\n",
            " 0.37393152 0.35812416 0.3514118  0.36296312 0.47218276 0.40970248\n",
            " 0.39709124 0.42396928 0.3899674  0.3459736  0.37596636 0.35070412\n",
            " 0.30864824 0.30550936 0.29595908 0.32075716 0.30268844 0.34025332\n",
            " 0.34924704 0.28024172]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NluiRYaVIMx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30b52a8b-5af1-4de7-e41d-f184a8cd7ee0"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2400, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9SlQRw15YpX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjvTktUNHx1p"
      },
      "source": [
        "# clustering for GroupKFold\n",
        "# expecting more accurate CV by putting similar RNAs into the same fold.\n",
        "kmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[0][:,:,0])\n",
        "train['cluster_id'] = kmeans_model.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fW5M_CKI7yj"
      },
      "source": [
        "def get_adjacency_matrix(inps):\n",
        "    As = []\n",
        "    for row in range(0, inps.shape[0]):\n",
        "        A = np.zeros((inps.shape[1], inps.shape[1]))\n",
        "        stack = []\n",
        "        opened_so_far = []\n",
        "\n",
        "        for seqpos in range(0, inps.shape[1]):\n",
        "          # A[seqpos, seqpos] = 1\n",
        "          if inps[row, seqpos, 1] == 0: # open\n",
        "              stack.append(seqpos)\n",
        "              opened_so_far.append(seqpos)\n",
        "          elif inps[row, seqpos, 1] == 1:\n",
        "              openpos = stack.pop()\n",
        "              power = 1\n",
        "              A[openpos, seqpos] = power\n",
        "              A[seqpos, openpos] = power\n",
        "        # for i in range(0, inps.shape[1]):\n",
        "        #   if i != 1:\n",
        "        #     A[1, i] = 1\n",
        "        #   if i != 2:\n",
        "        #     A[2, i] = 1\n",
        "        \n",
        "        As.append(A)\n",
        "    return np.array(As)\n",
        "\n",
        "def get_distance_matrix(As):\n",
        "    ## adjacent matrix based on distance on the sequence\n",
        "    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n",
        "    \n",
        "    idx = np.arange(As.shape[1])\n",
        "    Ds = []\n",
        "    for i in range(len(idx)):\n",
        "        d = np.abs(idx[i] - idx)\n",
        "        Ds.append(d)\n",
        "\n",
        "    Ds = np.array(Ds) + 1\n",
        "    Ds = 1/Ds\n",
        "    Ds = Ds[None, :,:]\n",
        "    Ds = np.repeat(Ds, len(As), axis = 0)\n",
        "    \n",
        "    Dss = []\n",
        "    for i in [1, 2, 4]: \n",
        "        Dss.append(Ds ** i)\n",
        "    Ds = np.stack(Dss, axis = 3)\n",
        "    print(Ds.shape)\n",
        "    return Ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCF3XkImI8q6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f3626648-e177-4ef8-8b3d-1fd109ff9d41"
      },
      "source": [
        "from tqdm import tqdm\n",
        "unp_train = []\n",
        "unp_public_test = []\n",
        "unp_private_test = []\n",
        "public_df = test.query(\"seq_length == 107\").copy()\n",
        "private_df = test.query(\"seq_length == 130\").copy()\n",
        "\n",
        "for id in tqdm(train.id.values):\n",
        "    unp_train.append(np.load(bpps_path+'/'+ id +'.npy'))\n",
        "unp_train = np.array(unp_train)\n",
        "for id in tqdm(public_df.id.values):\n",
        "    unp_public_test.append(np.load(bpps_path+'/'+ id +'.npy'))\n",
        "unp_public_test = np.array(unp_public_test)\n",
        "for id in tqdm(private_df.id.values):\n",
        "    unp_private_test.append(np.load(bpps_path+'/'+ id +'.npy'))\n",
        "unp_private_test = np.array(unp_private_test)\n",
        "\n",
        "trn, trn_ohe = preprocess_inputs(train[train.SN_filter == 1])\n",
        "print(train.shape, trn.shape)\n",
        "trn_As = get_adjacency_matrix(trn[:,:,:3])\n",
        "trn_unp = unp_train[np.where((train.SN_filter == 1).values)]\n",
        "# trn_Ds = get_distance_matrix(trn_unp)\n",
        "trn_As = np.concatenate([trn_unp[:,:,:,None], trn_As[:,:,:,None]], axis=3)\n",
        "\n",
        "public_inputs, public_ohe_inputs = preprocess_inputs(public_df)\n",
        "private_inputs, private_ohe_inputs = preprocess_inputs(private_df)\n",
        "\n",
        "public_As = get_adjacency_matrix(public_inputs[:,:,:3])             \n",
        "private_As = get_adjacency_matrix(private_inputs[:,:,:3])             \n",
        "# public_Ds = get_distance_matrix(unp_public_test)\n",
        "# private_Ds = get_distance_matrix(unp_private_test)\n",
        "\n",
        "public_As = np.concatenate([unp_public_test[:,:,:,None], public_As[:,:,:,None]], axis=3)\n",
        "private_As = np.concatenate([unp_private_test[:,:,:,None], private_As[:,:,:,None]], axis=3)\n",
        "\n",
        "# all_inps = np.concatenate([trn, public_inputs], axis=0)\n",
        "# all_ohes = np.concatenate([trn_ohe, public_ohe_inputs], axis=0)\n",
        "# all_As = np.concatenate([trn_As, public_As], axis=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:00<00:00, 3080.71it/s]\n",
            "100%|██████████| 629/629 [00:00<00:00, 3399.32it/s]\n",
            "100%|██████████| 3005/3005 [00:01<00:00, 2951.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2400, 23) (1589, 107, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaZZD3LFJBRY"
      },
      "source": [
        "# for t in range(0, 4):\n",
        "#   model_short_base = build_base_model(seq_len=107, pred_len=107, type=t, dropout=.5)\n",
        "#   model_long_base = build_base_model(seq_len=130, pred_len=130,type=t, dropout=.5)\n",
        "  \n",
        "#   model_short = build_model(model_short_base, seq_len=107, pred_len=107, type=t, for_pretrain=True)\n",
        "#   model_long = build_model(model_long_base, seq_len=130, pred_len=130, type=t, for_pretrain=True)\n",
        "  \n",
        "#   for i in range(4):\n",
        "#     try:\n",
        "#       model_short_base.load_weights(f\"outputs/base_model_t{t}.h5\")\n",
        "#     except:\n",
        "#       print(\"Cannot load weights\")\n",
        "    \n",
        "#     history = model_short.fit([public_inputs, public_ohe_inputs, public_As], public_ohe_inputs, epochs=5, verbose=0)\n",
        "#     print(i, \">>> public\", f\"outputs/base_model_t{t}.h5\", f\"loss={min(history.history['loss'])}\")\n",
        "#     model_short_base.save_weights(f\"outputs/base_model_t{t}.h5\")\n",
        "    \n",
        "#     try:\n",
        "#       model_long_base.load_weights(f\"outputs/base_model_t{t}.h5\")\n",
        "#     except:\n",
        "#       print(\"Cannot load weights\")\n",
        "    \n",
        "#     history = model_long.fit([private_inputs, private_ohe_inputs, private_As], private_ohe_inputs, epochs=5, verbose=0)\n",
        "#     print(i, \">>> private\", f\"outputs/base_model_t{t}.h5\", f\"loss={min(history.history['loss'])}\")\n",
        "#     model_long_base.save_weights(f\"outputs/base_model_t{t}.h5\")\n",
        "\n",
        "\n",
        "#     try:\n",
        "#       model_short_base.load_weights(f\"outputs/base_model_t{t}.h5\")\n",
        "#     except:\n",
        "#       print(\"Cannot load weights\")\n",
        "\n",
        "#     history = model_short.fit([trn, trn_ohe, trn_As], trn_ohe, epochs=5, verbose=0)\n",
        "#     print(i, \">>> train\", f\"outputs/base_model_t{t}.h5\", f\"loss={min(history.history['loss'])}\")\n",
        "#     model_short_base.save_weights(f\"outputs/base_model_t{t}.h5\")\n",
        "\n",
        "#     print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ94YnFPrwBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "808ba718-4f41-4be6-af9e-c8cc39c9c5ea"
      },
      "source": [
        "def aug_data(df):\n",
        "    target_df = df.copy()\n",
        "    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n",
        "                         \n",
        "    del target_df['structure']\n",
        "    del target_df['predicted_loop_type']\n",
        "    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n",
        "    print(new_df.columns)\n",
        "    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n",
        "    df['log_gamma'] = 100\n",
        "    df['score'] = 1.0\n",
        "    df[\"is_aug\"] = False\n",
        "    new_df[\"is_aug\"] = True\n",
        "    df = df.append(new_df[df.columns])\n",
        "    return df\n",
        "\n",
        "train = aug_data(train)\n",
        "test = aug_data(test)\n",
        "\n",
        "print(train[\"is_aug\"].mean())\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'sequence', 'structure', 'log_gamma', 'score', 'cnt',\n",
            "       'predicted_loop_type', 'index', 'signal_to_noise', 'SN_filter',\n",
            "       'seq_length', 'seq_scored', 'reactivity_error', 'deg_error_Mg_pH10',\n",
            "       'deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C', 'reactivity',\n",
            "       'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C', 'bpps_sum',\n",
            "       'bpps_max', 'bpps_nb', 'cluster_id'],\n",
            "      dtype='object')\n",
            "Index(['id', 'sequence', 'structure', 'log_gamma', 'score', 'cnt',\n",
            "       'predicted_loop_type', 'index', 'seq_length', 'seq_scored', 'bpps_sum',\n",
            "       'bpps_max', 'bpps_nb'],\n",
            "      dtype='object')\n",
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebacrzblkGkF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi4SklvTGtmE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3a40f0b8-7978-4200-eef9-4a9dd6ffe502"
      },
      "source": [
        "unp_train = []\n",
        "unp_public_test = []\n",
        "unp_private_test = []\n",
        "public_df = test.query(\"seq_length == 107\").copy()\n",
        "private_df = test.query(\"seq_length == 130\").copy()\n",
        "\n",
        "for id in tqdm(train.id.values):\n",
        "    unp_train.append(np.load(bpps_path+'/'+ id +'.npy'))\n",
        "unp_train = np.array(unp_train)\n",
        "for id in tqdm(public_df.id.values):\n",
        "    unp_public_test.append(np.load(bpps_path+'/'+ id +'.npy'))\n",
        "unp_public_test = np.array(unp_public_test)\n",
        "for id in tqdm(private_df.id.values):\n",
        "    unp_private_test.append(np.load(bpps_path+'/'+ id +'.npy'))\n",
        "unp_private_test = np.array(unp_private_test)\n",
        "\n",
        "\n",
        "public_inputs, public_ohe_inputs = preprocess_inputs(public_df)\n",
        "private_inputs, private_ohe_inputs = preprocess_inputs(private_df)\n",
        "\n",
        "public_As = get_adjacency_matrix(public_inputs[:,:,:3])             \n",
        "private_As = get_adjacency_matrix(private_inputs[:,:,:3])             \n",
        "\n",
        "public_As = np.concatenate([unp_public_test[:,:,:,None], public_As[:,:,:,None]], axis=3)\n",
        "private_As = np.concatenate([unp_private_test[:,:,:,None], private_As[:,:,:,None]], axis=3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4800/4800 [00:01<00:00, 3229.42it/s]\n",
            "100%|██████████| 1258/1258 [00:00<00:00, 3312.50it/s]\n",
            "100%|██████████| 6010/6010 [00:01<00:00, 3039.41it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZizUDVPTM3h"
      },
      "source": [
        "from swa.tfkeras import SWA\n",
        "import gc\n",
        "from math import pi\n",
        "from math import cos\n",
        "from math import floor\n",
        "\n",
        "class CosineAnnealingLearningRateSchedule(tf.keras.callbacks.Callback):\n",
        "\t# constructor\n",
        "\tdef __init__(self, n_epochs, n_cycles, lrate_max, verbose=0):\n",
        "\t\tself.epochs = n_epochs\n",
        "\t\tself.cycles = n_cycles\n",
        "\t\tself.lr_max = lrate_max\n",
        "\t\tself.lrates = list()\n",
        " \n",
        "\t# calculate learning rate for an epoch\n",
        "\tdef cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n",
        "\t\tepochs_per_cycle = floor(n_epochs/n_cycles)\n",
        "\t\tcos_inner = (pi * (epoch % epochs_per_cycle)) / (epochs_per_cycle)\n",
        "\t\treturn lrate_max/2 * (cos(cos_inner) + 1)\n",
        " \n",
        "\t# calculate and set learning rate at the start of the epoch\n",
        "\tdef on_epoch_begin(self, epoch, logs=None):\n",
        "\t\t# calculate learning rate\n",
        "\t\tlr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n",
        "\t\t# set learning rate\n",
        "\t\ttf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "\t\t# log value\n",
        "\t\tself.lrates.append(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZa8i6DUOr1f"
      },
      "source": [
        "def train_and_predict(type = 0, FOLD_N = 5):\n",
        "    global public_inputs, private_inputs\n",
        "    \n",
        "    gkf = GroupKFold(n_splits=FOLD_N)\n",
        "\n",
        "    holdouts = []\n",
        "    holdout_preds = []\n",
        "\n",
        "    for cv, (tr_idx, vl_idx) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n",
        "        # if cv != 2:\n",
        "          # continue\n",
        "        print()\n",
        "        print(f\"{'#'*5} Type {type} Fold {cv} {'#'*5}\")\n",
        "\n",
        "        val = train.iloc[vl_idx].reset_index(drop=True)\n",
        "        unp_val_all = unp_train[vl_idx]\n",
        "        # sds_val_all = shortest_Ds_trn[vl_idx]\n",
        "\n",
        "        a = (val.SN_filter == 1)\n",
        "        \n",
        "        x_val_all, x_ohe_val_all = preprocess_inputs(val)\n",
        "        val = val[a]\n",
        "        unp_val = unp_val_all[np.where(a)]\n",
        "        # sds_val = sds_val_all[np.where(a)]\n",
        "        \n",
        "        x_val, x_ohe_val = preprocess_inputs(val)\n",
        "        y_val = np.array(val[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
        "        y_err_val = np.log1p(np.array(val[pred_cols_2].values.tolist()).transpose((0, 2, 1)))\n",
        "        y_clf_val = y_val > 1\n",
        "\n",
        "        val_As = get_adjacency_matrix(x_val[:,:,:3])\n",
        "        val_all_As = get_adjacency_matrix(x_val_all[:,:,:3])\n",
        "        \n",
        "        trn = train.iloc[tr_idx]\n",
        "        unp_trn = unp_train[tr_idx]\n",
        "        # sds_trn = shortest_Ds_trn[tr_idx]\n",
        "\n",
        "        \n",
        "        y_good = np.array(trn[trn.SN_filter == 1][pred_cols].values.tolist()).transpose((0, 2, 1))\n",
        "        mins = np.minimum(y_good.reshape((-1, 5)).min(axis=0), y_val.reshape((-1, 5)).min(axis=0))\n",
        "        maxs = np.maximum(y_good.reshape((-1, 5)).max(axis=0), y_val.reshape((-1, 5)).max(axis=0))\n",
        "        print(\"trn.SN_filter == 1\", mins, maxs)\n",
        "        up = np.quantile(y_good.reshape(-1, 1), .999)\n",
        "        low = np.quantile(y_good.reshape(-1, 1), .001)\n",
        "        print(up, low)\n",
        "        \n",
        "\n",
        "        x_trn, x_ohe_trn = preprocess_inputs(trn)\n",
        "        y_trn = np.array(trn[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
        "        y_clf_trn = y_trn > 1\n",
        "\n",
        "        \n",
        "        for t_i in range(0, 5):\n",
        "          y_trn[:,:,t_i] = y_trn[:,:,t_i].clip(mins[t_i], maxs[t_i])\n",
        "          # y_trn[:,:,t_i] = y_trn[:,:,t_i].clip(low, up)\n",
        "\n",
        "        \n",
        "        y_err_trn = np.log1p(np.array(trn[pred_cols_2].values.tolist()).transpose((0, 2, 1)))\n",
        "        w_trn = np.log(trn.signal_to_noise+1.11)/2\n",
        "        train_As = get_adjacency_matrix(x_trn[:,:,:3])\n",
        "\n",
        "\n",
        "        train_As = np.concatenate([unp_trn[:,:,:,None]** 50, train_As[:,:,:,None]], axis=3)\n",
        "        val_As = np.concatenate([unp_val[:,:,:,None]** 50, val_As[:,:,:,None]], axis=3)\n",
        "        val_all_As = np.concatenate([unp_val_all[:,:,:,None]** 50, val_all_As[:,:,:,None]], axis=3)\n",
        "\n",
        "        # pos_trn = np.zeros((x_trn.shape[0], 68, 5))\n",
        "        # pos_val = np.zeros((x_val.shape[0], 68, 5))\n",
        "        # pos_val_all = np.zeros((x_val_all.shape[0], x_val_all.shape[1], 5))\n",
        "        # pos_public = np.zeros((public_inputs.shape[0], public_inputs.shape[1], 5))\n",
        "        # pos_private = np.zeros((private_inputs.shape[0], private_inputs.shape[1], 5))\n",
        "\n",
        "        # for seqpos in range(0, 68):\n",
        "        #   for t in range(0, 5):\n",
        "        #     t_avg = np.quantile(y_trn[:,seqpos,t], .75)\n",
        "        #     # print(seqpos, \"max\", np.max(y_trn[:,seqpos,t]), \"99%\", np.quantile(y_trn[:,seqpos,t], .95), \"90%\", np.quantile(y_trn[:,seqpos,t], .9), \"75%\", np.quantile(y_trn[:,seqpos,t], .75))\n",
        "\n",
        "        #     pos_trn[:,seqpos,t] = t_avg\n",
        "        #     pos_val[:,seqpos,t] = t_avg\n",
        "        #     pos_val_all[:,seqpos,t] = t_avg\n",
        "        #     pos_public[:,seqpos,t] = t_avg\n",
        "        #     pos_private[:,seqpos,t] = t_avg\n",
        "        # y_clf_trn = y_trn > pos_trn\n",
        "        # y_clf_val = y_val > pos_val\n",
        "\n",
        "        # for t in range(0,5):\n",
        "        #   pos_trn[:,69:,t] = pos_trn[:,67,t][0]\n",
        "        #   pos_val[:,69:,t] = pos_val[:,67,t][0]\n",
        "        #   pos_val_all[:,69:,t] = pos_val_all[:,67,t][0]\n",
        "        #   pos_public[:,69:,t] = pos_public[:,67,t][0]\n",
        "        #   pos_private[:,69:,t] = pos_private[:,67,t][0]\n",
        "        \n",
        "        # x_trn = np.concatenate([x_trn, pos_trn], axis=2)\n",
        "        # x_val = np.concatenate([x_val, pos_val], axis=2)\n",
        "        # x_val_all = np.concatenate([x_val_all, pos_val_all], axis=2)\n",
        "        # if public_inputs.shape[2] == 5:\n",
        "        #   public_inputs = np.concatenate([public_inputs, pos_public], axis=2)\n",
        "        #   private_inputs = np.concatenate([private_inputs, pos_private], axis=2)\n",
        "        # else:\n",
        "        #   public_inputs[:,:,-5:] = pos_public\n",
        "        #   private_inputs[:,:,-5:] = pos_private\n",
        "\n",
        "        model_base = build_base_model(seq_len=107, pred_len=107, type=type)\n",
        "        \n",
        "        # load base weight for smoother training\n",
        "        # print(\"load_weights\", f\"outputs/base_model_t{type}.h5\")\n",
        "        # model_base.load_weights(f\"outputs/base_model_t{type}.h5\")\n",
        "        model = build_model(model_base, type=type)\n",
        "        \n",
        "        \n",
        "        history = model.fit(\n",
        "            [x_trn, x_ohe_trn, train_As], [y_trn],\n",
        "            validation_data = ([x_val,x_ohe_val,val_As], [y_val]),\n",
        "            batch_size=64,\n",
        "            epochs=200,\n",
        "            sample_weight=w_trn,\n",
        "            callbacks=[\n",
        "                # CosineAnnealingLearningRateSchedule(100, 10, 0.001),\n",
        "                tf.keras.callbacks.ReduceLROnPlateau(verbose=1, monitor='val_mcrmse', patience=20),\n",
        "                tf.keras.callbacks.ModelCheckpoint(f'outputs/{str(type)}_model{Ver}_cv{cv}.h5',save_best_only=True, verbose=0, monitor='val_mcrmse'),\n",
        "                tf.keras.callbacks.EarlyStopping(\n",
        "                    patience=20, \n",
        "                    monitor='val_mcrmse',\n",
        "                    verbose=1,\n",
        "                    mode=\"auto\",\n",
        "                    baseline=None,\n",
        "                    restore_best_weights=True,\n",
        "                ),\n",
        "            ],\n",
        "            verbose=2\n",
        "        )\n",
        "        \n",
        "        print(f\"{'#'*5} Type {type} Fold {cv} {'#'*5}\")\n",
        "        print(f\"Fold {cv} validation loss={min(history.history['val_loss'])}\")\n",
        "\n",
        "        # model.save_weights(f'outputs/{str(type)}_model{Ver}_cv{cv}.h5')\n",
        "        model.load_weights(f'outputs/{str(type)}_model{Ver}_cv{cv}.h5')\n",
        "\n",
        "        oof_fea = train.iloc[vl_idx]\n",
        "        oof_preds = model.predict([x_val_all,x_ohe_val_all,val_all_As])\n",
        "        godd_oof_preds = model.predict([x_val,x_ohe_val,val_As])\n",
        "\n",
        "        print(tf.reduce_mean(mcrmse(y_val, godd_oof_preds)))\n",
        "\n",
        "        holdouts.append(oof_fea)\n",
        "        holdout_preds.append(oof_preds)\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "        model_short_base = build_base_model(seq_len=107, pred_len=107, type=type)\n",
        "        model_long_base = build_base_model(seq_len=130, pred_len=130,type=type)\n",
        "        model_short = build_model(model_short_base, seq_len=107, pred_len=107,type=type)\n",
        "        model_long = build_model(model_long_base, seq_len=130, pred_len=130,type=type)\n",
        "\n",
        "        model_short.load_weights(f'outputs/{str(type)}_model{Ver}_cv{cv}.h5')\n",
        "        model_long.load_weights(f'outputs/{str(type)}_model{Ver}_cv{cv}.h5')\n",
        "\n",
        "        if cv == 0:\n",
        "            public_preds = model_short.predict([public_inputs,public_ohe_inputs,public_As])/FOLD_N\n",
        "            private_preds = model_long.predict([private_inputs,private_ohe_inputs,private_As])/FOLD_N\n",
        "        else:\n",
        "            public_preds += model_short.predict([public_inputs,public_ohe_inputs,public_As])/FOLD_N\n",
        "            private_preds += model_long.predict([private_inputs,private_ohe_inputs,private_As])/FOLD_N\n",
        "        del train_As, val_As, val_all_As\n",
        "        gc.collect()\n",
        "    return holdouts, holdout_preds, public_df, public_preds, private_df, private_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VxQkERaOtbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c37db862-3a06-4a04-9469-cb9f78bb8405"
      },
      "source": [
        "val_df, val_preds, test_df, test_preds = [], [], [], []\n",
        "if debug:\n",
        "    nmodel = 1\n",
        "else:\n",
        "    nmodel = 4\n",
        "for i in range(nmodel):\n",
        "    # if i not in [3]:\n",
        "    #   continue\n",
        "    holdouts, holdout_preds, public_df, public_preds, private_df, private_preds = train_and_predict(i)\n",
        "    val_df += holdouts\n",
        "    val_preds += holdout_preds\n",
        "    test_df.append(public_df)\n",
        "    test_df.append(private_df)\n",
        "    test_preds.append(public_preds)\n",
        "    test_preds.append(private_preds)\n",
        "\n",
        "\n",
        "\n",
        "# pred\n",
        "test_preds_ls = []\n",
        "for df, preds in zip(test_df, test_preds):\n",
        "    for i, uid in enumerate(df.id):\n",
        "        single_pred = preds[i]\n",
        "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
        "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
        "        test_preds_ls.append(single_df)\n",
        "preds_df = pd.concat(test_preds_ls).groupby('id_seqpos').mean().reset_index()\n",
        "# .mean() is for\n",
        "# 1, Predictions from multiple models\n",
        "# 2, TTA (augmented test data)\n",
        "\n",
        "off_preds_ls = []\n",
        "for df, preds in zip(val_df, val_preds):\n",
        "    for i, uid in enumerate(df.id):\n",
        "        single_pred = preds[i]\n",
        "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
        "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
        "        single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n",
        "        off_preds_ls.append(single_df)\n",
        "holdouts_df = pd.concat(off_preds_ls).groupby('id_seqpos').mean().reset_index()\n",
        "\n",
        "# sub\n",
        "submission = preds_df[['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']]\n",
        "submission.to_csv(f'outputs/submission.csv', index=False)\n",
        "print(f'wrote to submission.csv')\n",
        "\n",
        "# vl\n",
        "def print_mse(prd):\n",
        "    val = pd.read_json(train_json_path, lines=True)\n",
        "\n",
        "    val_data = []\n",
        "    for mol_id in val['id'].unique():\n",
        "        sample_data = val.loc[val['id'] == mol_id]\n",
        "        sample_seq_length = sample_data.seq_length.values[0]\n",
        "        for i in range(68):\n",
        "            sample_dict = {\n",
        "                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n",
        "                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n",
        "                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n",
        "                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n",
        "                           }\n",
        "            val_data.append(sample_dict)\n",
        "    val_data = pd.DataFrame(val_data)\n",
        "    val_data = val_data.merge(prd, on='id_seqpos')\n",
        "\n",
        "    rmses = []\n",
        "    mses = []\n",
        "    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n",
        "        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n",
        "        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n",
        "        rmses.append(rmse)\n",
        "        mses.append(mse)\n",
        "        print(col, rmse, mse)\n",
        "    return(np.mean(rmses), np.mean(mses))\n",
        "\n",
        "holdouts_df.to_csv(\"outputs/holdouts_df.csv\", index=False)\n",
        "oof_rmse, oof_mse = print_mse(holdouts_df)\n",
        "gd_oof_rmse, gd_oof_mse = print_mse(holdouts_df[holdouts_df.SN_filter == 1])\n",
        "print(oof_rmse, gd_oof_rmse)\n",
        "\n",
        "# Store per model predictions\n",
        "for t in range(0, 4):\n",
        "  oof_df = pd.concat(off_preds_ls[4800*t: 4800*(t+1)]).groupby('id_seqpos').mean().reset_index()\n",
        "  print(\"type\", t)\n",
        "  print(print_mse(oof_df[oof_df.SN_filter == 1]))\n",
        "  oof_df.to_csv(f'outputs/oof_type_{t}.csv', index=False)\n",
        "\n",
        "for t in range(0, 4):\n",
        "  sub_df = pd.concat(test_preds_ls[7268*t: 7268*(t+1)]).groupby('id_seqpos').mean().reset_index()\n",
        "  sub_df.to_csv(f'outputs/preds_type_{t}.csv', index=False)\n",
        "\n",
        "\n",
        "# Blending\n",
        "oof_dfs = []\n",
        "for t in range(0, 4):\n",
        "  oof_df = pd.read_csv(f\"outputs/oof_type_{t}.csv\")\n",
        "  oof_dfs.append(oof_df)\n",
        "\n",
        "ws = [.3, .3, .2, .2]\n",
        "blended_oof = oof_dfs[0].copy()\n",
        "blended_oof[pred_cols] = 0\n",
        "for d, w in zip(oof_dfs, ws):\n",
        "  blended_oof[pred_cols] += d[pred_cols] * w\n",
        "\n",
        "oof_rmse, _ = print_mse(blended_oof)\n",
        "print(oof_rmse)\n",
        "oof_rmse, _ = print_mse(blended_oof[blended_oof.SN_filter == 1])\n",
        "print(oof_rmse)\n",
        "blended_oof.to_csv(\"outputs/blended_oof.csv\", index=False)\n",
        "\n",
        "sub_dfs = []\n",
        "for t in range(0, 4):\n",
        "  sub_df = pd.read_csv(f\"outputs/preds_type_{t}.csv\")\n",
        "  sub_dfs.append(sub_df)\n",
        "\n",
        "blended_sub = sub_dfs[0].copy()\n",
        "blended_sub[pred_cols] = 0\n",
        "for d, w in zip(sub_dfs, ws):\n",
        "  blended_sub[pred_cols] += d[pred_cols] * w\n",
        "print(blended_sub.head())\n",
        "blended_sub.to_csv(\"outputs/blended_sub.csv\", index=False)\n",
        "\n",
        "\n",
        "output_name = f\"outputs_oof_{oof_rmse:.5f}\"\n",
        "print(output_name)\n",
        "\n",
        "\n",
        "import shutil\n",
        "shutil.copytree(\"outputs\", f\"/content/drive/My Drive/Colab Notebooks/OpenVaccine/outputs/{output_name}\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "##### Type 0 Fold 0 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.1759 -0.1961\n",
            "Epoch 1/200\n",
            "60/60 - 9s - loss: 0.6819 - mcrmse: 0.5229 - val_loss: 0.5935 - val_mcrmse: 0.3292\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4668 - mcrmse: 0.3767 - val_loss: 0.5363 - val_mcrmse: 0.2938\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4316 - mcrmse: 0.3536 - val_loss: 0.5017 - val_mcrmse: 0.2742\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4079 - mcrmse: 0.3383 - val_loss: 0.4891 - val_mcrmse: 0.2680\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3916 - mcrmse: 0.3279 - val_loss: 0.4610 - val_mcrmse: 0.2504\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3732 - mcrmse: 0.3167 - val_loss: 0.4522 - val_mcrmse: 0.2448\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3632 - mcrmse: 0.3105 - val_loss: 0.4406 - val_mcrmse: 0.2405\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3504 - mcrmse: 0.3028 - val_loss: 0.4351 - val_mcrmse: 0.2360\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3437 - mcrmse: 0.2986 - val_loss: 0.4306 - val_mcrmse: 0.2337\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3339 - mcrmse: 0.2924 - val_loss: 0.4219 - val_mcrmse: 0.2292\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3276 - mcrmse: 0.2888 - val_loss: 0.4209 - val_mcrmse: 0.2280\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3200 - mcrmse: 0.2841 - val_loss: 0.4137 - val_mcrmse: 0.2243\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3141 - mcrmse: 0.2809 - val_loss: 0.4123 - val_mcrmse: 0.2237\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3080 - mcrmse: 0.2768 - val_loss: 0.4082 - val_mcrmse: 0.2218\n",
            "Epoch 15/200\n",
            "60/60 - 6s - loss: 0.3025 - mcrmse: 0.2736 - val_loss: 0.4232 - val_mcrmse: 0.2287\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2990 - mcrmse: 0.2716 - val_loss: 0.4061 - val_mcrmse: 0.2203\n",
            "Epoch 17/200\n",
            "60/60 - 6s - loss: 0.2916 - mcrmse: 0.2671 - val_loss: 0.4047 - val_mcrmse: 0.2209\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2883 - mcrmse: 0.2649 - val_loss: 0.4025 - val_mcrmse: 0.2183\n",
            "Epoch 19/200\n",
            "60/60 - 6s - loss: 0.2852 - mcrmse: 0.2632 - val_loss: 0.4045 - val_mcrmse: 0.2204\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2819 - mcrmse: 0.2613 - val_loss: 0.3972 - val_mcrmse: 0.2157\n",
            "Epoch 21/200\n",
            "60/60 - 6s - loss: 0.2786 - mcrmse: 0.2594 - val_loss: 0.3981 - val_mcrmse: 0.2168\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2737 - mcrmse: 0.2563 - val_loss: 0.3924 - val_mcrmse: 0.2136\n",
            "Epoch 23/200\n",
            "60/60 - 6s - loss: 0.2710 - mcrmse: 0.2546 - val_loss: 0.3996 - val_mcrmse: 0.2169\n",
            "Epoch 24/200\n",
            "60/60 - 6s - loss: 0.2663 - mcrmse: 0.2519 - val_loss: 0.3985 - val_mcrmse: 0.2162\n",
            "Epoch 25/200\n",
            "60/60 - 6s - loss: 0.2645 - mcrmse: 0.2505 - val_loss: 0.3982 - val_mcrmse: 0.2159\n",
            "Epoch 26/200\n",
            "60/60 - 6s - loss: 0.2607 - mcrmse: 0.2481 - val_loss: 0.3961 - val_mcrmse: 0.2149\n",
            "Epoch 27/200\n",
            "60/60 - 6s - loss: 0.2589 - mcrmse: 0.2470 - val_loss: 0.4012 - val_mcrmse: 0.2170\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2559 - mcrmse: 0.2453 - val_loss: 0.3937 - val_mcrmse: 0.2149\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2531 - mcrmse: 0.2435 - val_loss: 0.3931 - val_mcrmse: 0.2141\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2504 - mcrmse: 0.2417 - val_loss: 0.3968 - val_mcrmse: 0.2154\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2483 - mcrmse: 0.2403 - val_loss: 0.3906 - val_mcrmse: 0.2120\n",
            "Epoch 32/200\n",
            "60/60 - 6s - loss: 0.2467 - mcrmse: 0.2394 - val_loss: 0.3954 - val_mcrmse: 0.2151\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2442 - mcrmse: 0.2377 - val_loss: 0.3893 - val_mcrmse: 0.2115\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2410 - mcrmse: 0.2358 - val_loss: 0.3954 - val_mcrmse: 0.2145\n",
            "Epoch 35/200\n",
            "60/60 - 6s - loss: 0.2392 - mcrmse: 0.2348 - val_loss: 0.3929 - val_mcrmse: 0.2134\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2387 - mcrmse: 0.2343 - val_loss: 0.3916 - val_mcrmse: 0.2132\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2365 - mcrmse: 0.2329 - val_loss: 0.3901 - val_mcrmse: 0.2119\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2345 - mcrmse: 0.2317 - val_loss: 0.3940 - val_mcrmse: 0.2140\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2332 - mcrmse: 0.2308 - val_loss: 0.3935 - val_mcrmse: 0.2143\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2304 - mcrmse: 0.2290 - val_loss: 0.3883 - val_mcrmse: 0.2120\n",
            "Epoch 41/200\n",
            "60/60 - 6s - loss: 0.2284 - mcrmse: 0.2277 - val_loss: 0.3925 - val_mcrmse: 0.2138\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2271 - mcrmse: 0.2267 - val_loss: 0.3876 - val_mcrmse: 0.2110\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2255 - mcrmse: 0.2257 - val_loss: 0.3899 - val_mcrmse: 0.2124\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2233 - mcrmse: 0.2243 - val_loss: 0.3911 - val_mcrmse: 0.2128\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2225 - mcrmse: 0.2237 - val_loss: 0.3880 - val_mcrmse: 0.2115\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.2202 - mcrmse: 0.2223 - val_loss: 0.3866 - val_mcrmse: 0.2103\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2185 - mcrmse: 0.2212 - val_loss: 0.3892 - val_mcrmse: 0.2115\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2170 - mcrmse: 0.2201 - val_loss: 0.3867 - val_mcrmse: 0.2103\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2143 - mcrmse: 0.2183 - val_loss: 0.3871 - val_mcrmse: 0.2105\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2141 - mcrmse: 0.2179 - val_loss: 0.3883 - val_mcrmse: 0.2116\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2142 - mcrmse: 0.2178 - val_loss: 0.3893 - val_mcrmse: 0.2118\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2132 - mcrmse: 0.2171 - val_loss: 0.3890 - val_mcrmse: 0.2118\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2108 - mcrmse: 0.2159 - val_loss: 0.3867 - val_mcrmse: 0.2109\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.2105 - mcrmse: 0.2155 - val_loss: 0.3870 - val_mcrmse: 0.2101\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2085 - mcrmse: 0.2144 - val_loss: 0.3914 - val_mcrmse: 0.2132\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2064 - mcrmse: 0.2128 - val_loss: 0.3899 - val_mcrmse: 0.2115\n",
            "Epoch 57/200\n",
            "60/60 - 6s - loss: 0.2058 - mcrmse: 0.2124 - val_loss: 0.3935 - val_mcrmse: 0.2138\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2046 - mcrmse: 0.2116 - val_loss: 0.3879 - val_mcrmse: 0.2109\n",
            "Epoch 59/200\n",
            "60/60 - 6s - loss: 0.2038 - mcrmse: 0.2108 - val_loss: 0.3900 - val_mcrmse: 0.2122\n",
            "Epoch 60/200\n",
            "60/60 - 6s - loss: 0.2027 - mcrmse: 0.2101 - val_loss: 0.3878 - val_mcrmse: 0.2112\n",
            "Epoch 61/200\n",
            "60/60 - 7s - loss: 0.2013 - mcrmse: 0.2092 - val_loss: 0.3869 - val_mcrmse: 0.2100\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.2000 - mcrmse: 0.2084 - val_loss: 0.3888 - val_mcrmse: 0.2112\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.1996 - mcrmse: 0.2079 - val_loss: 0.3902 - val_mcrmse: 0.2117\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.1990 - mcrmse: 0.2074 - val_loss: 0.3938 - val_mcrmse: 0.2138\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.1984 - mcrmse: 0.2070 - val_loss: 0.3951 - val_mcrmse: 0.2141\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1970 - mcrmse: 0.2060 - val_loss: 0.3930 - val_mcrmse: 0.2135\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1956 - mcrmse: 0.2050 - val_loss: 0.3911 - val_mcrmse: 0.2121\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1948 - mcrmse: 0.2045 - val_loss: 0.3939 - val_mcrmse: 0.2136\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1940 - mcrmse: 0.2039 - val_loss: 0.3895 - val_mcrmse: 0.2118\n",
            "Epoch 70/200\n",
            "60/60 - 7s - loss: 0.1935 - mcrmse: 0.2035 - val_loss: 0.3864 - val_mcrmse: 0.2099\n",
            "Epoch 71/200\n",
            "60/60 - 7s - loss: 0.1913 - mcrmse: 0.2021 - val_loss: 0.3856 - val_mcrmse: 0.2098\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1915 - mcrmse: 0.2020 - val_loss: 0.3930 - val_mcrmse: 0.2133\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1906 - mcrmse: 0.2013 - val_loss: 0.3930 - val_mcrmse: 0.2129\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1900 - mcrmse: 0.2009 - val_loss: 0.3910 - val_mcrmse: 0.2128\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1893 - mcrmse: 0.2003 - val_loss: 0.3917 - val_mcrmse: 0.2126\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1883 - mcrmse: 0.1996 - val_loss: 0.3897 - val_mcrmse: 0.2120\n",
            "Epoch 77/200\n",
            "60/60 - 6s - loss: 0.1875 - mcrmse: 0.1989 - val_loss: 0.3884 - val_mcrmse: 0.2115\n",
            "Epoch 78/200\n",
            "60/60 - 6s - loss: 0.1869 - mcrmse: 0.1985 - val_loss: 0.3899 - val_mcrmse: 0.2119\n",
            "Epoch 79/200\n",
            "60/60 - 7s - loss: 0.1857 - mcrmse: 0.1977 - val_loss: 0.3857 - val_mcrmse: 0.2094\n",
            "Epoch 80/200\n",
            "60/60 - 6s - loss: 0.1853 - mcrmse: 0.1973 - val_loss: 0.3913 - val_mcrmse: 0.2129\n",
            "Epoch 81/200\n",
            "60/60 - 6s - loss: 0.1842 - mcrmse: 0.1966 - val_loss: 0.3908 - val_mcrmse: 0.2122\n",
            "Epoch 82/200\n",
            "60/60 - 6s - loss: 0.1831 - mcrmse: 0.1958 - val_loss: 0.3908 - val_mcrmse: 0.2123\n",
            "Epoch 83/200\n",
            "60/60 - 6s - loss: 0.1823 - mcrmse: 0.1950 - val_loss: 0.3902 - val_mcrmse: 0.2121\n",
            "Epoch 84/200\n",
            "60/60 - 6s - loss: 0.1813 - mcrmse: 0.1943 - val_loss: 0.3869 - val_mcrmse: 0.2102\n",
            "Epoch 85/200\n",
            "60/60 - 6s - loss: 0.1807 - mcrmse: 0.1941 - val_loss: 0.3909 - val_mcrmse: 0.2127\n",
            "Epoch 86/200\n",
            "60/60 - 6s - loss: 0.1801 - mcrmse: 0.1933 - val_loss: 0.3934 - val_mcrmse: 0.2139\n",
            "Epoch 87/200\n",
            "60/60 - 6s - loss: 0.1800 - mcrmse: 0.1931 - val_loss: 0.3934 - val_mcrmse: 0.2136\n",
            "Epoch 88/200\n",
            "60/60 - 6s - loss: 0.1794 - mcrmse: 0.1928 - val_loss: 0.3917 - val_mcrmse: 0.2128\n",
            "Epoch 89/200\n",
            "60/60 - 6s - loss: 0.1786 - mcrmse: 0.1922 - val_loss: 0.3920 - val_mcrmse: 0.2134\n",
            "Epoch 90/200\n",
            "60/60 - 6s - loss: 0.1783 - mcrmse: 0.1918 - val_loss: 0.3915 - val_mcrmse: 0.2127\n",
            "Epoch 91/200\n",
            "60/60 - 6s - loss: 0.1771 - mcrmse: 0.1909 - val_loss: 0.3898 - val_mcrmse: 0.2118\n",
            "Epoch 92/200\n",
            "60/60 - 6s - loss: 0.1767 - mcrmse: 0.1903 - val_loss: 0.3901 - val_mcrmse: 0.2121\n",
            "Epoch 93/200\n",
            "60/60 - 6s - loss: 0.1762 - mcrmse: 0.1901 - val_loss: 0.3926 - val_mcrmse: 0.2131\n",
            "Epoch 94/200\n",
            "60/60 - 6s - loss: 0.1754 - mcrmse: 0.1895 - val_loss: 0.3930 - val_mcrmse: 0.2134\n",
            "Epoch 95/200\n",
            "60/60 - 6s - loss: 0.1749 - mcrmse: 0.1891 - val_loss: 0.3930 - val_mcrmse: 0.2135\n",
            "Epoch 96/200\n",
            "60/60 - 6s - loss: 0.1742 - mcrmse: 0.1882 - val_loss: 0.3935 - val_mcrmse: 0.2134\n",
            "Epoch 97/200\n",
            "60/60 - 6s - loss: 0.1738 - mcrmse: 0.1880 - val_loss: 0.3970 - val_mcrmse: 0.2150\n",
            "Epoch 98/200\n",
            "60/60 - 6s - loss: 0.1731 - mcrmse: 0.1874 - val_loss: 0.3924 - val_mcrmse: 0.2131\n",
            "Epoch 99/200\n",
            "\n",
            "Epoch 00099: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1722 - mcrmse: 0.1868 - val_loss: 0.3942 - val_mcrmse: 0.2141\n",
            "Epoch 00099: early stopping\n",
            "##### Type 0 Fold 0 #####\n",
            "Fold 0 validation loss=0.38555437326431274\n",
            "tf.Tensor(0.20938037, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 0 Fold 1 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.255600400000066 -0.17248410000000003\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7670 - mcrmse: 0.5913 - val_loss: 0.6530 - val_mcrmse: 0.3565\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4715 - mcrmse: 0.3957 - val_loss: 0.5636 - val_mcrmse: 0.3074\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4376 - mcrmse: 0.3721 - val_loss: 0.5385 - val_mcrmse: 0.2935\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4137 - mcrmse: 0.3559 - val_loss: 0.5011 - val_mcrmse: 0.2734\n",
            "Epoch 5/200\n",
            "60/60 - 6s - loss: 0.3967 - mcrmse: 0.3447 - val_loss: 0.5051 - val_mcrmse: 0.2747\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3790 - mcrmse: 0.3335 - val_loss: 0.4794 - val_mcrmse: 0.2619\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3629 - mcrmse: 0.3236 - val_loss: 0.4630 - val_mcrmse: 0.2532\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3550 - mcrmse: 0.3185 - val_loss: 0.4500 - val_mcrmse: 0.2458\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3447 - mcrmse: 0.3121 - val_loss: 0.4454 - val_mcrmse: 0.2428\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3410 - mcrmse: 0.3100 - val_loss: 0.4295 - val_mcrmse: 0.2345\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3313 - mcrmse: 0.3039 - val_loss: 0.4286 - val_mcrmse: 0.2339\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3243 - mcrmse: 0.2996 - val_loss: 0.4262 - val_mcrmse: 0.2324\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3182 - mcrmse: 0.2960 - val_loss: 0.4236 - val_mcrmse: 0.2308\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3118 - mcrmse: 0.2922 - val_loss: 0.4138 - val_mcrmse: 0.2271\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.3062 - mcrmse: 0.2886 - val_loss: 0.4124 - val_mcrmse: 0.2263\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.3034 - mcrmse: 0.2869 - val_loss: 0.4066 - val_mcrmse: 0.2226\n",
            "Epoch 17/200\n",
            "60/60 - 6s - loss: 0.2987 - mcrmse: 0.2841 - val_loss: 0.4099 - val_mcrmse: 0.2247\n",
            "Epoch 18/200\n",
            "60/60 - 6s - loss: 0.2925 - mcrmse: 0.2804 - val_loss: 0.4073 - val_mcrmse: 0.2227\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2895 - mcrmse: 0.2784 - val_loss: 0.4028 - val_mcrmse: 0.2200\n",
            "Epoch 20/200\n",
            "60/60 - 6s - loss: 0.2848 - mcrmse: 0.2756 - val_loss: 0.4038 - val_mcrmse: 0.2209\n",
            "Epoch 21/200\n",
            "60/60 - 6s - loss: 0.2799 - mcrmse: 0.2725 - val_loss: 0.4046 - val_mcrmse: 0.2218\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2770 - mcrmse: 0.2708 - val_loss: 0.4016 - val_mcrmse: 0.2194\n",
            "Epoch 23/200\n",
            "60/60 - 6s - loss: 0.2756 - mcrmse: 0.2698 - val_loss: 0.4030 - val_mcrmse: 0.2199\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2724 - mcrmse: 0.2679 - val_loss: 0.3981 - val_mcrmse: 0.2181\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2676 - mcrmse: 0.2651 - val_loss: 0.3969 - val_mcrmse: 0.2169\n",
            "Epoch 26/200\n",
            "60/60 - 6s - loss: 0.2644 - mcrmse: 0.2630 - val_loss: 0.4002 - val_mcrmse: 0.2198\n",
            "Epoch 27/200\n",
            "60/60 - 6s - loss: 0.2632 - mcrmse: 0.2623 - val_loss: 0.4027 - val_mcrmse: 0.2205\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2589 - mcrmse: 0.2597 - val_loss: 0.3937 - val_mcrmse: 0.2149\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2561 - mcrmse: 0.2580 - val_loss: 0.3940 - val_mcrmse: 0.2164\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2545 - mcrmse: 0.2569 - val_loss: 0.3974 - val_mcrmse: 0.2179\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2519 - mcrmse: 0.2553 - val_loss: 0.3945 - val_mcrmse: 0.2156\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2492 - mcrmse: 0.2537 - val_loss: 0.3922 - val_mcrmse: 0.2140\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2480 - mcrmse: 0.2530 - val_loss: 0.3911 - val_mcrmse: 0.2137\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2455 - mcrmse: 0.2514 - val_loss: 0.3947 - val_mcrmse: 0.2163\n",
            "Epoch 35/200\n",
            "60/60 - 6s - loss: 0.2433 - mcrmse: 0.2499 - val_loss: 0.3947 - val_mcrmse: 0.2156\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2407 - mcrmse: 0.2482 - val_loss: 0.3906 - val_mcrmse: 0.2146\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2386 - mcrmse: 0.2470 - val_loss: 0.3885 - val_mcrmse: 0.2133\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2369 - mcrmse: 0.2458 - val_loss: 0.3884 - val_mcrmse: 0.2122\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2354 - mcrmse: 0.2448 - val_loss: 0.3899 - val_mcrmse: 0.2133\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2336 - mcrmse: 0.2439 - val_loss: 0.3899 - val_mcrmse: 0.2134\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2325 - mcrmse: 0.2429 - val_loss: 0.3875 - val_mcrmse: 0.2121\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2307 - mcrmse: 0.2418 - val_loss: 0.3862 - val_mcrmse: 0.2108\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2278 - mcrmse: 0.2401 - val_loss: 0.3869 - val_mcrmse: 0.2113\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2272 - mcrmse: 0.2396 - val_loss: 0.3930 - val_mcrmse: 0.2157\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2246 - mcrmse: 0.2380 - val_loss: 0.3918 - val_mcrmse: 0.2142\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2239 - mcrmse: 0.2374 - val_loss: 0.3867 - val_mcrmse: 0.2120\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2226 - mcrmse: 0.2366 - val_loss: 0.3877 - val_mcrmse: 0.2117\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.2210 - mcrmse: 0.2355 - val_loss: 0.3858 - val_mcrmse: 0.2105\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.2184 - mcrmse: 0.2339 - val_loss: 0.3844 - val_mcrmse: 0.2099\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2167 - mcrmse: 0.2327 - val_loss: 0.3855 - val_mcrmse: 0.2105\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2163 - mcrmse: 0.2323 - val_loss: 0.3880 - val_mcrmse: 0.2120\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2151 - mcrmse: 0.2315 - val_loss: 0.3858 - val_mcrmse: 0.2108\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2140 - mcrmse: 0.2308 - val_loss: 0.3870 - val_mcrmse: 0.2113\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2124 - mcrmse: 0.2297 - val_loss: 0.3871 - val_mcrmse: 0.2113\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2111 - mcrmse: 0.2290 - val_loss: 0.3885 - val_mcrmse: 0.2124\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2110 - mcrmse: 0.2286 - val_loss: 0.3885 - val_mcrmse: 0.2119\n",
            "Epoch 57/200\n",
            "60/60 - 6s - loss: 0.2088 - mcrmse: 0.2273 - val_loss: 0.3877 - val_mcrmse: 0.2118\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2078 - mcrmse: 0.2267 - val_loss: 0.3882 - val_mcrmse: 0.2121\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.2064 - mcrmse: 0.2256 - val_loss: 0.3834 - val_mcrmse: 0.2094\n",
            "Epoch 60/200\n",
            "60/60 - 6s - loss: 0.2053 - mcrmse: 0.2249 - val_loss: 0.3848 - val_mcrmse: 0.2107\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2043 - mcrmse: 0.2242 - val_loss: 0.3857 - val_mcrmse: 0.2104\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.2029 - mcrmse: 0.2230 - val_loss: 0.3833 - val_mcrmse: 0.2098\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.2021 - mcrmse: 0.2226 - val_loss: 0.3885 - val_mcrmse: 0.2125\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.2013 - mcrmse: 0.2220 - val_loss: 0.3858 - val_mcrmse: 0.2112\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.2003 - mcrmse: 0.2212 - val_loss: 0.3867 - val_mcrmse: 0.2113\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1988 - mcrmse: 0.2202 - val_loss: 0.3875 - val_mcrmse: 0.2113\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1992 - mcrmse: 0.2202 - val_loss: 0.3937 - val_mcrmse: 0.2154\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1975 - mcrmse: 0.2191 - val_loss: 0.3857 - val_mcrmse: 0.2108\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1962 - mcrmse: 0.2183 - val_loss: 0.3838 - val_mcrmse: 0.2097\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1958 - mcrmse: 0.2179 - val_loss: 0.3877 - val_mcrmse: 0.2120\n",
            "Epoch 71/200\n",
            "60/60 - 6s - loss: 0.1959 - mcrmse: 0.2179 - val_loss: 0.3849 - val_mcrmse: 0.2102\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1933 - mcrmse: 0.2161 - val_loss: 0.3880 - val_mcrmse: 0.2123\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1930 - mcrmse: 0.2159 - val_loss: 0.3862 - val_mcrmse: 0.2108\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1916 - mcrmse: 0.2150 - val_loss: 0.3856 - val_mcrmse: 0.2105\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1912 - mcrmse: 0.2146 - val_loss: 0.3863 - val_mcrmse: 0.2110\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1908 - mcrmse: 0.2140 - val_loss: 0.3898 - val_mcrmse: 0.2128\n",
            "Epoch 77/200\n",
            "60/60 - 6s - loss: 0.1900 - mcrmse: 0.2134 - val_loss: 0.3844 - val_mcrmse: 0.2103\n",
            "Epoch 78/200\n",
            "60/60 - 6s - loss: 0.1883 - mcrmse: 0.2121 - val_loss: 0.3859 - val_mcrmse: 0.2115\n",
            "Epoch 79/200\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1871 - mcrmse: 0.2116 - val_loss: 0.3859 - val_mcrmse: 0.2109\n",
            "Epoch 00079: early stopping\n",
            "##### Type 0 Fold 1 #####\n",
            "Fold 1 validation loss=0.38329756259918213\n",
            "tf.Tensor(0.20942634, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 0 Fold 2 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2932 -0.1845\n",
            "Epoch 1/200\n",
            "60/60 - 9s - loss: 0.7601 - mcrmse: 0.5925 - val_loss: 0.6096 - val_mcrmse: 0.3368\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4667 - mcrmse: 0.3967 - val_loss: 0.5421 - val_mcrmse: 0.2972\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4255 - mcrmse: 0.3689 - val_loss: 0.5161 - val_mcrmse: 0.2813\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3978 - mcrmse: 0.3506 - val_loss: 0.5049 - val_mcrmse: 0.2739\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3776 - mcrmse: 0.3383 - val_loss: 0.4825 - val_mcrmse: 0.2621\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3631 - mcrmse: 0.3293 - val_loss: 0.4674 - val_mcrmse: 0.2542\n",
            "Epoch 7/200\n",
            "60/60 - 6s - loss: 0.3522 - mcrmse: 0.3227 - val_loss: 0.4761 - val_mcrmse: 0.2598\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3439 - mcrmse: 0.3177 - val_loss: 0.4516 - val_mcrmse: 0.2458\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3338 - mcrmse: 0.3117 - val_loss: 0.4379 - val_mcrmse: 0.2372\n",
            "Epoch 10/200\n",
            "60/60 - 6s - loss: 0.3266 - mcrmse: 0.3075 - val_loss: 0.4536 - val_mcrmse: 0.2455\n",
            "Epoch 11/200\n",
            "60/60 - 6s - loss: 0.3195 - mcrmse: 0.3030 - val_loss: 0.4418 - val_mcrmse: 0.2391\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3134 - mcrmse: 0.2997 - val_loss: 0.4273 - val_mcrmse: 0.2322\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3092 - mcrmse: 0.2970 - val_loss: 0.4249 - val_mcrmse: 0.2311\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3011 - mcrmse: 0.2919 - val_loss: 0.4191 - val_mcrmse: 0.2265\n",
            "Epoch 15/200\n",
            "60/60 - 6s - loss: 0.2984 - mcrmse: 0.2903 - val_loss: 0.4310 - val_mcrmse: 0.2326\n",
            "Epoch 16/200\n",
            "60/60 - 6s - loss: 0.2948 - mcrmse: 0.2883 - val_loss: 0.4203 - val_mcrmse: 0.2280\n",
            "Epoch 17/200\n",
            "60/60 - 6s - loss: 0.2918 - mcrmse: 0.2862 - val_loss: 0.4238 - val_mcrmse: 0.2287\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2847 - mcrmse: 0.2820 - val_loss: 0.4163 - val_mcrmse: 0.2241\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2819 - mcrmse: 0.2801 - val_loss: 0.4158 - val_mcrmse: 0.2237\n",
            "Epoch 20/200\n",
            "60/60 - 6s - loss: 0.2801 - mcrmse: 0.2791 - val_loss: 0.4159 - val_mcrmse: 0.2253\n",
            "Epoch 21/200\n",
            "60/60 - 6s - loss: 0.2742 - mcrmse: 0.2758 - val_loss: 0.4182 - val_mcrmse: 0.2252\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2705 - mcrmse: 0.2735 - val_loss: 0.4124 - val_mcrmse: 0.2219\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2676 - mcrmse: 0.2715 - val_loss: 0.4075 - val_mcrmse: 0.2198\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2651 - mcrmse: 0.2700 - val_loss: 0.4066 - val_mcrmse: 0.2194\n",
            "Epoch 25/200\n",
            "60/60 - 6s - loss: 0.2629 - mcrmse: 0.2687 - val_loss: 0.4163 - val_mcrmse: 0.2246\n",
            "Epoch 26/200\n",
            "60/60 - 6s - loss: 0.2618 - mcrmse: 0.2679 - val_loss: 0.4089 - val_mcrmse: 0.2204\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2590 - mcrmse: 0.2661 - val_loss: 0.4063 - val_mcrmse: 0.2192\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2560 - mcrmse: 0.2645 - val_loss: 0.4148 - val_mcrmse: 0.2227\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2538 - mcrmse: 0.2630 - val_loss: 0.4125 - val_mcrmse: 0.2239\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2499 - mcrmse: 0.2607 - val_loss: 0.4070 - val_mcrmse: 0.2190\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2484 - mcrmse: 0.2596 - val_loss: 0.4055 - val_mcrmse: 0.2182\n",
            "Epoch 32/200\n",
            "60/60 - 6s - loss: 0.2465 - mcrmse: 0.2583 - val_loss: 0.4080 - val_mcrmse: 0.2199\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2433 - mcrmse: 0.2565 - val_loss: 0.4030 - val_mcrmse: 0.2167\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2419 - mcrmse: 0.2557 - val_loss: 0.4061 - val_mcrmse: 0.2191\n",
            "Epoch 35/200\n",
            "60/60 - 6s - loss: 0.2389 - mcrmse: 0.2539 - val_loss: 0.4060 - val_mcrmse: 0.2187\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2373 - mcrmse: 0.2527 - val_loss: 0.4022 - val_mcrmse: 0.2168\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2358 - mcrmse: 0.2517 - val_loss: 0.4073 - val_mcrmse: 0.2196\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2348 - mcrmse: 0.2510 - val_loss: 0.4016 - val_mcrmse: 0.2165\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2330 - mcrmse: 0.2498 - val_loss: 0.4032 - val_mcrmse: 0.2170\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2301 - mcrmse: 0.2480 - val_loss: 0.4011 - val_mcrmse: 0.2155\n",
            "Epoch 41/200\n",
            "60/60 - 6s - loss: 0.2285 - mcrmse: 0.2468 - val_loss: 0.4015 - val_mcrmse: 0.2163\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2270 - mcrmse: 0.2461 - val_loss: 0.4025 - val_mcrmse: 0.2170\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2270 - mcrmse: 0.2458 - val_loss: 0.3996 - val_mcrmse: 0.2156\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2250 - mcrmse: 0.2444 - val_loss: 0.3989 - val_mcrmse: 0.2161\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.2223 - mcrmse: 0.2431 - val_loss: 0.3970 - val_mcrmse: 0.2140\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2208 - mcrmse: 0.2418 - val_loss: 0.4013 - val_mcrmse: 0.2172\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2194 - mcrmse: 0.2409 - val_loss: 0.4000 - val_mcrmse: 0.2154\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2198 - mcrmse: 0.2411 - val_loss: 0.4036 - val_mcrmse: 0.2170\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2167 - mcrmse: 0.2390 - val_loss: 0.3977 - val_mcrmse: 0.2145\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2147 - mcrmse: 0.2380 - val_loss: 0.3975 - val_mcrmse: 0.2142\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2149 - mcrmse: 0.2377 - val_loss: 0.3977 - val_mcrmse: 0.2148\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2140 - mcrmse: 0.2371 - val_loss: 0.3998 - val_mcrmse: 0.2157\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2118 - mcrmse: 0.2356 - val_loss: 0.4004 - val_mcrmse: 0.2166\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2104 - mcrmse: 0.2349 - val_loss: 0.4030 - val_mcrmse: 0.2177\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2098 - mcrmse: 0.2342 - val_loss: 0.4077 - val_mcrmse: 0.2199\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2086 - mcrmse: 0.2335 - val_loss: 0.3983 - val_mcrmse: 0.2148\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.2079 - mcrmse: 0.2329 - val_loss: 0.3946 - val_mcrmse: 0.2125\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2064 - mcrmse: 0.2319 - val_loss: 0.3979 - val_mcrmse: 0.2146\n",
            "Epoch 59/200\n",
            "60/60 - 6s - loss: 0.2044 - mcrmse: 0.2305 - val_loss: 0.3966 - val_mcrmse: 0.2141\n",
            "Epoch 60/200\n",
            "60/60 - 6s - loss: 0.2042 - mcrmse: 0.2303 - val_loss: 0.4014 - val_mcrmse: 0.2167\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2030 - mcrmse: 0.2293 - val_loss: 0.3980 - val_mcrmse: 0.2149\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.2021 - mcrmse: 0.2286 - val_loss: 0.3976 - val_mcrmse: 0.2146\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.2007 - mcrmse: 0.2277 - val_loss: 0.4011 - val_mcrmse: 0.2164\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.2011 - mcrmse: 0.2278 - val_loss: 0.4017 - val_mcrmse: 0.2174\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.1988 - mcrmse: 0.2263 - val_loss: 0.3940 - val_mcrmse: 0.2130\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1978 - mcrmse: 0.2256 - val_loss: 0.3958 - val_mcrmse: 0.2138\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1972 - mcrmse: 0.2253 - val_loss: 0.3983 - val_mcrmse: 0.2152\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1963 - mcrmse: 0.2243 - val_loss: 0.3995 - val_mcrmse: 0.2158\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1952 - mcrmse: 0.2236 - val_loss: 0.3984 - val_mcrmse: 0.2151\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1939 - mcrmse: 0.2228 - val_loss: 0.3993 - val_mcrmse: 0.2163\n",
            "Epoch 71/200\n",
            "60/60 - 6s - loss: 0.1932 - mcrmse: 0.2220 - val_loss: 0.3975 - val_mcrmse: 0.2150\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1923 - mcrmse: 0.2214 - val_loss: 0.3996 - val_mcrmse: 0.2159\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1913 - mcrmse: 0.2207 - val_loss: 0.4036 - val_mcrmse: 0.2185\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1904 - mcrmse: 0.2201 - val_loss: 0.3998 - val_mcrmse: 0.2167\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1891 - mcrmse: 0.2193 - val_loss: 0.3995 - val_mcrmse: 0.2162\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1892 - mcrmse: 0.2189 - val_loss: 0.4010 - val_mcrmse: 0.2168\n",
            "Epoch 77/200\n",
            "\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1884 - mcrmse: 0.2184 - val_loss: 0.4017 - val_mcrmse: 0.2167\n",
            "Epoch 00077: early stopping\n",
            "##### Type 0 Fold 2 #####\n",
            "Fold 2 validation loss=0.3939611613750458\n",
            "tf.Tensor(0.21250571, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 0 Fold 3 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.155131100000162 -0.18742859999999997\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7327 - mcrmse: 0.5613 - val_loss: 0.5981 - val_mcrmse: 0.3352\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4626 - mcrmse: 0.3819 - val_loss: 0.5676 - val_mcrmse: 0.3115\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4342 - mcrmse: 0.3620 - val_loss: 0.5243 - val_mcrmse: 0.2867\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4042 - mcrmse: 0.3424 - val_loss: 0.4988 - val_mcrmse: 0.2724\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3868 - mcrmse: 0.3311 - val_loss: 0.4767 - val_mcrmse: 0.2620\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3731 - mcrmse: 0.3220 - val_loss: 0.4582 - val_mcrmse: 0.2508\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3584 - mcrmse: 0.3128 - val_loss: 0.4557 - val_mcrmse: 0.2499\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3495 - mcrmse: 0.3074 - val_loss: 0.4504 - val_mcrmse: 0.2466\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3403 - mcrmse: 0.3015 - val_loss: 0.4338 - val_mcrmse: 0.2378\n",
            "Epoch 10/200\n",
            "60/60 - 6s - loss: 0.3306 - mcrmse: 0.2953 - val_loss: 0.4485 - val_mcrmse: 0.2440\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3285 - mcrmse: 0.2940 - val_loss: 0.4193 - val_mcrmse: 0.2294\n",
            "Epoch 12/200\n",
            "60/60 - 6s - loss: 0.3183 - mcrmse: 0.2878 - val_loss: 0.4216 - val_mcrmse: 0.2306\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3131 - mcrmse: 0.2845 - val_loss: 0.4067 - val_mcrmse: 0.2222\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3068 - mcrmse: 0.2808 - val_loss: 0.4060 - val_mcrmse: 0.2216\n",
            "Epoch 15/200\n",
            "60/60 - 6s - loss: 0.3023 - mcrmse: 0.2780 - val_loss: 0.4078 - val_mcrmse: 0.2223\n",
            "Epoch 16/200\n",
            "60/60 - 6s - loss: 0.2969 - mcrmse: 0.2746 - val_loss: 0.4107 - val_mcrmse: 0.2234\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2931 - mcrmse: 0.2722 - val_loss: 0.4062 - val_mcrmse: 0.2208\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2896 - mcrmse: 0.2699 - val_loss: 0.3968 - val_mcrmse: 0.2169\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2851 - mcrmse: 0.2671 - val_loss: 0.3960 - val_mcrmse: 0.2161\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2797 - mcrmse: 0.2639 - val_loss: 0.3941 - val_mcrmse: 0.2147\n",
            "Epoch 21/200\n",
            "60/60 - 6s - loss: 0.2772 - mcrmse: 0.2623 - val_loss: 0.4034 - val_mcrmse: 0.2202\n",
            "Epoch 22/200\n",
            "60/60 - 6s - loss: 0.2747 - mcrmse: 0.2605 - val_loss: 0.3949 - val_mcrmse: 0.2149\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2702 - mcrmse: 0.2578 - val_loss: 0.3885 - val_mcrmse: 0.2121\n",
            "Epoch 24/200\n",
            "60/60 - 6s - loss: 0.2670 - mcrmse: 0.2557 - val_loss: 0.3937 - val_mcrmse: 0.2145\n",
            "Epoch 25/200\n",
            "60/60 - 6s - loss: 0.2654 - mcrmse: 0.2545 - val_loss: 0.3934 - val_mcrmse: 0.2152\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2611 - mcrmse: 0.2521 - val_loss: 0.3871 - val_mcrmse: 0.2112\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2586 - mcrmse: 0.2504 - val_loss: 0.3849 - val_mcrmse: 0.2097\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2572 - mcrmse: 0.2495 - val_loss: 0.3917 - val_mcrmse: 0.2132\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2553 - mcrmse: 0.2481 - val_loss: 0.3880 - val_mcrmse: 0.2114\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2516 - mcrmse: 0.2458 - val_loss: 0.3863 - val_mcrmse: 0.2108\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2505 - mcrmse: 0.2450 - val_loss: 0.3853 - val_mcrmse: 0.2099\n",
            "Epoch 32/200\n",
            "60/60 - 6s - loss: 0.2481 - mcrmse: 0.2435 - val_loss: 0.3871 - val_mcrmse: 0.2112\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2453 - mcrmse: 0.2418 - val_loss: 0.3855 - val_mcrmse: 0.2104\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2432 - mcrmse: 0.2405 - val_loss: 0.3841 - val_mcrmse: 0.2099\n",
            "Epoch 35/200\n",
            "60/60 - 6s - loss: 0.2412 - mcrmse: 0.2392 - val_loss: 0.3895 - val_mcrmse: 0.2123\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2384 - mcrmse: 0.2374 - val_loss: 0.3840 - val_mcrmse: 0.2096\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2376 - mcrmse: 0.2367 - val_loss: 0.3846 - val_mcrmse: 0.2096\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2353 - mcrmse: 0.2353 - val_loss: 0.3880 - val_mcrmse: 0.2109\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2340 - mcrmse: 0.2344 - val_loss: 0.3853 - val_mcrmse: 0.2100\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2322 - mcrmse: 0.2331 - val_loss: 0.3828 - val_mcrmse: 0.2086\n",
            "Epoch 41/200\n",
            "60/60 - 6s - loss: 0.2305 - mcrmse: 0.2319 - val_loss: 0.3830 - val_mcrmse: 0.2090\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2285 - mcrmse: 0.2307 - val_loss: 0.3858 - val_mcrmse: 0.2106\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2276 - mcrmse: 0.2298 - val_loss: 0.3840 - val_mcrmse: 0.2098\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2258 - mcrmse: 0.2287 - val_loss: 0.3837 - val_mcrmse: 0.2096\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.2236 - mcrmse: 0.2273 - val_loss: 0.3812 - val_mcrmse: 0.2081\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2223 - mcrmse: 0.2263 - val_loss: 0.3857 - val_mcrmse: 0.2104\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.2205 - mcrmse: 0.2251 - val_loss: 0.3819 - val_mcrmse: 0.2079\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2191 - mcrmse: 0.2240 - val_loss: 0.3816 - val_mcrmse: 0.2080\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2178 - mcrmse: 0.2232 - val_loss: 0.3837 - val_mcrmse: 0.2099\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2167 - mcrmse: 0.2226 - val_loss: 0.3813 - val_mcrmse: 0.2083\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2155 - mcrmse: 0.2216 - val_loss: 0.3865 - val_mcrmse: 0.2110\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2148 - mcrmse: 0.2210 - val_loss: 0.3820 - val_mcrmse: 0.2084\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.2133 - mcrmse: 0.2201 - val_loss: 0.3806 - val_mcrmse: 0.2075\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2118 - mcrmse: 0.2189 - val_loss: 0.3849 - val_mcrmse: 0.2099\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2111 - mcrmse: 0.2182 - val_loss: 0.3815 - val_mcrmse: 0.2087\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2096 - mcrmse: 0.2174 - val_loss: 0.3808 - val_mcrmse: 0.2076\n",
            "Epoch 57/200\n",
            "60/60 - 6s - loss: 0.2087 - mcrmse: 0.2165 - val_loss: 0.3818 - val_mcrmse: 0.2083\n",
            "Epoch 58/200\n",
            "60/60 - 7s - loss: 0.2086 - mcrmse: 0.2166 - val_loss: 0.3792 - val_mcrmse: 0.2071\n",
            "Epoch 59/200\n",
            "60/60 - 6s - loss: 0.2069 - mcrmse: 0.2154 - val_loss: 0.3845 - val_mcrmse: 0.2108\n",
            "Epoch 60/200\n",
            "60/60 - 6s - loss: 0.2055 - mcrmse: 0.2146 - val_loss: 0.3822 - val_mcrmse: 0.2087\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2054 - mcrmse: 0.2142 - val_loss: 0.3826 - val_mcrmse: 0.2090\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.2043 - mcrmse: 0.2136 - val_loss: 0.3833 - val_mcrmse: 0.2094\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.2021 - mcrmse: 0.2120 - val_loss: 0.3799 - val_mcrmse: 0.2075\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.2015 - mcrmse: 0.2115 - val_loss: 0.3825 - val_mcrmse: 0.2095\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.2006 - mcrmse: 0.2109 - val_loss: 0.3819 - val_mcrmse: 0.2083\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1995 - mcrmse: 0.2101 - val_loss: 0.3837 - val_mcrmse: 0.2093\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1985 - mcrmse: 0.2093 - val_loss: 0.3842 - val_mcrmse: 0.2093\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1974 - mcrmse: 0.2084 - val_loss: 0.3856 - val_mcrmse: 0.2110\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1966 - mcrmse: 0.2078 - val_loss: 0.3818 - val_mcrmse: 0.2080\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1962 - mcrmse: 0.2074 - val_loss: 0.3855 - val_mcrmse: 0.2106\n",
            "Epoch 71/200\n",
            "60/60 - 6s - loss: 0.1956 - mcrmse: 0.2072 - val_loss: 0.3837 - val_mcrmse: 0.2093\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1938 - mcrmse: 0.2058 - val_loss: 0.3820 - val_mcrmse: 0.2085\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1940 - mcrmse: 0.2058 - val_loss: 0.3819 - val_mcrmse: 0.2080\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1926 - mcrmse: 0.2048 - val_loss: 0.3829 - val_mcrmse: 0.2086\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1915 - mcrmse: 0.2039 - val_loss: 0.3844 - val_mcrmse: 0.2095\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1910 - mcrmse: 0.2036 - val_loss: 0.3836 - val_mcrmse: 0.2093\n",
            "Epoch 77/200\n",
            "60/60 - 6s - loss: 0.1906 - mcrmse: 0.2031 - val_loss: 0.3826 - val_mcrmse: 0.2085\n",
            "Epoch 78/200\n",
            "\n",
            "Epoch 00078: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1893 - mcrmse: 0.2023 - val_loss: 0.3859 - val_mcrmse: 0.2101\n",
            "Epoch 00078: early stopping\n",
            "##### Type 0 Fold 3 #####\n",
            "Fold 3 validation loss=0.37915655970573425\n",
            "tf.Tensor(0.20714322, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 0 Fold 4 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2344 -0.186\n",
            "Epoch 1/200\n",
            "60/60 - 9s - loss: 0.7347 - mcrmse: 0.5801 - val_loss: 0.6254 - val_mcrmse: 0.3445\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4658 - mcrmse: 0.3936 - val_loss: 0.5482 - val_mcrmse: 0.2993\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4254 - mcrmse: 0.3662 - val_loss: 0.5192 - val_mcrmse: 0.2812\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4003 - mcrmse: 0.3499 - val_loss: 0.4980 - val_mcrmse: 0.2689\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3813 - mcrmse: 0.3375 - val_loss: 0.4841 - val_mcrmse: 0.2624\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3675 - mcrmse: 0.3285 - val_loss: 0.4684 - val_mcrmse: 0.2517\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3563 - mcrmse: 0.3216 - val_loss: 0.4479 - val_mcrmse: 0.2420\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3459 - mcrmse: 0.3156 - val_loss: 0.4387 - val_mcrmse: 0.2359\n",
            "Epoch 9/200\n",
            "60/60 - 6s - loss: 0.3382 - mcrmse: 0.3106 - val_loss: 0.4376 - val_mcrmse: 0.2365\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3302 - mcrmse: 0.3055 - val_loss: 0.4354 - val_mcrmse: 0.2341\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3222 - mcrmse: 0.3010 - val_loss: 0.4282 - val_mcrmse: 0.2310\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3152 - mcrmse: 0.2967 - val_loss: 0.4150 - val_mcrmse: 0.2237\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3097 - mcrmse: 0.2932 - val_loss: 0.4150 - val_mcrmse: 0.2230\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3041 - mcrmse: 0.2897 - val_loss: 0.4122 - val_mcrmse: 0.2216\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2995 - mcrmse: 0.2869 - val_loss: 0.4046 - val_mcrmse: 0.2181\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2941 - mcrmse: 0.2835 - val_loss: 0.4012 - val_mcrmse: 0.2164\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2900 - mcrmse: 0.2811 - val_loss: 0.3950 - val_mcrmse: 0.2131\n",
            "Epoch 18/200\n",
            "60/60 - 6s - loss: 0.2848 - mcrmse: 0.2780 - val_loss: 0.3981 - val_mcrmse: 0.2145\n",
            "Epoch 19/200\n",
            "60/60 - 6s - loss: 0.2818 - mcrmse: 0.2760 - val_loss: 0.3974 - val_mcrmse: 0.2138\n",
            "Epoch 20/200\n",
            "60/60 - 6s - loss: 0.2788 - mcrmse: 0.2740 - val_loss: 0.3990 - val_mcrmse: 0.2152\n",
            "Epoch 21/200\n",
            "60/60 - 6s - loss: 0.2753 - mcrmse: 0.2719 - val_loss: 0.3992 - val_mcrmse: 0.2148\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2736 - mcrmse: 0.2707 - val_loss: 0.3890 - val_mcrmse: 0.2096\n",
            "Epoch 23/200\n",
            "60/60 - 6s - loss: 0.2683 - mcrmse: 0.2676 - val_loss: 0.3969 - val_mcrmse: 0.2141\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2647 - mcrmse: 0.2654 - val_loss: 0.3884 - val_mcrmse: 0.2093\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2601 - mcrmse: 0.2624 - val_loss: 0.3862 - val_mcrmse: 0.2084\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2591 - mcrmse: 0.2619 - val_loss: 0.3849 - val_mcrmse: 0.2073\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2557 - mcrmse: 0.2597 - val_loss: 0.3827 - val_mcrmse: 0.2060\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2528 - mcrmse: 0.2578 - val_loss: 0.3819 - val_mcrmse: 0.2057\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2523 - mcrmse: 0.2575 - val_loss: 0.3842 - val_mcrmse: 0.2067\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2477 - mcrmse: 0.2547 - val_loss: 0.3844 - val_mcrmse: 0.2076\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2465 - mcrmse: 0.2537 - val_loss: 0.3823 - val_mcrmse: 0.2058\n",
            "Epoch 32/200\n",
            "60/60 - 6s - loss: 0.2451 - mcrmse: 0.2529 - val_loss: 0.3855 - val_mcrmse: 0.2074\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2420 - mcrmse: 0.2509 - val_loss: 0.3836 - val_mcrmse: 0.2067\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2400 - mcrmse: 0.2496 - val_loss: 0.3843 - val_mcrmse: 0.2067\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2384 - mcrmse: 0.2485 - val_loss: 0.3809 - val_mcrmse: 0.2052\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2354 - mcrmse: 0.2467 - val_loss: 0.3832 - val_mcrmse: 0.2061\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2337 - mcrmse: 0.2457 - val_loss: 0.3828 - val_mcrmse: 0.2062\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2327 - mcrmse: 0.2447 - val_loss: 0.3840 - val_mcrmse: 0.2072\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2311 - mcrmse: 0.2436 - val_loss: 0.3781 - val_mcrmse: 0.2036\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2276 - mcrmse: 0.2415 - val_loss: 0.3815 - val_mcrmse: 0.2058\n",
            "Epoch 41/200\n",
            "60/60 - 6s - loss: 0.2269 - mcrmse: 0.2410 - val_loss: 0.3813 - val_mcrmse: 0.2058\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2253 - mcrmse: 0.2399 - val_loss: 0.3761 - val_mcrmse: 0.2027\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2242 - mcrmse: 0.2391 - val_loss: 0.3791 - val_mcrmse: 0.2042\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2224 - mcrmse: 0.2379 - val_loss: 0.3792 - val_mcrmse: 0.2042\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2210 - mcrmse: 0.2368 - val_loss: 0.3781 - val_mcrmse: 0.2037\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.2192 - mcrmse: 0.2357 - val_loss: 0.3749 - val_mcrmse: 0.2019\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2178 - mcrmse: 0.2348 - val_loss: 0.3764 - val_mcrmse: 0.2030\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2176 - mcrmse: 0.2346 - val_loss: 0.3762 - val_mcrmse: 0.2025\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2147 - mcrmse: 0.2327 - val_loss: 0.3778 - val_mcrmse: 0.2036\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2142 - mcrmse: 0.2322 - val_loss: 0.3804 - val_mcrmse: 0.2051\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2127 - mcrmse: 0.2312 - val_loss: 0.3780 - val_mcrmse: 0.2037\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2110 - mcrmse: 0.2300 - val_loss: 0.3764 - val_mcrmse: 0.2026\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2105 - mcrmse: 0.2296 - val_loss: 0.3772 - val_mcrmse: 0.2036\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2092 - mcrmse: 0.2287 - val_loss: 0.3791 - val_mcrmse: 0.2045\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2076 - mcrmse: 0.2275 - val_loss: 0.3746 - val_mcrmse: 0.2021\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2066 - mcrmse: 0.2269 - val_loss: 0.3789 - val_mcrmse: 0.2040\n",
            "Epoch 57/200\n",
            "60/60 - 6s - loss: 0.2055 - mcrmse: 0.2263 - val_loss: 0.3762 - val_mcrmse: 0.2026\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2042 - mcrmse: 0.2252 - val_loss: 0.3770 - val_mcrmse: 0.2042\n",
            "Epoch 59/200\n",
            "60/60 - 6s - loss: 0.2030 - mcrmse: 0.2243 - val_loss: 0.3767 - val_mcrmse: 0.2032\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.2019 - mcrmse: 0.2236 - val_loss: 0.3740 - val_mcrmse: 0.2017\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2004 - mcrmse: 0.2225 - val_loss: 0.3760 - val_mcrmse: 0.2025\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.2019 - mcrmse: 0.2232 - val_loss: 0.3769 - val_mcrmse: 0.2032\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.1999 - mcrmse: 0.2219 - val_loss: 0.3776 - val_mcrmse: 0.2036\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.1981 - mcrmse: 0.2207 - val_loss: 0.3754 - val_mcrmse: 0.2029\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.1964 - mcrmse: 0.2196 - val_loss: 0.3780 - val_mcrmse: 0.2036\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1960 - mcrmse: 0.2192 - val_loss: 0.3757 - val_mcrmse: 0.2028\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1956 - mcrmse: 0.2188 - val_loss: 0.3762 - val_mcrmse: 0.2033\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1936 - mcrmse: 0.2176 - val_loss: 0.3769 - val_mcrmse: 0.2035\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1939 - mcrmse: 0.2175 - val_loss: 0.3752 - val_mcrmse: 0.2026\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1931 - mcrmse: 0.2169 - val_loss: 0.3768 - val_mcrmse: 0.2036\n",
            "Epoch 71/200\n",
            "60/60 - 6s - loss: 0.1922 - mcrmse: 0.2162 - val_loss: 0.3774 - val_mcrmse: 0.2038\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1915 - mcrmse: 0.2157 - val_loss: 0.3782 - val_mcrmse: 0.2040\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1904 - mcrmse: 0.2150 - val_loss: 0.3777 - val_mcrmse: 0.2043\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1896 - mcrmse: 0.2142 - val_loss: 0.3774 - val_mcrmse: 0.2039\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1883 - mcrmse: 0.2135 - val_loss: 0.3751 - val_mcrmse: 0.2023\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1875 - mcrmse: 0.2128 - val_loss: 0.3819 - val_mcrmse: 0.2075\n",
            "Epoch 77/200\n",
            "60/60 - 6s - loss: 0.1872 - mcrmse: 0.2127 - val_loss: 0.3758 - val_mcrmse: 0.2026\n",
            "Epoch 78/200\n",
            "60/60 - 6s - loss: 0.1857 - mcrmse: 0.2113 - val_loss: 0.3757 - val_mcrmse: 0.2027\n",
            "Epoch 79/200\n",
            "60/60 - 6s - loss: 0.1853 - mcrmse: 0.2113 - val_loss: 0.3757 - val_mcrmse: 0.2027\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1846 - mcrmse: 0.2105 - val_loss: 0.3758 - val_mcrmse: 0.2029\n",
            "Epoch 00080: early stopping\n",
            "##### Type 0 Fold 4 #####\n",
            "Fold 4 validation loss=0.37395888566970825\n",
            "tf.Tensor(0.20168716, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 1 Fold 0 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.1759 -0.1961\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7174 - mcrmse: 0.5431 - val_loss: 0.6274 - val_mcrmse: 0.3504\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4806 - mcrmse: 0.3850 - val_loss: 0.5581 - val_mcrmse: 0.3037\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4442 - mcrmse: 0.3607 - val_loss: 0.5062 - val_mcrmse: 0.2762\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4104 - mcrmse: 0.3399 - val_loss: 0.4858 - val_mcrmse: 0.2652\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3891 - mcrmse: 0.3265 - val_loss: 0.4680 - val_mcrmse: 0.2559\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3724 - mcrmse: 0.3166 - val_loss: 0.4461 - val_mcrmse: 0.2431\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3601 - mcrmse: 0.3088 - val_loss: 0.4370 - val_mcrmse: 0.2383\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3479 - mcrmse: 0.3014 - val_loss: 0.4299 - val_mcrmse: 0.2328\n",
            "Epoch 9/200\n",
            "60/60 - 6s - loss: 0.3375 - mcrmse: 0.2950 - val_loss: 0.4315 - val_mcrmse: 0.2346\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3337 - mcrmse: 0.2928 - val_loss: 0.4250 - val_mcrmse: 0.2300\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3250 - mcrmse: 0.2875 - val_loss: 0.4133 - val_mcrmse: 0.2246\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3172 - mcrmse: 0.2829 - val_loss: 0.4103 - val_mcrmse: 0.2236\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3104 - mcrmse: 0.2786 - val_loss: 0.4082 - val_mcrmse: 0.2215\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3054 - mcrmse: 0.2757 - val_loss: 0.4078 - val_mcrmse: 0.2211\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.3007 - mcrmse: 0.2726 - val_loss: 0.4049 - val_mcrmse: 0.2198\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2981 - mcrmse: 0.2711 - val_loss: 0.3990 - val_mcrmse: 0.2168\n",
            "Epoch 17/200\n",
            "60/60 - 6s - loss: 0.2918 - mcrmse: 0.2674 - val_loss: 0.3999 - val_mcrmse: 0.2174\n",
            "Epoch 18/200\n",
            "60/60 - 6s - loss: 0.2877 - mcrmse: 0.2648 - val_loss: 0.4075 - val_mcrmse: 0.2209\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2838 - mcrmse: 0.2623 - val_loss: 0.3960 - val_mcrmse: 0.2152\n",
            "Epoch 20/200\n",
            "60/60 - 6s - loss: 0.2791 - mcrmse: 0.2594 - val_loss: 0.3992 - val_mcrmse: 0.2172\n",
            "Epoch 21/200\n",
            "60/60 - 6s - loss: 0.2760 - mcrmse: 0.2575 - val_loss: 0.3959 - val_mcrmse: 0.2155\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2726 - mcrmse: 0.2554 - val_loss: 0.3944 - val_mcrmse: 0.2148\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2703 - mcrmse: 0.2540 - val_loss: 0.3919 - val_mcrmse: 0.2131\n",
            "Epoch 24/200\n",
            "60/60 - 6s - loss: 0.2668 - mcrmse: 0.2520 - val_loss: 0.3970 - val_mcrmse: 0.2153\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2644 - mcrmse: 0.2503 - val_loss: 0.3905 - val_mcrmse: 0.2126\n",
            "Epoch 26/200\n",
            "60/60 - 6s - loss: 0.2609 - mcrmse: 0.2482 - val_loss: 0.3929 - val_mcrmse: 0.2138\n",
            "Epoch 27/200\n",
            "60/60 - 6s - loss: 0.2578 - mcrmse: 0.2463 - val_loss: 0.3937 - val_mcrmse: 0.2139\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2560 - mcrmse: 0.2451 - val_loss: 0.3963 - val_mcrmse: 0.2150\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2542 - mcrmse: 0.2439 - val_loss: 0.3933 - val_mcrmse: 0.2149\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2503 - mcrmse: 0.2416 - val_loss: 0.3903 - val_mcrmse: 0.2132\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2478 - mcrmse: 0.2399 - val_loss: 0.3925 - val_mcrmse: 0.2134\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2454 - mcrmse: 0.2384 - val_loss: 0.3854 - val_mcrmse: 0.2098\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2434 - mcrmse: 0.2370 - val_loss: 0.3903 - val_mcrmse: 0.2129\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2421 - mcrmse: 0.2363 - val_loss: 0.3901 - val_mcrmse: 0.2124\n",
            "Epoch 35/200\n",
            "60/60 - 6s - loss: 0.2399 - mcrmse: 0.2349 - val_loss: 0.3890 - val_mcrmse: 0.2113\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2374 - mcrmse: 0.2333 - val_loss: 0.3915 - val_mcrmse: 0.2126\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2351 - mcrmse: 0.2317 - val_loss: 0.3913 - val_mcrmse: 0.2127\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2346 - mcrmse: 0.2314 - val_loss: 0.3908 - val_mcrmse: 0.2125\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2308 - mcrmse: 0.2292 - val_loss: 0.3943 - val_mcrmse: 0.2141\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2302 - mcrmse: 0.2284 - val_loss: 0.3891 - val_mcrmse: 0.2126\n",
            "Epoch 41/200\n",
            "60/60 - 6s - loss: 0.2278 - mcrmse: 0.2271 - val_loss: 0.3953 - val_mcrmse: 0.2145\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2255 - mcrmse: 0.2255 - val_loss: 0.3929 - val_mcrmse: 0.2137\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2243 - mcrmse: 0.2246 - val_loss: 0.3956 - val_mcrmse: 0.2147\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2234 - mcrmse: 0.2240 - val_loss: 0.3942 - val_mcrmse: 0.2152\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2213 - mcrmse: 0.2226 - val_loss: 0.3988 - val_mcrmse: 0.2169\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2206 - mcrmse: 0.2220 - val_loss: 0.3892 - val_mcrmse: 0.2112\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2186 - mcrmse: 0.2206 - val_loss: 0.3924 - val_mcrmse: 0.2134\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2164 - mcrmse: 0.2193 - val_loss: 0.3931 - val_mcrmse: 0.2140\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2171 - mcrmse: 0.2195 - val_loss: 0.3922 - val_mcrmse: 0.2133\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2145 - mcrmse: 0.2179 - val_loss: 0.3967 - val_mcrmse: 0.2154\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2131 - mcrmse: 0.2171 - val_loss: 0.3923 - val_mcrmse: 0.2131\n",
            "Epoch 52/200\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.2114 - mcrmse: 0.2157 - val_loss: 0.3981 - val_mcrmse: 0.2165\n",
            "Epoch 00052: early stopping\n",
            "##### Type 1 Fold 0 #####\n",
            "Fold 0 validation loss=0.385391503572464\n",
            "tf.Tensor(0.20976613, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 1 Fold 1 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.255600400000066 -0.17248410000000003\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7299 - mcrmse: 0.5703 - val_loss: 0.5980 - val_mcrmse: 0.3335\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4699 - mcrmse: 0.3934 - val_loss: 0.5328 - val_mcrmse: 0.2946\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4314 - mcrmse: 0.3665 - val_loss: 0.5159 - val_mcrmse: 0.2816\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4049 - mcrmse: 0.3497 - val_loss: 0.4808 - val_mcrmse: 0.2632\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3811 - mcrmse: 0.3351 - val_loss: 0.4598 - val_mcrmse: 0.2532\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3684 - mcrmse: 0.3264 - val_loss: 0.4539 - val_mcrmse: 0.2482\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3554 - mcrmse: 0.3184 - val_loss: 0.4384 - val_mcrmse: 0.2412\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3448 - mcrmse: 0.3120 - val_loss: 0.4343 - val_mcrmse: 0.2381\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3350 - mcrmse: 0.3059 - val_loss: 0.4252 - val_mcrmse: 0.2360\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3268 - mcrmse: 0.3007 - val_loss: 0.4165 - val_mcrmse: 0.2296\n",
            "Epoch 11/200\n",
            "60/60 - 6s - loss: 0.3191 - mcrmse: 0.2960 - val_loss: 0.4181 - val_mcrmse: 0.2313\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3158 - mcrmse: 0.2940 - val_loss: 0.4108 - val_mcrmse: 0.2254\n",
            "Epoch 13/200\n",
            "60/60 - 6s - loss: 0.3072 - mcrmse: 0.2891 - val_loss: 0.4139 - val_mcrmse: 0.2267\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3017 - mcrmse: 0.2857 - val_loss: 0.4048 - val_mcrmse: 0.2220\n",
            "Epoch 15/200\n",
            "60/60 - 6s - loss: 0.2966 - mcrmse: 0.2823 - val_loss: 0.4112 - val_mcrmse: 0.2252\n",
            "Epoch 16/200\n",
            "60/60 - 6s - loss: 0.2929 - mcrmse: 0.2806 - val_loss: 0.4108 - val_mcrmse: 0.2266\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2883 - mcrmse: 0.2776 - val_loss: 0.3978 - val_mcrmse: 0.2185\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2830 - mcrmse: 0.2743 - val_loss: 0.4063 - val_mcrmse: 0.2229\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2793 - mcrmse: 0.2721 - val_loss: 0.3981 - val_mcrmse: 0.2181\n",
            "Epoch 20/200\n",
            "60/60 - 6s - loss: 0.2759 - mcrmse: 0.2698 - val_loss: 0.4041 - val_mcrmse: 0.2216\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2719 - mcrmse: 0.2676 - val_loss: 0.3967 - val_mcrmse: 0.2176\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2680 - mcrmse: 0.2651 - val_loss: 0.3960 - val_mcrmse: 0.2177\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2669 - mcrmse: 0.2645 - val_loss: 0.3943 - val_mcrmse: 0.2161\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2621 - mcrmse: 0.2618 - val_loss: 0.3907 - val_mcrmse: 0.2138\n",
            "Epoch 25/200\n",
            "60/60 - 6s - loss: 0.2598 - mcrmse: 0.2603 - val_loss: 0.3937 - val_mcrmse: 0.2164\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2582 - mcrmse: 0.2590 - val_loss: 0.3911 - val_mcrmse: 0.2137\n",
            "Epoch 27/200\n",
            "60/60 - 6s - loss: 0.2542 - mcrmse: 0.2567 - val_loss: 0.3961 - val_mcrmse: 0.2170\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2520 - mcrmse: 0.2553 - val_loss: 0.3924 - val_mcrmse: 0.2161\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2507 - mcrmse: 0.2545 - val_loss: 0.3912 - val_mcrmse: 0.2142\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2457 - mcrmse: 0.2512 - val_loss: 0.3874 - val_mcrmse: 0.2125\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2441 - mcrmse: 0.2503 - val_loss: 0.3900 - val_mcrmse: 0.2136\n",
            "Epoch 32/200\n",
            "60/60 - 6s - loss: 0.2423 - mcrmse: 0.2492 - val_loss: 0.3893 - val_mcrmse: 0.2135\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2405 - mcrmse: 0.2481 - val_loss: 0.3915 - val_mcrmse: 0.2141\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2389 - mcrmse: 0.2471 - val_loss: 0.3912 - val_mcrmse: 0.2141\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2372 - mcrmse: 0.2460 - val_loss: 0.3863 - val_mcrmse: 0.2120\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2349 - mcrmse: 0.2446 - val_loss: 0.3844 - val_mcrmse: 0.2103\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2327 - mcrmse: 0.2432 - val_loss: 0.3884 - val_mcrmse: 0.2129\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2316 - mcrmse: 0.2423 - val_loss: 0.3896 - val_mcrmse: 0.2145\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2290 - mcrmse: 0.2409 - val_loss: 0.3873 - val_mcrmse: 0.2124\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2277 - mcrmse: 0.2398 - val_loss: 0.3872 - val_mcrmse: 0.2124\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2264 - mcrmse: 0.2389 - val_loss: 0.3840 - val_mcrmse: 0.2099\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2240 - mcrmse: 0.2374 - val_loss: 0.3862 - val_mcrmse: 0.2118\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2216 - mcrmse: 0.2357 - val_loss: 0.3863 - val_mcrmse: 0.2108\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2214 - mcrmse: 0.2356 - val_loss: 0.3859 - val_mcrmse: 0.2111\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2203 - mcrmse: 0.2349 - val_loss: 0.3871 - val_mcrmse: 0.2113\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.2180 - mcrmse: 0.2335 - val_loss: 0.3827 - val_mcrmse: 0.2099\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2167 - mcrmse: 0.2326 - val_loss: 0.3839 - val_mcrmse: 0.2105\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.2152 - mcrmse: 0.2316 - val_loss: 0.3820 - val_mcrmse: 0.2089\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2143 - mcrmse: 0.2309 - val_loss: 0.3832 - val_mcrmse: 0.2092\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2132 - mcrmse: 0.2303 - val_loss: 0.3823 - val_mcrmse: 0.2095\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2108 - mcrmse: 0.2285 - val_loss: 0.3841 - val_mcrmse: 0.2100\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2106 - mcrmse: 0.2284 - val_loss: 0.3849 - val_mcrmse: 0.2105\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2094 - mcrmse: 0.2275 - val_loss: 0.3818 - val_mcrmse: 0.2091\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.2073 - mcrmse: 0.2262 - val_loss: 0.3818 - val_mcrmse: 0.2085\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2064 - mcrmse: 0.2255 - val_loss: 0.3861 - val_mcrmse: 0.2116\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2056 - mcrmse: 0.2250 - val_loss: 0.3823 - val_mcrmse: 0.2096\n",
            "Epoch 57/200\n",
            "60/60 - 6s - loss: 0.2042 - mcrmse: 0.2241 - val_loss: 0.3860 - val_mcrmse: 0.2120\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2034 - mcrmse: 0.2235 - val_loss: 0.3818 - val_mcrmse: 0.2088\n",
            "Epoch 59/200\n",
            "60/60 - 6s - loss: 0.2031 - mcrmse: 0.2233 - val_loss: 0.3825 - val_mcrmse: 0.2089\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.2003 - mcrmse: 0.2215 - val_loss: 0.3806 - val_mcrmse: 0.2078\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2000 - mcrmse: 0.2214 - val_loss: 0.3830 - val_mcrmse: 0.2091\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.1990 - mcrmse: 0.2206 - val_loss: 0.3839 - val_mcrmse: 0.2105\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.1989 - mcrmse: 0.2202 - val_loss: 0.3848 - val_mcrmse: 0.2101\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.1976 - mcrmse: 0.2195 - val_loss: 0.3814 - val_mcrmse: 0.2086\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.1961 - mcrmse: 0.2185 - val_loss: 0.3838 - val_mcrmse: 0.2099\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1950 - mcrmse: 0.2177 - val_loss: 0.3828 - val_mcrmse: 0.2092\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1943 - mcrmse: 0.2171 - val_loss: 0.3822 - val_mcrmse: 0.2087\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1933 - mcrmse: 0.2165 - val_loss: 0.3816 - val_mcrmse: 0.2088\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1924 - mcrmse: 0.2157 - val_loss: 0.3819 - val_mcrmse: 0.2090\n",
            "Epoch 70/200\n",
            "60/60 - 7s - loss: 0.1919 - mcrmse: 0.2154 - val_loss: 0.3798 - val_mcrmse: 0.2082\n",
            "Epoch 71/200\n",
            "60/60 - 6s - loss: 0.1905 - mcrmse: 0.2145 - val_loss: 0.3838 - val_mcrmse: 0.2097\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1896 - mcrmse: 0.2139 - val_loss: 0.3800 - val_mcrmse: 0.2079\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1897 - mcrmse: 0.2138 - val_loss: 0.3847 - val_mcrmse: 0.2103\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1892 - mcrmse: 0.2133 - val_loss: 0.3855 - val_mcrmse: 0.2110\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1877 - mcrmse: 0.2124 - val_loss: 0.3827 - val_mcrmse: 0.2095\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1859 - mcrmse: 0.2113 - val_loss: 0.3825 - val_mcrmse: 0.2089\n",
            "Epoch 77/200\n",
            "60/60 - 6s - loss: 0.1857 - mcrmse: 0.2109 - val_loss: 0.3810 - val_mcrmse: 0.2082\n",
            "Epoch 78/200\n",
            "60/60 - 6s - loss: 0.1855 - mcrmse: 0.2107 - val_loss: 0.3815 - val_mcrmse: 0.2086\n",
            "Epoch 79/200\n",
            "60/60 - 6s - loss: 0.1850 - mcrmse: 0.2106 - val_loss: 0.3838 - val_mcrmse: 0.2102\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1835 - mcrmse: 0.2094 - val_loss: 0.3832 - val_mcrmse: 0.2098\n",
            "Epoch 00080: early stopping\n",
            "##### Type 1 Fold 1 #####\n",
            "Fold 1 validation loss=0.3798370957374573\n",
            "tf.Tensor(0.20778513, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 1 Fold 2 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2932 -0.1845\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7753 - mcrmse: 0.6043 - val_loss: 0.5943 - val_mcrmse: 0.3316\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4758 - mcrmse: 0.4023 - val_loss: 0.5708 - val_mcrmse: 0.3135\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4341 - mcrmse: 0.3742 - val_loss: 0.5225 - val_mcrmse: 0.2857\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4070 - mcrmse: 0.3572 - val_loss: 0.4835 - val_mcrmse: 0.2673\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3835 - mcrmse: 0.3425 - val_loss: 0.4737 - val_mcrmse: 0.2611\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3702 - mcrmse: 0.3341 - val_loss: 0.4597 - val_mcrmse: 0.2512\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3597 - mcrmse: 0.3276 - val_loss: 0.4493 - val_mcrmse: 0.2444\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3492 - mcrmse: 0.3211 - val_loss: 0.4422 - val_mcrmse: 0.2402\n",
            "Epoch 9/200\n",
            "60/60 - 6s - loss: 0.3377 - mcrmse: 0.3140 - val_loss: 0.4513 - val_mcrmse: 0.2486\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3311 - mcrmse: 0.3101 - val_loss: 0.4319 - val_mcrmse: 0.2351\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3229 - mcrmse: 0.3051 - val_loss: 0.4302 - val_mcrmse: 0.2336\n",
            "Epoch 12/200\n",
            "60/60 - 6s - loss: 0.3140 - mcrmse: 0.2997 - val_loss: 0.4377 - val_mcrmse: 0.2368\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3084 - mcrmse: 0.2962 - val_loss: 0.4253 - val_mcrmse: 0.2317\n",
            "Epoch 14/200\n",
            "60/60 - 6s - loss: 0.3036 - mcrmse: 0.2934 - val_loss: 0.4282 - val_mcrmse: 0.2338\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2978 - mcrmse: 0.2901 - val_loss: 0.4217 - val_mcrmse: 0.2281\n",
            "Epoch 16/200\n",
            "60/60 - 6s - loss: 0.2946 - mcrmse: 0.2882 - val_loss: 0.4252 - val_mcrmse: 0.2311\n",
            "Epoch 17/200\n",
            "60/60 - 6s - loss: 0.2886 - mcrmse: 0.2845 - val_loss: 0.4230 - val_mcrmse: 0.2289\n",
            "Epoch 18/200\n",
            "60/60 - 6s - loss: 0.2848 - mcrmse: 0.2821 - val_loss: 0.4224 - val_mcrmse: 0.2285\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2812 - mcrmse: 0.2799 - val_loss: 0.4295 - val_mcrmse: 0.2324\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2775 - mcrmse: 0.2778 - val_loss: 0.4126 - val_mcrmse: 0.2231\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2736 - mcrmse: 0.2753 - val_loss: 0.4124 - val_mcrmse: 0.2229\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2699 - mcrmse: 0.2730 - val_loss: 0.4122 - val_mcrmse: 0.2224\n",
            "Epoch 23/200\n",
            "60/60 - 6s - loss: 0.2665 - mcrmse: 0.2709 - val_loss: 0.4205 - val_mcrmse: 0.2267\n",
            "Epoch 24/200\n",
            "60/60 - 6s - loss: 0.2637 - mcrmse: 0.2689 - val_loss: 0.4136 - val_mcrmse: 0.2230\n",
            "Epoch 25/200\n",
            "60/60 - 6s - loss: 0.2607 - mcrmse: 0.2671 - val_loss: 0.4118 - val_mcrmse: 0.2226\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2574 - mcrmse: 0.2651 - val_loss: 0.4097 - val_mcrmse: 0.2208\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2545 - mcrmse: 0.2634 - val_loss: 0.4069 - val_mcrmse: 0.2194\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2518 - mcrmse: 0.2617 - val_loss: 0.4069 - val_mcrmse: 0.2193\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2498 - mcrmse: 0.2604 - val_loss: 0.4127 - val_mcrmse: 0.2224\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2469 - mcrmse: 0.2587 - val_loss: 0.4040 - val_mcrmse: 0.2181\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2457 - mcrmse: 0.2580 - val_loss: 0.4115 - val_mcrmse: 0.2222\n",
            "Epoch 32/200\n",
            "60/60 - 6s - loss: 0.2429 - mcrmse: 0.2562 - val_loss: 0.4110 - val_mcrmse: 0.2217\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2409 - mcrmse: 0.2548 - val_loss: 0.4124 - val_mcrmse: 0.2229\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2383 - mcrmse: 0.2532 - val_loss: 0.4063 - val_mcrmse: 0.2192\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2378 - mcrmse: 0.2527 - val_loss: 0.4064 - val_mcrmse: 0.2195\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2346 - mcrmse: 0.2509 - val_loss: 0.4088 - val_mcrmse: 0.2205\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2329 - mcrmse: 0.2497 - val_loss: 0.4086 - val_mcrmse: 0.2205\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2315 - mcrmse: 0.2488 - val_loss: 0.4073 - val_mcrmse: 0.2198\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2295 - mcrmse: 0.2474 - val_loss: 0.4069 - val_mcrmse: 0.2193\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2285 - mcrmse: 0.2468 - val_loss: 0.4039 - val_mcrmse: 0.2176\n",
            "Epoch 41/200\n",
            "60/60 - 6s - loss: 0.2260 - mcrmse: 0.2453 - val_loss: 0.4055 - val_mcrmse: 0.2187\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2239 - mcrmse: 0.2437 - val_loss: 0.4078 - val_mcrmse: 0.2195\n",
            "Epoch 43/200\n",
            "60/60 - 7s - loss: 0.2221 - mcrmse: 0.2427 - val_loss: 0.4019 - val_mcrmse: 0.2164\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2215 - mcrmse: 0.2421 - val_loss: 0.4038 - val_mcrmse: 0.2179\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2196 - mcrmse: 0.2409 - val_loss: 0.4047 - val_mcrmse: 0.2178\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2196 - mcrmse: 0.2407 - val_loss: 0.4068 - val_mcrmse: 0.2190\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.2169 - mcrmse: 0.2392 - val_loss: 0.3990 - val_mcrmse: 0.2149\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2152 - mcrmse: 0.2379 - val_loss: 0.4020 - val_mcrmse: 0.2166\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2140 - mcrmse: 0.2372 - val_loss: 0.4070 - val_mcrmse: 0.2194\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2128 - mcrmse: 0.2363 - val_loss: 0.4009 - val_mcrmse: 0.2158\n",
            "Epoch 51/200\n",
            "60/60 - 7s - loss: 0.2123 - mcrmse: 0.2359 - val_loss: 0.4031 - val_mcrmse: 0.2173\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2111 - mcrmse: 0.2351 - val_loss: 0.4051 - val_mcrmse: 0.2181\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2083 - mcrmse: 0.2331 - val_loss: 0.4018 - val_mcrmse: 0.2163\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2077 - mcrmse: 0.2329 - val_loss: 0.4047 - val_mcrmse: 0.2180\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2064 - mcrmse: 0.2317 - val_loss: 0.4046 - val_mcrmse: 0.2180\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2050 - mcrmse: 0.2309 - val_loss: 0.4017 - val_mcrmse: 0.2161\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.2044 - mcrmse: 0.2303 - val_loss: 0.3986 - val_mcrmse: 0.2147\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2033 - mcrmse: 0.2295 - val_loss: 0.4020 - val_mcrmse: 0.2167\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.2021 - mcrmse: 0.2286 - val_loss: 0.4052 - val_mcrmse: 0.2186\n",
            "Epoch 60/200\n",
            "60/60 - 6s - loss: 0.2016 - mcrmse: 0.2283 - val_loss: 0.4061 - val_mcrmse: 0.2186\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2003 - mcrmse: 0.2273 - val_loss: 0.4030 - val_mcrmse: 0.2173\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.1988 - mcrmse: 0.2262 - val_loss: 0.4017 - val_mcrmse: 0.2166\n",
            "Epoch 63/200\n",
            "60/60 - 7s - loss: 0.1982 - mcrmse: 0.2258 - val_loss: 0.4019 - val_mcrmse: 0.2164\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.1964 - mcrmse: 0.2246 - val_loss: 0.4025 - val_mcrmse: 0.2171\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.1968 - mcrmse: 0.2245 - val_loss: 0.3997 - val_mcrmse: 0.2154\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1958 - mcrmse: 0.2240 - val_loss: 0.4077 - val_mcrmse: 0.2194\n",
            "Epoch 67/200\n",
            "60/60 - 7s - loss: 0.1947 - mcrmse: 0.2231 - val_loss: 0.4061 - val_mcrmse: 0.2189\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1943 - mcrmse: 0.2227 - val_loss: 0.4014 - val_mcrmse: 0.2159\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1926 - mcrmse: 0.2217 - val_loss: 0.4038 - val_mcrmse: 0.2173\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1921 - mcrmse: 0.2213 - val_loss: 0.4061 - val_mcrmse: 0.2191\n",
            "Epoch 71/200\n",
            "60/60 - 7s - loss: 0.1912 - mcrmse: 0.2204 - val_loss: 0.3998 - val_mcrmse: 0.2152\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1905 - mcrmse: 0.2198 - val_loss: 0.4060 - val_mcrmse: 0.2188\n",
            "Epoch 73/200\n",
            "60/60 - 6s - loss: 0.1892 - mcrmse: 0.2191 - val_loss: 0.4040 - val_mcrmse: 0.2177\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1882 - mcrmse: 0.2184 - val_loss: 0.4047 - val_mcrmse: 0.2181\n",
            "Epoch 75/200\n",
            "60/60 - 7s - loss: 0.1875 - mcrmse: 0.2177 - val_loss: 0.4012 - val_mcrmse: 0.2163\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1870 - mcrmse: 0.2173 - val_loss: 0.4015 - val_mcrmse: 0.2167\n",
            "Epoch 77/200\n",
            "\n",
            "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1858 - mcrmse: 0.2165 - val_loss: 0.4060 - val_mcrmse: 0.2185\n",
            "Epoch 00077: early stopping\n",
            "##### Type 1 Fold 2 #####\n",
            "Fold 2 validation loss=0.3986486792564392\n",
            "tf.Tensor(0.21465386, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 1 Fold 3 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.155131100000162 -0.18742859999999997\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7322 - mcrmse: 0.5647 - val_loss: 0.5927 - val_mcrmse: 0.3381\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4714 - mcrmse: 0.3883 - val_loss: 0.5280 - val_mcrmse: 0.2922\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4303 - mcrmse: 0.3591 - val_loss: 0.4991 - val_mcrmse: 0.2744\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.4023 - mcrmse: 0.3409 - val_loss: 0.4743 - val_mcrmse: 0.2606\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3815 - mcrmse: 0.3278 - val_loss: 0.4642 - val_mcrmse: 0.2536\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3686 - mcrmse: 0.3195 - val_loss: 0.4488 - val_mcrmse: 0.2464\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3571 - mcrmse: 0.3123 - val_loss: 0.4358 - val_mcrmse: 0.2395\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3453 - mcrmse: 0.3049 - val_loss: 0.4270 - val_mcrmse: 0.2346\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3359 - mcrmse: 0.2991 - val_loss: 0.4196 - val_mcrmse: 0.2302\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3282 - mcrmse: 0.2940 - val_loss: 0.4171 - val_mcrmse: 0.2281\n",
            "Epoch 11/200\n",
            "60/60 - 6s - loss: 0.3237 - mcrmse: 0.2914 - val_loss: 0.4170 - val_mcrmse: 0.2283\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3165 - mcrmse: 0.2867 - val_loss: 0.4070 - val_mcrmse: 0.2233\n",
            "Epoch 13/200\n",
            "60/60 - 6s - loss: 0.3110 - mcrmse: 0.2834 - val_loss: 0.4108 - val_mcrmse: 0.2236\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.3052 - mcrmse: 0.2798 - val_loss: 0.4067 - val_mcrmse: 0.2221\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.3000 - mcrmse: 0.2764 - val_loss: 0.4015 - val_mcrmse: 0.2197\n",
            "Epoch 16/200\n",
            "60/60 - 6s - loss: 0.2969 - mcrmse: 0.2744 - val_loss: 0.4011 - val_mcrmse: 0.2198\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2922 - mcrmse: 0.2715 - val_loss: 0.3988 - val_mcrmse: 0.2179\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2873 - mcrmse: 0.2685 - val_loss: 0.3916 - val_mcrmse: 0.2149\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2827 - mcrmse: 0.2656 - val_loss: 0.3885 - val_mcrmse: 0.2136\n",
            "Epoch 20/200\n",
            "60/60 - 6s - loss: 0.2807 - mcrmse: 0.2641 - val_loss: 0.3901 - val_mcrmse: 0.2152\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2766 - mcrmse: 0.2617 - val_loss: 0.3870 - val_mcrmse: 0.2113\n",
            "Epoch 22/200\n",
            "60/60 - 6s - loss: 0.2726 - mcrmse: 0.2590 - val_loss: 0.3866 - val_mcrmse: 0.2126\n",
            "Epoch 23/200\n",
            "60/60 - 6s - loss: 0.2705 - mcrmse: 0.2574 - val_loss: 0.3871 - val_mcrmse: 0.2117\n",
            "Epoch 24/200\n",
            "60/60 - 6s - loss: 0.2681 - mcrmse: 0.2562 - val_loss: 0.3893 - val_mcrmse: 0.2133\n",
            "Epoch 25/200\n",
            "60/60 - 6s - loss: 0.2649 - mcrmse: 0.2541 - val_loss: 0.3874 - val_mcrmse: 0.2116\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2615 - mcrmse: 0.2520 - val_loss: 0.3858 - val_mcrmse: 0.2112\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2581 - mcrmse: 0.2500 - val_loss: 0.3814 - val_mcrmse: 0.2086\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2567 - mcrmse: 0.2489 - val_loss: 0.3842 - val_mcrmse: 0.2098\n",
            "Epoch 29/200\n",
            "60/60 - 6s - loss: 0.2541 - mcrmse: 0.2472 - val_loss: 0.3826 - val_mcrmse: 0.2091\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2517 - mcrmse: 0.2457 - val_loss: 0.3871 - val_mcrmse: 0.2113\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2501 - mcrmse: 0.2447 - val_loss: 0.3818 - val_mcrmse: 0.2086\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2472 - mcrmse: 0.2428 - val_loss: 0.3797 - val_mcrmse: 0.2078\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2456 - mcrmse: 0.2418 - val_loss: 0.3815 - val_mcrmse: 0.2090\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2456 - mcrmse: 0.2418 - val_loss: 0.3800 - val_mcrmse: 0.2075\n",
            "Epoch 35/200\n",
            "60/60 - 6s - loss: 0.2419 - mcrmse: 0.2394 - val_loss: 0.3811 - val_mcrmse: 0.2084\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2387 - mcrmse: 0.2373 - val_loss: 0.3841 - val_mcrmse: 0.2098\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2377 - mcrmse: 0.2366 - val_loss: 0.3832 - val_mcrmse: 0.2090\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2349 - mcrmse: 0.2347 - val_loss: 0.3797 - val_mcrmse: 0.2076\n",
            "Epoch 39/200\n",
            "60/60 - 6s - loss: 0.2334 - mcrmse: 0.2337 - val_loss: 0.3820 - val_mcrmse: 0.2087\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2319 - mcrmse: 0.2327 - val_loss: 0.3837 - val_mcrmse: 0.2103\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2302 - mcrmse: 0.2316 - val_loss: 0.3767 - val_mcrmse: 0.2063\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2293 - mcrmse: 0.2308 - val_loss: 0.3796 - val_mcrmse: 0.2075\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2266 - mcrmse: 0.2293 - val_loss: 0.3796 - val_mcrmse: 0.2074\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2262 - mcrmse: 0.2288 - val_loss: 0.3817 - val_mcrmse: 0.2086\n",
            "Epoch 45/200\n",
            "60/60 - 6s - loss: 0.2234 - mcrmse: 0.2270 - val_loss: 0.3791 - val_mcrmse: 0.2076\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2224 - mcrmse: 0.2262 - val_loss: 0.3815 - val_mcrmse: 0.2082\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2212 - mcrmse: 0.2254 - val_loss: 0.3789 - val_mcrmse: 0.2069\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2200 - mcrmse: 0.2246 - val_loss: 0.3796 - val_mcrmse: 0.2075\n",
            "Epoch 49/200\n",
            "60/60 - 6s - loss: 0.2188 - mcrmse: 0.2238 - val_loss: 0.3783 - val_mcrmse: 0.2064\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2173 - mcrmse: 0.2227 - val_loss: 0.3789 - val_mcrmse: 0.2071\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2167 - mcrmse: 0.2221 - val_loss: 0.3800 - val_mcrmse: 0.2082\n",
            "Epoch 52/200\n",
            "60/60 - 7s - loss: 0.2154 - mcrmse: 0.2213 - val_loss: 0.3769 - val_mcrmse: 0.2060\n",
            "Epoch 53/200\n",
            "60/60 - 6s - loss: 0.2135 - mcrmse: 0.2201 - val_loss: 0.3780 - val_mcrmse: 0.2067\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2120 - mcrmse: 0.2190 - val_loss: 0.3778 - val_mcrmse: 0.2063\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2110 - mcrmse: 0.2184 - val_loss: 0.3799 - val_mcrmse: 0.2074\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2094 - mcrmse: 0.2173 - val_loss: 0.3789 - val_mcrmse: 0.2070\n",
            "Epoch 57/200\n",
            "60/60 - 6s - loss: 0.2080 - mcrmse: 0.2163 - val_loss: 0.3806 - val_mcrmse: 0.2080\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2079 - mcrmse: 0.2160 - val_loss: 0.3844 - val_mcrmse: 0.2097\n",
            "Epoch 59/200\n",
            "60/60 - 6s - loss: 0.2063 - mcrmse: 0.2149 - val_loss: 0.3803 - val_mcrmse: 0.2080\n",
            "Epoch 60/200\n",
            "60/60 - 6s - loss: 0.2048 - mcrmse: 0.2140 - val_loss: 0.3819 - val_mcrmse: 0.2086\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.2038 - mcrmse: 0.2133 - val_loss: 0.3785 - val_mcrmse: 0.2068\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.2027 - mcrmse: 0.2124 - val_loss: 0.3790 - val_mcrmse: 0.2068\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.2016 - mcrmse: 0.2116 - val_loss: 0.3766 - val_mcrmse: 0.2061\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.2013 - mcrmse: 0.2111 - val_loss: 0.3788 - val_mcrmse: 0.2069\n",
            "Epoch 65/200\n",
            "60/60 - 6s - loss: 0.1997 - mcrmse: 0.2105 - val_loss: 0.3793 - val_mcrmse: 0.2078\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1992 - mcrmse: 0.2098 - val_loss: 0.3768 - val_mcrmse: 0.2060\n",
            "Epoch 67/200\n",
            "60/60 - 6s - loss: 0.1978 - mcrmse: 0.2087 - val_loss: 0.3777 - val_mcrmse: 0.2066\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1968 - mcrmse: 0.2083 - val_loss: 0.3797 - val_mcrmse: 0.2077\n",
            "Epoch 69/200\n",
            "60/60 - 6s - loss: 0.1963 - mcrmse: 0.2077 - val_loss: 0.3800 - val_mcrmse: 0.2074\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1955 - mcrmse: 0.2072 - val_loss: 0.3791 - val_mcrmse: 0.2070\n",
            "Epoch 71/200\n",
            "60/60 - 6s - loss: 0.1936 - mcrmse: 0.2057 - val_loss: 0.3795 - val_mcrmse: 0.2071\n",
            "Epoch 72/200\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 6s - loss: 0.1933 - mcrmse: 0.2054 - val_loss: 0.3800 - val_mcrmse: 0.2084\n",
            "Epoch 00072: early stopping\n",
            "##### Type 1 Fold 3 #####\n",
            "Fold 3 validation loss=0.3766336143016815\n",
            "tf.Tensor(0.20598559, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 1 Fold 4 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2344 -0.186\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.7133 - mcrmse: 0.5665 - val_loss: 0.5866 - val_mcrmse: 0.3292\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4706 - mcrmse: 0.3958 - val_loss: 0.5253 - val_mcrmse: 0.2872\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4268 - mcrmse: 0.3666 - val_loss: 0.5039 - val_mcrmse: 0.2734\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3985 - mcrmse: 0.3484 - val_loss: 0.4645 - val_mcrmse: 0.2520\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3783 - mcrmse: 0.3354 - val_loss: 0.4517 - val_mcrmse: 0.2442\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3634 - mcrmse: 0.3261 - val_loss: 0.4352 - val_mcrmse: 0.2355\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3536 - mcrmse: 0.3198 - val_loss: 0.4287 - val_mcrmse: 0.2322\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3414 - mcrmse: 0.3126 - val_loss: 0.4225 - val_mcrmse: 0.2294\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3300 - mcrmse: 0.3058 - val_loss: 0.4179 - val_mcrmse: 0.2260\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3237 - mcrmse: 0.3019 - val_loss: 0.4116 - val_mcrmse: 0.2225\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3161 - mcrmse: 0.2971 - val_loss: 0.4065 - val_mcrmse: 0.2201\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3096 - mcrmse: 0.2929 - val_loss: 0.4046 - val_mcrmse: 0.2184\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3041 - mcrmse: 0.2899 - val_loss: 0.4011 - val_mcrmse: 0.2177\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.2983 - mcrmse: 0.2862 - val_loss: 0.3968 - val_mcrmse: 0.2149\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2937 - mcrmse: 0.2832 - val_loss: 0.3959 - val_mcrmse: 0.2137\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2892 - mcrmse: 0.2806 - val_loss: 0.3953 - val_mcrmse: 0.2131\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2849 - mcrmse: 0.2779 - val_loss: 0.3914 - val_mcrmse: 0.2112\n",
            "Epoch 18/200\n",
            "60/60 - 6s - loss: 0.2810 - mcrmse: 0.2755 - val_loss: 0.3918 - val_mcrmse: 0.2116\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2789 - mcrmse: 0.2740 - val_loss: 0.3870 - val_mcrmse: 0.2086\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2740 - mcrmse: 0.2709 - val_loss: 0.3856 - val_mcrmse: 0.2083\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2703 - mcrmse: 0.2688 - val_loss: 0.3924 - val_mcrmse: 0.2115\n",
            "Epoch 22/200\n",
            "60/60 - 6s - loss: 0.2663 - mcrmse: 0.2661 - val_loss: 0.3897 - val_mcrmse: 0.2098\n",
            "Epoch 23/200\n",
            "60/60 - 6s - loss: 0.2641 - mcrmse: 0.2648 - val_loss: 0.3871 - val_mcrmse: 0.2096\n",
            "Epoch 24/200\n",
            "60/60 - 6s - loss: 0.2613 - mcrmse: 0.2631 - val_loss: 0.3900 - val_mcrmse: 0.2103\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2573 - mcrmse: 0.2607 - val_loss: 0.3874 - val_mcrmse: 0.2099\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2547 - mcrmse: 0.2587 - val_loss: 0.3807 - val_mcrmse: 0.2056\n",
            "Epoch 27/200\n",
            "60/60 - 6s - loss: 0.2525 - mcrmse: 0.2575 - val_loss: 0.3851 - val_mcrmse: 0.2075\n",
            "Epoch 28/200\n",
            "60/60 - 6s - loss: 0.2501 - mcrmse: 0.2558 - val_loss: 0.3841 - val_mcrmse: 0.2069\n",
            "Epoch 29/200\n",
            "60/60 - 7s - loss: 0.2481 - mcrmse: 0.2547 - val_loss: 0.3830 - val_mcrmse: 0.2060\n",
            "Epoch 30/200\n",
            "60/60 - 6s - loss: 0.2463 - mcrmse: 0.2536 - val_loss: 0.3861 - val_mcrmse: 0.2082\n",
            "Epoch 31/200\n",
            "60/60 - 6s - loss: 0.2424 - mcrmse: 0.2511 - val_loss: 0.3826 - val_mcrmse: 0.2067\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2408 - mcrmse: 0.2500 - val_loss: 0.3793 - val_mcrmse: 0.2053\n",
            "Epoch 33/200\n",
            "60/60 - 6s - loss: 0.2392 - mcrmse: 0.2491 - val_loss: 0.3846 - val_mcrmse: 0.2068\n",
            "Epoch 34/200\n",
            "60/60 - 6s - loss: 0.2373 - mcrmse: 0.2477 - val_loss: 0.3838 - val_mcrmse: 0.2068\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2358 - mcrmse: 0.2467 - val_loss: 0.3787 - val_mcrmse: 0.2038\n",
            "Epoch 36/200\n",
            "60/60 - 6s - loss: 0.2331 - mcrmse: 0.2449 - val_loss: 0.3787 - val_mcrmse: 0.2043\n",
            "Epoch 37/200\n",
            "60/60 - 6s - loss: 0.2317 - mcrmse: 0.2442 - val_loss: 0.3836 - val_mcrmse: 0.2087\n",
            "Epoch 38/200\n",
            "60/60 - 6s - loss: 0.2292 - mcrmse: 0.2425 - val_loss: 0.3790 - val_mcrmse: 0.2044\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2275 - mcrmse: 0.2414 - val_loss: 0.3770 - val_mcrmse: 0.2031\n",
            "Epoch 40/200\n",
            "60/60 - 6s - loss: 0.2258 - mcrmse: 0.2401 - val_loss: 0.3786 - val_mcrmse: 0.2039\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2248 - mcrmse: 0.2395 - val_loss: 0.3794 - val_mcrmse: 0.2042\n",
            "Epoch 42/200\n",
            "60/60 - 6s - loss: 0.2235 - mcrmse: 0.2384 - val_loss: 0.3785 - val_mcrmse: 0.2039\n",
            "Epoch 43/200\n",
            "60/60 - 6s - loss: 0.2238 - mcrmse: 0.2386 - val_loss: 0.3816 - val_mcrmse: 0.2052\n",
            "Epoch 44/200\n",
            "60/60 - 6s - loss: 0.2230 - mcrmse: 0.2379 - val_loss: 0.3863 - val_mcrmse: 0.2084\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.2197 - mcrmse: 0.2361 - val_loss: 0.3771 - val_mcrmse: 0.2029\n",
            "Epoch 46/200\n",
            "60/60 - 6s - loss: 0.2179 - mcrmse: 0.2346 - val_loss: 0.3794 - val_mcrmse: 0.2047\n",
            "Epoch 47/200\n",
            "60/60 - 6s - loss: 0.2163 - mcrmse: 0.2337 - val_loss: 0.3763 - val_mcrmse: 0.2029\n",
            "Epoch 48/200\n",
            "60/60 - 6s - loss: 0.2151 - mcrmse: 0.2329 - val_loss: 0.3794 - val_mcrmse: 0.2046\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.2137 - mcrmse: 0.2318 - val_loss: 0.3748 - val_mcrmse: 0.2019\n",
            "Epoch 50/200\n",
            "60/60 - 6s - loss: 0.2125 - mcrmse: 0.2309 - val_loss: 0.3792 - val_mcrmse: 0.2048\n",
            "Epoch 51/200\n",
            "60/60 - 6s - loss: 0.2115 - mcrmse: 0.2300 - val_loss: 0.3759 - val_mcrmse: 0.2024\n",
            "Epoch 52/200\n",
            "60/60 - 6s - loss: 0.2105 - mcrmse: 0.2295 - val_loss: 0.3765 - val_mcrmse: 0.2030\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.2098 - mcrmse: 0.2288 - val_loss: 0.3787 - val_mcrmse: 0.2038\n",
            "Epoch 54/200\n",
            "60/60 - 6s - loss: 0.2077 - mcrmse: 0.2276 - val_loss: 0.3790 - val_mcrmse: 0.2045\n",
            "Epoch 55/200\n",
            "60/60 - 6s - loss: 0.2062 - mcrmse: 0.2266 - val_loss: 0.3785 - val_mcrmse: 0.2040\n",
            "Epoch 56/200\n",
            "60/60 - 6s - loss: 0.2041 - mcrmse: 0.2251 - val_loss: 0.3751 - val_mcrmse: 0.2023\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.2048 - mcrmse: 0.2253 - val_loss: 0.3785 - val_mcrmse: 0.2036\n",
            "Epoch 58/200\n",
            "60/60 - 6s - loss: 0.2037 - mcrmse: 0.2249 - val_loss: 0.3748 - val_mcrmse: 0.2025\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.2020 - mcrmse: 0.2235 - val_loss: 0.3734 - val_mcrmse: 0.2018\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.2003 - mcrmse: 0.2224 - val_loss: 0.3741 - val_mcrmse: 0.2015\n",
            "Epoch 61/200\n",
            "60/60 - 6s - loss: 0.1987 - mcrmse: 0.2213 - val_loss: 0.3752 - val_mcrmse: 0.2022\n",
            "Epoch 62/200\n",
            "60/60 - 6s - loss: 0.1986 - mcrmse: 0.2210 - val_loss: 0.3787 - val_mcrmse: 0.2040\n",
            "Epoch 63/200\n",
            "60/60 - 6s - loss: 0.1979 - mcrmse: 0.2205 - val_loss: 0.3736 - val_mcrmse: 0.2017\n",
            "Epoch 64/200\n",
            "60/60 - 6s - loss: 0.1970 - mcrmse: 0.2197 - val_loss: 0.3757 - val_mcrmse: 0.2025\n",
            "Epoch 65/200\n",
            "60/60 - 7s - loss: 0.1966 - mcrmse: 0.2195 - val_loss: 0.3749 - val_mcrmse: 0.2021\n",
            "Epoch 66/200\n",
            "60/60 - 6s - loss: 0.1955 - mcrmse: 0.2187 - val_loss: 0.3754 - val_mcrmse: 0.2025\n",
            "Epoch 67/200\n",
            "60/60 - 7s - loss: 0.1936 - mcrmse: 0.2173 - val_loss: 0.3736 - val_mcrmse: 0.2014\n",
            "Epoch 68/200\n",
            "60/60 - 6s - loss: 0.1929 - mcrmse: 0.2169 - val_loss: 0.3733 - val_mcrmse: 0.2014\n",
            "Epoch 69/200\n",
            "60/60 - 7s - loss: 0.1924 - mcrmse: 0.2163 - val_loss: 0.3749 - val_mcrmse: 0.2028\n",
            "Epoch 70/200\n",
            "60/60 - 6s - loss: 0.1920 - mcrmse: 0.2161 - val_loss: 0.3745 - val_mcrmse: 0.2019\n",
            "Epoch 71/200\n",
            "60/60 - 7s - loss: 0.1909 - mcrmse: 0.2150 - val_loss: 0.3730 - val_mcrmse: 0.2010\n",
            "Epoch 72/200\n",
            "60/60 - 6s - loss: 0.1901 - mcrmse: 0.2145 - val_loss: 0.3741 - val_mcrmse: 0.2020\n",
            "Epoch 73/200\n",
            "60/60 - 7s - loss: 0.1887 - mcrmse: 0.2136 - val_loss: 0.3763 - val_mcrmse: 0.2028\n",
            "Epoch 74/200\n",
            "60/60 - 6s - loss: 0.1882 - mcrmse: 0.2133 - val_loss: 0.3734 - val_mcrmse: 0.2013\n",
            "Epoch 75/200\n",
            "60/60 - 6s - loss: 0.1883 - mcrmse: 0.2132 - val_loss: 0.3756 - val_mcrmse: 0.2026\n",
            "Epoch 76/200\n",
            "60/60 - 6s - loss: 0.1873 - mcrmse: 0.2123 - val_loss: 0.3734 - val_mcrmse: 0.2021\n",
            "Epoch 77/200\n",
            "60/60 - 7s - loss: 0.1858 - mcrmse: 0.2111 - val_loss: 0.3750 - val_mcrmse: 0.2023\n",
            "Epoch 78/200\n",
            "60/60 - 6s - loss: 0.1846 - mcrmse: 0.2103 - val_loss: 0.3737 - val_mcrmse: 0.2023\n",
            "Epoch 79/200\n",
            "60/60 - 6s - loss: 0.1844 - mcrmse: 0.2101 - val_loss: 0.3743 - val_mcrmse: 0.2023\n",
            "Epoch 80/200\n",
            "60/60 - 6s - loss: 0.1834 - mcrmse: 0.2096 - val_loss: 0.3777 - val_mcrmse: 0.2045\n",
            "Epoch 81/200\n",
            "60/60 - 6s - loss: 0.1826 - mcrmse: 0.2088 - val_loss: 0.3746 - val_mcrmse: 0.2023\n",
            "Epoch 82/200\n",
            "60/60 - 6s - loss: 0.1819 - mcrmse: 0.2082 - val_loss: 0.3741 - val_mcrmse: 0.2018\n",
            "Epoch 83/200\n",
            "60/60 - 6s - loss: 0.1814 - mcrmse: 0.2078 - val_loss: 0.3769 - val_mcrmse: 0.2034\n",
            "Epoch 84/200\n",
            "60/60 - 6s - loss: 0.1805 - mcrmse: 0.2072 - val_loss: 0.3729 - val_mcrmse: 0.2017\n",
            "Epoch 85/200\n",
            "60/60 - 7s - loss: 0.1805 - mcrmse: 0.2071 - val_loss: 0.3730 - val_mcrmse: 0.2018\n",
            "Epoch 86/200\n",
            "60/60 - 6s - loss: 0.1796 - mcrmse: 0.2063 - val_loss: 0.3744 - val_mcrmse: 0.2022\n",
            "Epoch 87/200\n",
            "60/60 - 6s - loss: 0.1793 - mcrmse: 0.2060 - val_loss: 0.3724 - val_mcrmse: 0.2012\n",
            "Epoch 88/200\n",
            "60/60 - 7s - loss: 0.1782 - mcrmse: 0.2052 - val_loss: 0.3720 - val_mcrmse: 0.2006\n",
            "Epoch 89/200\n",
            "60/60 - 7s - loss: 0.1775 - mcrmse: 0.2045 - val_loss: 0.3741 - val_mcrmse: 0.2017\n",
            "Epoch 90/200\n",
            "60/60 - 6s - loss: 0.1765 - mcrmse: 0.2038 - val_loss: 0.3730 - val_mcrmse: 0.2018\n",
            "Epoch 91/200\n",
            "60/60 - 6s - loss: 0.1759 - mcrmse: 0.2036 - val_loss: 0.3733 - val_mcrmse: 0.2015\n",
            "Epoch 92/200\n",
            "60/60 - 6s - loss: 0.1749 - mcrmse: 0.2026 - val_loss: 0.3725 - val_mcrmse: 0.2009\n",
            "Epoch 93/200\n",
            "60/60 - 6s - loss: 0.1754 - mcrmse: 0.2029 - val_loss: 0.3746 - val_mcrmse: 0.2025\n",
            "Epoch 94/200\n",
            "60/60 - 6s - loss: 0.1738 - mcrmse: 0.2018 - val_loss: 0.3748 - val_mcrmse: 0.2029\n",
            "Epoch 95/200\n",
            "60/60 - 6s - loss: 0.1738 - mcrmse: 0.2015 - val_loss: 0.3750 - val_mcrmse: 0.2023\n",
            "Epoch 96/200\n",
            "60/60 - 6s - loss: 0.1730 - mcrmse: 0.2009 - val_loss: 0.3739 - val_mcrmse: 0.2019\n",
            "Epoch 97/200\n",
            "60/60 - 7s - loss: 0.1732 - mcrmse: 0.2009 - val_loss: 0.3736 - val_mcrmse: 0.2016\n",
            "Epoch 98/200\n",
            "60/60 - 6s - loss: 0.1724 - mcrmse: 0.2002 - val_loss: 0.3742 - val_mcrmse: 0.2023\n",
            "Epoch 99/200\n",
            "60/60 - 6s - loss: 0.1713 - mcrmse: 0.1995 - val_loss: 0.3752 - val_mcrmse: 0.2026\n",
            "Epoch 100/200\n",
            "60/60 - 6s - loss: 0.1706 - mcrmse: 0.1988 - val_loss: 0.3740 - val_mcrmse: 0.2019\n",
            "Epoch 101/200\n",
            "60/60 - 7s - loss: 0.1706 - mcrmse: 0.1986 - val_loss: 0.3738 - val_mcrmse: 0.2019\n",
            "Epoch 102/200\n",
            "60/60 - 6s - loss: 0.1704 - mcrmse: 0.1985 - val_loss: 0.3773 - val_mcrmse: 0.2039\n",
            "Epoch 103/200\n",
            "60/60 - 6s - loss: 0.1692 - mcrmse: 0.1977 - val_loss: 0.3742 - val_mcrmse: 0.2021\n",
            "Epoch 104/200\n",
            "60/60 - 6s - loss: 0.1688 - mcrmse: 0.1972 - val_loss: 0.3762 - val_mcrmse: 0.2033\n",
            "Epoch 105/200\n",
            "60/60 - 7s - loss: 0.1680 - mcrmse: 0.1964 - val_loss: 0.3755 - val_mcrmse: 0.2026\n",
            "Epoch 106/200\n",
            "60/60 - 6s - loss: 0.1680 - mcrmse: 0.1964 - val_loss: 0.3753 - val_mcrmse: 0.2035\n",
            "Epoch 107/200\n",
            "60/60 - 6s - loss: 0.1666 - mcrmse: 0.1956 - val_loss: 0.3744 - val_mcrmse: 0.2024\n",
            "Epoch 108/200\n",
            "\n",
            "Epoch 00108: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 7s - loss: 0.1661 - mcrmse: 0.1950 - val_loss: 0.3728 - val_mcrmse: 0.2019\n",
            "Epoch 00108: early stopping\n",
            "##### Type 1 Fold 4 #####\n",
            "Fold 4 validation loss=0.3720293641090393\n",
            "tf.Tensor(0.20063794, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 2 Fold 0 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.1759 -0.1961\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0295s vs `on_train_batch_end` time: 0.0515s). Check your callbacks.\n",
            "60/60 - 6s - loss: 0.6845 - mcrmse: 0.5279 - val_loss: 0.6290 - val_mcrmse: 0.3575\n",
            "Epoch 2/200\n",
            "60/60 - 5s - loss: 0.4744 - mcrmse: 0.3838 - val_loss: 0.5342 - val_mcrmse: 0.2937\n",
            "Epoch 3/200\n",
            "60/60 - 5s - loss: 0.4221 - mcrmse: 0.3481 - val_loss: 0.4948 - val_mcrmse: 0.2707\n",
            "Epoch 4/200\n",
            "60/60 - 5s - loss: 0.3936 - mcrmse: 0.3299 - val_loss: 0.4671 - val_mcrmse: 0.2551\n",
            "Epoch 5/200\n",
            "60/60 - 5s - loss: 0.3798 - mcrmse: 0.3215 - val_loss: 0.4580 - val_mcrmse: 0.2493\n",
            "Epoch 6/200\n",
            "60/60 - 5s - loss: 0.3653 - mcrmse: 0.3126 - val_loss: 0.4457 - val_mcrmse: 0.2427\n",
            "Epoch 7/200\n",
            "60/60 - 5s - loss: 0.3550 - mcrmse: 0.3064 - val_loss: 0.4493 - val_mcrmse: 0.2448\n",
            "Epoch 8/200\n",
            "60/60 - 5s - loss: 0.3453 - mcrmse: 0.3007 - val_loss: 0.4351 - val_mcrmse: 0.2364\n",
            "Epoch 9/200\n",
            "60/60 - 5s - loss: 0.3407 - mcrmse: 0.2980 - val_loss: 0.4307 - val_mcrmse: 0.2347\n",
            "Epoch 10/200\n",
            "60/60 - 5s - loss: 0.3322 - mcrmse: 0.2928 - val_loss: 0.4251 - val_mcrmse: 0.2326\n",
            "Epoch 11/200\n",
            "60/60 - 5s - loss: 0.3266 - mcrmse: 0.2895 - val_loss: 0.4346 - val_mcrmse: 0.2356\n",
            "Epoch 12/200\n",
            "60/60 - 5s - loss: 0.3191 - mcrmse: 0.2852 - val_loss: 0.4242 - val_mcrmse: 0.2308\n",
            "Epoch 13/200\n",
            "60/60 - 5s - loss: 0.3144 - mcrmse: 0.2822 - val_loss: 0.4200 - val_mcrmse: 0.2282\n",
            "Epoch 14/200\n",
            "60/60 - 5s - loss: 0.3100 - mcrmse: 0.2794 - val_loss: 0.4190 - val_mcrmse: 0.2277\n",
            "Epoch 15/200\n",
            "60/60 - 5s - loss: 0.3051 - mcrmse: 0.2764 - val_loss: 0.4182 - val_mcrmse: 0.2268\n",
            "Epoch 16/200\n",
            "60/60 - 5s - loss: 0.2995 - mcrmse: 0.2732 - val_loss: 0.4134 - val_mcrmse: 0.2251\n",
            "Epoch 17/200\n",
            "60/60 - 5s - loss: 0.2948 - mcrmse: 0.2700 - val_loss: 0.4167 - val_mcrmse: 0.2256\n",
            "Epoch 18/200\n",
            "60/60 - 5s - loss: 0.2896 - mcrmse: 0.2671 - val_loss: 0.4108 - val_mcrmse: 0.2233\n",
            "Epoch 19/200\n",
            "60/60 - 5s - loss: 0.2872 - mcrmse: 0.2654 - val_loss: 0.4157 - val_mcrmse: 0.2253\n",
            "Epoch 20/200\n",
            "60/60 - 5s - loss: 0.2848 - mcrmse: 0.2641 - val_loss: 0.4094 - val_mcrmse: 0.2226\n",
            "Epoch 21/200\n",
            "60/60 - 5s - loss: 0.2792 - mcrmse: 0.2607 - val_loss: 0.4106 - val_mcrmse: 0.2227\n",
            "Epoch 22/200\n",
            "60/60 - 5s - loss: 0.2753 - mcrmse: 0.2579 - val_loss: 0.4027 - val_mcrmse: 0.2193\n",
            "Epoch 23/200\n",
            "60/60 - 5s - loss: 0.2740 - mcrmse: 0.2569 - val_loss: 0.4063 - val_mcrmse: 0.2205\n",
            "Epoch 24/200\n",
            "60/60 - 5s - loss: 0.2687 - mcrmse: 0.2541 - val_loss: 0.4072 - val_mcrmse: 0.2221\n",
            "Epoch 25/200\n",
            "60/60 - 5s - loss: 0.2662 - mcrmse: 0.2523 - val_loss: 0.4012 - val_mcrmse: 0.2180\n",
            "Epoch 26/200\n",
            "60/60 - 5s - loss: 0.2624 - mcrmse: 0.2500 - val_loss: 0.3978 - val_mcrmse: 0.2165\n",
            "Epoch 27/200\n",
            "60/60 - 5s - loss: 0.2599 - mcrmse: 0.2485 - val_loss: 0.3986 - val_mcrmse: 0.2174\n",
            "Epoch 28/200\n",
            "60/60 - 5s - loss: 0.2582 - mcrmse: 0.2473 - val_loss: 0.3996 - val_mcrmse: 0.2179\n",
            "Epoch 29/200\n",
            "60/60 - 5s - loss: 0.2567 - mcrmse: 0.2462 - val_loss: 0.4037 - val_mcrmse: 0.2194\n",
            "Epoch 30/200\n",
            "60/60 - 5s - loss: 0.2522 - mcrmse: 0.2435 - val_loss: 0.4005 - val_mcrmse: 0.2177\n",
            "Epoch 31/200\n",
            "60/60 - 5s - loss: 0.2496 - mcrmse: 0.2419 - val_loss: 0.4052 - val_mcrmse: 0.2203\n",
            "Epoch 32/200\n",
            "60/60 - 5s - loss: 0.2462 - mcrmse: 0.2398 - val_loss: 0.4015 - val_mcrmse: 0.2192\n",
            "Epoch 33/200\n",
            "60/60 - 5s - loss: 0.2436 - mcrmse: 0.2383 - val_loss: 0.4003 - val_mcrmse: 0.2178\n",
            "Epoch 34/200\n",
            "60/60 - 5s - loss: 0.2417 - mcrmse: 0.2369 - val_loss: 0.3981 - val_mcrmse: 0.2171\n",
            "Epoch 35/200\n",
            "60/60 - 5s - loss: 0.2402 - mcrmse: 0.2358 - val_loss: 0.3964 - val_mcrmse: 0.2159\n",
            "Epoch 36/200\n",
            "60/60 - 5s - loss: 0.2380 - mcrmse: 0.2346 - val_loss: 0.4009 - val_mcrmse: 0.2178\n",
            "Epoch 37/200\n",
            "60/60 - 5s - loss: 0.2363 - mcrmse: 0.2335 - val_loss: 0.3991 - val_mcrmse: 0.2169\n",
            "Epoch 38/200\n",
            "60/60 - 5s - loss: 0.2344 - mcrmse: 0.2321 - val_loss: 0.3958 - val_mcrmse: 0.2155\n",
            "Epoch 39/200\n",
            "60/60 - 5s - loss: 0.2314 - mcrmse: 0.2303 - val_loss: 0.3977 - val_mcrmse: 0.2160\n",
            "Epoch 40/200\n",
            "60/60 - 5s - loss: 0.2287 - mcrmse: 0.2287 - val_loss: 0.4022 - val_mcrmse: 0.2184\n",
            "Epoch 41/200\n",
            "60/60 - 5s - loss: 0.2255 - mcrmse: 0.2267 - val_loss: 0.3976 - val_mcrmse: 0.2168\n",
            "Epoch 42/200\n",
            "60/60 - 5s - loss: 0.2249 - mcrmse: 0.2261 - val_loss: 0.3971 - val_mcrmse: 0.2160\n",
            "Epoch 43/200\n",
            "60/60 - 5s - loss: 0.2244 - mcrmse: 0.2257 - val_loss: 0.4018 - val_mcrmse: 0.2193\n",
            "Epoch 44/200\n",
            "60/60 - 5s - loss: 0.2222 - mcrmse: 0.2244 - val_loss: 0.3981 - val_mcrmse: 0.2166\n",
            "Epoch 45/200\n",
            "60/60 - 5s - loss: 0.2208 - mcrmse: 0.2235 - val_loss: 0.3970 - val_mcrmse: 0.2162\n",
            "Epoch 46/200\n",
            "60/60 - 5s - loss: 0.2188 - mcrmse: 0.2222 - val_loss: 0.3986 - val_mcrmse: 0.2180\n",
            "Epoch 47/200\n",
            "60/60 - 5s - loss: 0.2166 - mcrmse: 0.2205 - val_loss: 0.3978 - val_mcrmse: 0.2167\n",
            "Epoch 48/200\n",
            "60/60 - 5s - loss: 0.2146 - mcrmse: 0.2193 - val_loss: 0.4033 - val_mcrmse: 0.2188\n",
            "Epoch 49/200\n",
            "60/60 - 5s - loss: 0.2136 - mcrmse: 0.2185 - val_loss: 0.3989 - val_mcrmse: 0.2174\n",
            "Epoch 50/200\n",
            "60/60 - 5s - loss: 0.2124 - mcrmse: 0.2179 - val_loss: 0.3981 - val_mcrmse: 0.2167\n",
            "Epoch 51/200\n",
            "60/60 - 5s - loss: 0.2107 - mcrmse: 0.2167 - val_loss: 0.4005 - val_mcrmse: 0.2177\n",
            "Epoch 52/200\n",
            "60/60 - 5s - loss: 0.2088 - mcrmse: 0.2155 - val_loss: 0.4008 - val_mcrmse: 0.2183\n",
            "Epoch 53/200\n",
            "60/60 - 5s - loss: 0.2078 - mcrmse: 0.2148 - val_loss: 0.4013 - val_mcrmse: 0.2182\n",
            "Epoch 54/200\n",
            "60/60 - 5s - loss: 0.2060 - mcrmse: 0.2136 - val_loss: 0.4051 - val_mcrmse: 0.2201\n",
            "Epoch 55/200\n",
            "60/60 - 5s - loss: 0.2047 - mcrmse: 0.2127 - val_loss: 0.4034 - val_mcrmse: 0.2191\n",
            "Epoch 56/200\n",
            "60/60 - 5s - loss: 0.2031 - mcrmse: 0.2116 - val_loss: 0.4009 - val_mcrmse: 0.2182\n",
            "Epoch 57/200\n",
            "60/60 - 5s - loss: 0.2030 - mcrmse: 0.2115 - val_loss: 0.4013 - val_mcrmse: 0.2184\n",
            "Epoch 58/200\n",
            "\n",
            "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 5s - loss: 0.2012 - mcrmse: 0.2098 - val_loss: 0.4034 - val_mcrmse: 0.2194\n",
            "Epoch 00058: early stopping\n",
            "##### Type 2 Fold 0 #####\n",
            "Fold 0 validation loss=0.3957878053188324\n",
            "tf.Tensor(0.21549188, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 2 Fold 1 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.255600400000066 -0.17248410000000003\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0284s vs `on_train_batch_end` time: 0.0435s). Check your callbacks.\n",
            "60/60 - 6s - loss: 0.6218 - mcrmse: 0.5048 - val_loss: 0.6027 - val_mcrmse: 0.3376\n",
            "Epoch 2/200\n",
            "60/60 - 5s - loss: 0.4592 - mcrmse: 0.3866 - val_loss: 0.5144 - val_mcrmse: 0.2829\n",
            "Epoch 3/200\n",
            "60/60 - 5s - loss: 0.4075 - mcrmse: 0.3520 - val_loss: 0.4765 - val_mcrmse: 0.2642\n",
            "Epoch 4/200\n",
            "60/60 - 5s - loss: 0.3794 - mcrmse: 0.3347 - val_loss: 0.4591 - val_mcrmse: 0.2558\n",
            "Epoch 5/200\n",
            "60/60 - 5s - loss: 0.3627 - mcrmse: 0.3241 - val_loss: 0.4471 - val_mcrmse: 0.2457\n",
            "Epoch 6/200\n",
            "60/60 - 5s - loss: 0.3523 - mcrmse: 0.3175 - val_loss: 0.4448 - val_mcrmse: 0.2440\n",
            "Epoch 7/200\n",
            "60/60 - 5s - loss: 0.3426 - mcrmse: 0.3118 - val_loss: 0.4347 - val_mcrmse: 0.2393\n",
            "Epoch 8/200\n",
            "60/60 - 5s - loss: 0.3332 - mcrmse: 0.3061 - val_loss: 0.4330 - val_mcrmse: 0.2371\n",
            "Epoch 9/200\n",
            "60/60 - 5s - loss: 0.3270 - mcrmse: 0.3024 - val_loss: 0.4248 - val_mcrmse: 0.2343\n",
            "Epoch 10/200\n",
            "60/60 - 5s - loss: 0.3191 - mcrmse: 0.2977 - val_loss: 0.4211 - val_mcrmse: 0.2320\n",
            "Epoch 11/200\n",
            "60/60 - 5s - loss: 0.3130 - mcrmse: 0.2939 - val_loss: 0.4212 - val_mcrmse: 0.2330\n",
            "Epoch 12/200\n",
            "60/60 - 5s - loss: 0.3077 - mcrmse: 0.2906 - val_loss: 0.4188 - val_mcrmse: 0.2293\n",
            "Epoch 13/200\n",
            "60/60 - 5s - loss: 0.3038 - mcrmse: 0.2885 - val_loss: 0.4152 - val_mcrmse: 0.2303\n",
            "Epoch 14/200\n",
            "60/60 - 5s - loss: 0.2979 - mcrmse: 0.2846 - val_loss: 0.4139 - val_mcrmse: 0.2292\n",
            "Epoch 15/200\n",
            "60/60 - 5s - loss: 0.2934 - mcrmse: 0.2819 - val_loss: 0.4096 - val_mcrmse: 0.2252\n",
            "Epoch 16/200\n",
            "60/60 - 5s - loss: 0.2903 - mcrmse: 0.2797 - val_loss: 0.4092 - val_mcrmse: 0.2252\n",
            "Epoch 17/200\n",
            "60/60 - 5s - loss: 0.2843 - mcrmse: 0.2762 - val_loss: 0.4098 - val_mcrmse: 0.2264\n",
            "Epoch 18/200\n",
            "60/60 - 5s - loss: 0.2800 - mcrmse: 0.2734 - val_loss: 0.4045 - val_mcrmse: 0.2222\n",
            "Epoch 19/200\n",
            "60/60 - 5s - loss: 0.2769 - mcrmse: 0.2716 - val_loss: 0.4070 - val_mcrmse: 0.2244\n",
            "Epoch 20/200\n",
            "60/60 - 5s - loss: 0.2739 - mcrmse: 0.2697 - val_loss: 0.4085 - val_mcrmse: 0.2235\n",
            "Epoch 21/200\n",
            "60/60 - 5s - loss: 0.2695 - mcrmse: 0.2670 - val_loss: 0.4025 - val_mcrmse: 0.2215\n",
            "Epoch 22/200\n",
            "60/60 - 5s - loss: 0.2668 - mcrmse: 0.2652 - val_loss: 0.4025 - val_mcrmse: 0.2206\n",
            "Epoch 23/200\n",
            "60/60 - 5s - loss: 0.2637 - mcrmse: 0.2631 - val_loss: 0.4025 - val_mcrmse: 0.2206\n",
            "Epoch 24/200\n",
            "60/60 - 5s - loss: 0.2595 - mcrmse: 0.2608 - val_loss: 0.4025 - val_mcrmse: 0.2218\n",
            "Epoch 25/200\n",
            "60/60 - 5s - loss: 0.2564 - mcrmse: 0.2589 - val_loss: 0.4009 - val_mcrmse: 0.2212\n",
            "Epoch 26/200\n",
            "60/60 - 5s - loss: 0.2535 - mcrmse: 0.2571 - val_loss: 0.4007 - val_mcrmse: 0.2198\n",
            "Epoch 27/200\n",
            "60/60 - 5s - loss: 0.2514 - mcrmse: 0.2558 - val_loss: 0.4009 - val_mcrmse: 0.2211\n",
            "Epoch 28/200\n",
            "60/60 - 5s - loss: 0.2486 - mcrmse: 0.2541 - val_loss: 0.3978 - val_mcrmse: 0.2179\n",
            "Epoch 29/200\n",
            "60/60 - 5s - loss: 0.2460 - mcrmse: 0.2525 - val_loss: 0.3964 - val_mcrmse: 0.2171\n",
            "Epoch 30/200\n",
            "60/60 - 5s - loss: 0.2430 - mcrmse: 0.2505 - val_loss: 0.3971 - val_mcrmse: 0.2182\n",
            "Epoch 31/200\n",
            "60/60 - 5s - loss: 0.2401 - mcrmse: 0.2485 - val_loss: 0.3913 - val_mcrmse: 0.2152\n",
            "Epoch 32/200\n",
            "60/60 - 5s - loss: 0.2380 - mcrmse: 0.2474 - val_loss: 0.3956 - val_mcrmse: 0.2171\n",
            "Epoch 33/200\n",
            "60/60 - 5s - loss: 0.2350 - mcrmse: 0.2457 - val_loss: 0.3948 - val_mcrmse: 0.2177\n",
            "Epoch 34/200\n",
            "60/60 - 5s - loss: 0.2337 - mcrmse: 0.2447 - val_loss: 0.3939 - val_mcrmse: 0.2167\n",
            "Epoch 35/200\n",
            "60/60 - 5s - loss: 0.2317 - mcrmse: 0.2435 - val_loss: 0.3938 - val_mcrmse: 0.2175\n",
            "Epoch 36/200\n",
            "60/60 - 5s - loss: 0.2297 - mcrmse: 0.2420 - val_loss: 0.3959 - val_mcrmse: 0.2177\n",
            "Epoch 37/200\n",
            "60/60 - 5s - loss: 0.2269 - mcrmse: 0.2404 - val_loss: 0.3927 - val_mcrmse: 0.2154\n",
            "Epoch 38/200\n",
            "60/60 - 5s - loss: 0.2249 - mcrmse: 0.2391 - val_loss: 0.3933 - val_mcrmse: 0.2164\n",
            "Epoch 39/200\n",
            "60/60 - 5s - loss: 0.2234 - mcrmse: 0.2381 - val_loss: 0.3953 - val_mcrmse: 0.2175\n",
            "Epoch 40/200\n",
            "60/60 - 5s - loss: 0.2213 - mcrmse: 0.2368 - val_loss: 0.3927 - val_mcrmse: 0.2156\n",
            "Epoch 41/200\n",
            "60/60 - 5s - loss: 0.2191 - mcrmse: 0.2352 - val_loss: 0.3910 - val_mcrmse: 0.2156\n",
            "Epoch 42/200\n",
            "60/60 - 5s - loss: 0.2180 - mcrmse: 0.2345 - val_loss: 0.3904 - val_mcrmse: 0.2142\n",
            "Epoch 43/200\n",
            "60/60 - 5s - loss: 0.2167 - mcrmse: 0.2336 - val_loss: 0.3900 - val_mcrmse: 0.2144\n",
            "Epoch 44/200\n",
            "60/60 - 5s - loss: 0.2137 - mcrmse: 0.2319 - val_loss: 0.3917 - val_mcrmse: 0.2150\n",
            "Epoch 45/200\n",
            "60/60 - 5s - loss: 0.2121 - mcrmse: 0.2306 - val_loss: 0.3954 - val_mcrmse: 0.2164\n",
            "Epoch 46/200\n",
            "60/60 - 5s - loss: 0.2109 - mcrmse: 0.2297 - val_loss: 0.3885 - val_mcrmse: 0.2128\n",
            "Epoch 47/200\n",
            "60/60 - 5s - loss: 0.2093 - mcrmse: 0.2287 - val_loss: 0.3922 - val_mcrmse: 0.2149\n",
            "Epoch 48/200\n",
            "60/60 - 5s - loss: 0.2077 - mcrmse: 0.2276 - val_loss: 0.3910 - val_mcrmse: 0.2148\n",
            "Epoch 49/200\n",
            "60/60 - 5s - loss: 0.2059 - mcrmse: 0.2265 - val_loss: 0.3939 - val_mcrmse: 0.2155\n",
            "Epoch 50/200\n",
            "60/60 - 5s - loss: 0.2055 - mcrmse: 0.2261 - val_loss: 0.3912 - val_mcrmse: 0.2143\n",
            "Epoch 51/200\n",
            "60/60 - 5s - loss: 0.2037 - mcrmse: 0.2249 - val_loss: 0.3946 - val_mcrmse: 0.2164\n",
            "Epoch 52/200\n",
            "60/60 - 5s - loss: 0.2021 - mcrmse: 0.2238 - val_loss: 0.3921 - val_mcrmse: 0.2151\n",
            "Epoch 53/200\n",
            "60/60 - 5s - loss: 0.2003 - mcrmse: 0.2226 - val_loss: 0.3919 - val_mcrmse: 0.2152\n",
            "Epoch 54/200\n",
            "60/60 - 5s - loss: 0.1996 - mcrmse: 0.2219 - val_loss: 0.3928 - val_mcrmse: 0.2168\n",
            "Epoch 55/200\n",
            "60/60 - 5s - loss: 0.1983 - mcrmse: 0.2212 - val_loss: 0.3944 - val_mcrmse: 0.2168\n",
            "Epoch 56/200\n",
            "60/60 - 5s - loss: 0.1967 - mcrmse: 0.2200 - val_loss: 0.3913 - val_mcrmse: 0.2146\n",
            "Epoch 57/200\n",
            "60/60 - 5s - loss: 0.1957 - mcrmse: 0.2193 - val_loss: 0.3930 - val_mcrmse: 0.2155\n",
            "Epoch 58/200\n",
            "60/60 - 5s - loss: 0.1948 - mcrmse: 0.2186 - val_loss: 0.3911 - val_mcrmse: 0.2142\n",
            "Epoch 59/200\n",
            "60/60 - 5s - loss: 0.1938 - mcrmse: 0.2179 - val_loss: 0.3919 - val_mcrmse: 0.2148\n",
            "Epoch 60/200\n",
            "60/60 - 5s - loss: 0.1929 - mcrmse: 0.2171 - val_loss: 0.3914 - val_mcrmse: 0.2151\n",
            "Epoch 61/200\n",
            "60/60 - 5s - loss: 0.1914 - mcrmse: 0.2161 - val_loss: 0.3919 - val_mcrmse: 0.2153\n",
            "Epoch 62/200\n",
            "60/60 - 5s - loss: 0.1900 - mcrmse: 0.2152 - val_loss: 0.3923 - val_mcrmse: 0.2160\n",
            "Epoch 63/200\n",
            "60/60 - 5s - loss: 0.1892 - mcrmse: 0.2146 - val_loss: 0.3909 - val_mcrmse: 0.2150\n",
            "Epoch 64/200\n",
            "60/60 - 5s - loss: 0.1878 - mcrmse: 0.2136 - val_loss: 0.3932 - val_mcrmse: 0.2159\n",
            "Epoch 65/200\n",
            "60/60 - 5s - loss: 0.1870 - mcrmse: 0.2127 - val_loss: 0.3926 - val_mcrmse: 0.2162\n",
            "Epoch 66/200\n",
            "\n",
            "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 5s - loss: 0.1866 - mcrmse: 0.2124 - val_loss: 0.3915 - val_mcrmse: 0.2155\n",
            "Epoch 00066: early stopping\n",
            "##### Type 2 Fold 1 #####\n",
            "Fold 1 validation loss=0.38853657245635986\n",
            "tf.Tensor(0.2127657, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 2 Fold 2 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2932 -0.1845\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0272s vs `on_train_batch_end` time: 0.0444s). Check your callbacks.\n",
            "60/60 - 6s - loss: 0.6369 - mcrmse: 0.5201 - val_loss: 0.6005 - val_mcrmse: 0.3387\n",
            "Epoch 2/200\n",
            "60/60 - 5s - loss: 0.4599 - mcrmse: 0.3939 - val_loss: 0.5283 - val_mcrmse: 0.2934\n",
            "Epoch 3/200\n",
            "60/60 - 5s - loss: 0.4108 - mcrmse: 0.3603 - val_loss: 0.4888 - val_mcrmse: 0.2699\n",
            "Epoch 4/200\n",
            "60/60 - 5s - loss: 0.3802 - mcrmse: 0.3409 - val_loss: 0.4714 - val_mcrmse: 0.2624\n",
            "Epoch 5/200\n",
            "60/60 - 5s - loss: 0.3633 - mcrmse: 0.3304 - val_loss: 0.4677 - val_mcrmse: 0.2588\n",
            "Epoch 6/200\n",
            "60/60 - 5s - loss: 0.3520 - mcrmse: 0.3233 - val_loss: 0.4496 - val_mcrmse: 0.2461\n",
            "Epoch 7/200\n",
            "60/60 - 5s - loss: 0.3397 - mcrmse: 0.3158 - val_loss: 0.4492 - val_mcrmse: 0.2467\n",
            "Epoch 8/200\n",
            "60/60 - 5s - loss: 0.3318 - mcrmse: 0.3109 - val_loss: 0.4441 - val_mcrmse: 0.2434\n",
            "Epoch 9/200\n",
            "60/60 - 5s - loss: 0.3253 - mcrmse: 0.3071 - val_loss: 0.4395 - val_mcrmse: 0.2398\n",
            "Epoch 10/200\n",
            "60/60 - 5s - loss: 0.3180 - mcrmse: 0.3029 - val_loss: 0.4384 - val_mcrmse: 0.2376\n",
            "Epoch 11/200\n",
            "60/60 - 5s - loss: 0.3116 - mcrmse: 0.2988 - val_loss: 0.4375 - val_mcrmse: 0.2378\n",
            "Epoch 12/200\n",
            "60/60 - 5s - loss: 0.3061 - mcrmse: 0.2951 - val_loss: 0.4265 - val_mcrmse: 0.2328\n",
            "Epoch 13/200\n",
            "60/60 - 5s - loss: 0.3029 - mcrmse: 0.2934 - val_loss: 0.4262 - val_mcrmse: 0.2311\n",
            "Epoch 14/200\n",
            "60/60 - 5s - loss: 0.2963 - mcrmse: 0.2894 - val_loss: 0.4282 - val_mcrmse: 0.2320\n",
            "Epoch 15/200\n",
            "60/60 - 5s - loss: 0.2926 - mcrmse: 0.2871 - val_loss: 0.4278 - val_mcrmse: 0.2319\n",
            "Epoch 16/200\n",
            "60/60 - 5s - loss: 0.2882 - mcrmse: 0.2845 - val_loss: 0.4276 - val_mcrmse: 0.2315\n",
            "Epoch 17/200\n",
            "60/60 - 5s - loss: 0.2840 - mcrmse: 0.2818 - val_loss: 0.4287 - val_mcrmse: 0.2325\n",
            "Epoch 18/200\n",
            "60/60 - 5s - loss: 0.2801 - mcrmse: 0.2793 - val_loss: 0.4228 - val_mcrmse: 0.2290\n",
            "Epoch 19/200\n",
            "60/60 - 5s - loss: 0.2777 - mcrmse: 0.2779 - val_loss: 0.4277 - val_mcrmse: 0.2313\n",
            "Epoch 20/200\n",
            "60/60 - 5s - loss: 0.2715 - mcrmse: 0.2743 - val_loss: 0.4220 - val_mcrmse: 0.2286\n",
            "Epoch 21/200\n",
            "60/60 - 5s - loss: 0.2685 - mcrmse: 0.2724 - val_loss: 0.4165 - val_mcrmse: 0.2262\n",
            "Epoch 22/200\n",
            "60/60 - 5s - loss: 0.2654 - mcrmse: 0.2703 - val_loss: 0.4260 - val_mcrmse: 0.2309\n",
            "Epoch 23/200\n",
            "60/60 - 5s - loss: 0.2631 - mcrmse: 0.2689 - val_loss: 0.4224 - val_mcrmse: 0.2285\n",
            "Epoch 24/200\n",
            "60/60 - 5s - loss: 0.2590 - mcrmse: 0.2663 - val_loss: 0.4223 - val_mcrmse: 0.2287\n",
            "Epoch 25/200\n",
            "60/60 - 5s - loss: 0.2564 - mcrmse: 0.2647 - val_loss: 0.4225 - val_mcrmse: 0.2293\n",
            "Epoch 26/200\n",
            "60/60 - 5s - loss: 0.2541 - mcrmse: 0.2633 - val_loss: 0.4160 - val_mcrmse: 0.2250\n",
            "Epoch 27/200\n",
            "60/60 - 5s - loss: 0.2507 - mcrmse: 0.2612 - val_loss: 0.4160 - val_mcrmse: 0.2252\n",
            "Epoch 28/200\n",
            "60/60 - 5s - loss: 0.2469 - mcrmse: 0.2588 - val_loss: 0.4151 - val_mcrmse: 0.2247\n",
            "Epoch 29/200\n",
            "60/60 - 5s - loss: 0.2444 - mcrmse: 0.2574 - val_loss: 0.4221 - val_mcrmse: 0.2291\n",
            "Epoch 30/200\n",
            "60/60 - 5s - loss: 0.2423 - mcrmse: 0.2560 - val_loss: 0.4228 - val_mcrmse: 0.2280\n",
            "Epoch 31/200\n",
            "60/60 - 5s - loss: 0.2408 - mcrmse: 0.2551 - val_loss: 0.4188 - val_mcrmse: 0.2264\n",
            "Epoch 32/200\n",
            "60/60 - 5s - loss: 0.2384 - mcrmse: 0.2534 - val_loss: 0.4175 - val_mcrmse: 0.2265\n",
            "Epoch 33/200\n",
            "60/60 - 5s - loss: 0.2355 - mcrmse: 0.2517 - val_loss: 0.4164 - val_mcrmse: 0.2249\n",
            "Epoch 34/200\n",
            "60/60 - 5s - loss: 0.2336 - mcrmse: 0.2505 - val_loss: 0.4133 - val_mcrmse: 0.2236\n",
            "Epoch 35/200\n",
            "60/60 - 5s - loss: 0.2304 - mcrmse: 0.2485 - val_loss: 0.4159 - val_mcrmse: 0.2250\n",
            "Epoch 36/200\n",
            "60/60 - 5s - loss: 0.2280 - mcrmse: 0.2469 - val_loss: 0.4171 - val_mcrmse: 0.2261\n",
            "Epoch 37/200\n",
            "60/60 - 5s - loss: 0.2268 - mcrmse: 0.2461 - val_loss: 0.4193 - val_mcrmse: 0.2278\n",
            "Epoch 38/200\n",
            "60/60 - 5s - loss: 0.2240 - mcrmse: 0.2444 - val_loss: 0.4149 - val_mcrmse: 0.2246\n",
            "Epoch 39/200\n",
            "60/60 - 5s - loss: 0.2247 - mcrmse: 0.2446 - val_loss: 0.4204 - val_mcrmse: 0.2283\n",
            "Epoch 40/200\n",
            "60/60 - 5s - loss: 0.2218 - mcrmse: 0.2428 - val_loss: 0.4169 - val_mcrmse: 0.2260\n",
            "Epoch 41/200\n",
            "60/60 - 5s - loss: 0.2180 - mcrmse: 0.2405 - val_loss: 0.4147 - val_mcrmse: 0.2246\n",
            "Epoch 42/200\n",
            "60/60 - 5s - loss: 0.2171 - mcrmse: 0.2397 - val_loss: 0.4126 - val_mcrmse: 0.2237\n",
            "Epoch 43/200\n",
            "60/60 - 5s - loss: 0.2150 - mcrmse: 0.2383 - val_loss: 0.4211 - val_mcrmse: 0.2287\n",
            "Epoch 44/200\n",
            "60/60 - 5s - loss: 0.2143 - mcrmse: 0.2379 - val_loss: 0.4181 - val_mcrmse: 0.2269\n",
            "Epoch 45/200\n",
            "60/60 - 5s - loss: 0.2130 - mcrmse: 0.2370 - val_loss: 0.4107 - val_mcrmse: 0.2224\n",
            "Epoch 46/200\n",
            "60/60 - 5s - loss: 0.2094 - mcrmse: 0.2346 - val_loss: 0.4125 - val_mcrmse: 0.2236\n",
            "Epoch 47/200\n",
            "60/60 - 5s - loss: 0.2089 - mcrmse: 0.2343 - val_loss: 0.4182 - val_mcrmse: 0.2260\n",
            "Epoch 48/200\n",
            "60/60 - 5s - loss: 0.2081 - mcrmse: 0.2338 - val_loss: 0.4189 - val_mcrmse: 0.2271\n",
            "Epoch 49/200\n",
            "60/60 - 5s - loss: 0.2055 - mcrmse: 0.2319 - val_loss: 0.4170 - val_mcrmse: 0.2263\n",
            "Epoch 50/200\n",
            "60/60 - 5s - loss: 0.2041 - mcrmse: 0.2310 - val_loss: 0.4133 - val_mcrmse: 0.2238\n",
            "Epoch 51/200\n",
            "60/60 - 5s - loss: 0.2037 - mcrmse: 0.2305 - val_loss: 0.4152 - val_mcrmse: 0.2245\n",
            "Epoch 52/200\n",
            "60/60 - 5s - loss: 0.2012 - mcrmse: 0.2290 - val_loss: 0.4117 - val_mcrmse: 0.2232\n",
            "Epoch 53/200\n",
            "60/60 - 5s - loss: 0.1996 - mcrmse: 0.2280 - val_loss: 0.4146 - val_mcrmse: 0.2251\n",
            "Epoch 54/200\n",
            "60/60 - 5s - loss: 0.1992 - mcrmse: 0.2275 - val_loss: 0.4149 - val_mcrmse: 0.2250\n",
            "Epoch 55/200\n",
            "60/60 - 5s - loss: 0.1972 - mcrmse: 0.2259 - val_loss: 0.4175 - val_mcrmse: 0.2271\n",
            "Epoch 56/200\n",
            "60/60 - 5s - loss: 0.1966 - mcrmse: 0.2256 - val_loss: 0.4199 - val_mcrmse: 0.2273\n",
            "Epoch 57/200\n",
            "60/60 - 5s - loss: 0.1967 - mcrmse: 0.2254 - val_loss: 0.4150 - val_mcrmse: 0.2250\n",
            "Epoch 58/200\n",
            "60/60 - 5s - loss: 0.1947 - mcrmse: 0.2241 - val_loss: 0.4148 - val_mcrmse: 0.2253\n",
            "Epoch 59/200\n",
            "60/60 - 5s - loss: 0.1927 - mcrmse: 0.2227 - val_loss: 0.4183 - val_mcrmse: 0.2267\n",
            "Epoch 60/200\n",
            "60/60 - 5s - loss: 0.1922 - mcrmse: 0.2223 - val_loss: 0.4157 - val_mcrmse: 0.2255\n",
            "Epoch 61/200\n",
            "60/60 - 5s - loss: 0.1909 - mcrmse: 0.2213 - val_loss: 0.4177 - val_mcrmse: 0.2262\n",
            "Epoch 62/200\n",
            "60/60 - 5s - loss: 0.1895 - mcrmse: 0.2204 - val_loss: 0.4127 - val_mcrmse: 0.2238\n",
            "Epoch 63/200\n",
            "60/60 - 5s - loss: 0.1882 - mcrmse: 0.2195 - val_loss: 0.4160 - val_mcrmse: 0.2252\n",
            "Epoch 64/200\n",
            "60/60 - 5s - loss: 0.1876 - mcrmse: 0.2191 - val_loss: 0.4172 - val_mcrmse: 0.2262\n",
            "Epoch 65/200\n",
            "\n",
            "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 5s - loss: 0.1866 - mcrmse: 0.2183 - val_loss: 0.4140 - val_mcrmse: 0.2240\n",
            "Epoch 00065: early stopping\n",
            "##### Type 2 Fold 2 #####\n",
            "Fold 2 validation loss=0.41071411967277527\n",
            "tf.Tensor(0.22244331, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 2 Fold 3 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.155131100000162 -0.18742859999999997\n",
            "Epoch 1/200\n",
            "60/60 - 6s - loss: 0.6473 - mcrmse: 0.5122 - val_loss: 0.6049 - val_mcrmse: 0.3470\n",
            "Epoch 2/200\n",
            "60/60 - 5s - loss: 0.4680 - mcrmse: 0.3873 - val_loss: 0.5171 - val_mcrmse: 0.2876\n",
            "Epoch 3/200\n",
            "60/60 - 5s - loss: 0.4169 - mcrmse: 0.3515 - val_loss: 0.4833 - val_mcrmse: 0.2679\n",
            "Epoch 4/200\n",
            "60/60 - 5s - loss: 0.3865 - mcrmse: 0.3315 - val_loss: 0.4569 - val_mcrmse: 0.2517\n",
            "Epoch 5/200\n",
            "60/60 - 5s - loss: 0.3699 - mcrmse: 0.3211 - val_loss: 0.4473 - val_mcrmse: 0.2466\n",
            "Epoch 6/200\n",
            "60/60 - 5s - loss: 0.3574 - mcrmse: 0.3130 - val_loss: 0.4416 - val_mcrmse: 0.2422\n",
            "Epoch 7/200\n",
            "60/60 - 5s - loss: 0.3493 - mcrmse: 0.3079 - val_loss: 0.4390 - val_mcrmse: 0.2404\n",
            "Epoch 8/200\n",
            "60/60 - 5s - loss: 0.3400 - mcrmse: 0.3027 - val_loss: 0.4398 - val_mcrmse: 0.2406\n",
            "Epoch 9/200\n",
            "60/60 - 5s - loss: 0.3318 - mcrmse: 0.2971 - val_loss: 0.4222 - val_mcrmse: 0.2316\n",
            "Epoch 10/200\n",
            "60/60 - 5s - loss: 0.3238 - mcrmse: 0.2922 - val_loss: 0.4153 - val_mcrmse: 0.2278\n",
            "Epoch 11/200\n",
            "60/60 - 5s - loss: 0.3182 - mcrmse: 0.2886 - val_loss: 0.4174 - val_mcrmse: 0.2284\n",
            "Epoch 12/200\n",
            "60/60 - 5s - loss: 0.3119 - mcrmse: 0.2848 - val_loss: 0.4168 - val_mcrmse: 0.2278\n",
            "Epoch 13/200\n",
            "60/60 - 5s - loss: 0.3132 - mcrmse: 0.2852 - val_loss: 0.4094 - val_mcrmse: 0.2244\n",
            "Epoch 14/200\n",
            "60/60 - 5s - loss: 0.3016 - mcrmse: 0.2782 - val_loss: 0.4069 - val_mcrmse: 0.2230\n",
            "Epoch 15/200\n",
            "60/60 - 5s - loss: 0.2984 - mcrmse: 0.2762 - val_loss: 0.4044 - val_mcrmse: 0.2216\n",
            "Epoch 16/200\n",
            "60/60 - 5s - loss: 0.2939 - mcrmse: 0.2734 - val_loss: 0.4037 - val_mcrmse: 0.2208\n",
            "Epoch 17/200\n",
            "60/60 - 4s - loss: 0.2893 - mcrmse: 0.2705 - val_loss: 0.4042 - val_mcrmse: 0.2212\n",
            "Epoch 18/200\n",
            "60/60 - 5s - loss: 0.2868 - mcrmse: 0.2687 - val_loss: 0.3998 - val_mcrmse: 0.2191\n",
            "Epoch 19/200\n",
            "60/60 - 5s - loss: 0.2802 - mcrmse: 0.2645 - val_loss: 0.4014 - val_mcrmse: 0.2201\n",
            "Epoch 20/200\n",
            "60/60 - 5s - loss: 0.2790 - mcrmse: 0.2637 - val_loss: 0.3995 - val_mcrmse: 0.2185\n",
            "Epoch 21/200\n",
            "60/60 - 5s - loss: 0.2731 - mcrmse: 0.2601 - val_loss: 0.3946 - val_mcrmse: 0.2164\n",
            "Epoch 22/200\n",
            "60/60 - 5s - loss: 0.2703 - mcrmse: 0.2581 - val_loss: 0.3992 - val_mcrmse: 0.2183\n",
            "Epoch 23/200\n",
            "60/60 - 5s - loss: 0.2672 - mcrmse: 0.2561 - val_loss: 0.3947 - val_mcrmse: 0.2160\n",
            "Epoch 24/200\n",
            "60/60 - 5s - loss: 0.2651 - mcrmse: 0.2547 - val_loss: 0.3954 - val_mcrmse: 0.2168\n",
            "Epoch 25/200\n",
            "60/60 - 5s - loss: 0.2612 - mcrmse: 0.2523 - val_loss: 0.3987 - val_mcrmse: 0.2181\n",
            "Epoch 26/200\n",
            "60/60 - 5s - loss: 0.2590 - mcrmse: 0.2511 - val_loss: 0.3930 - val_mcrmse: 0.2152\n",
            "Epoch 27/200\n",
            "60/60 - 5s - loss: 0.2555 - mcrmse: 0.2486 - val_loss: 0.3944 - val_mcrmse: 0.2159\n",
            "Epoch 28/200\n",
            "60/60 - 5s - loss: 0.2533 - mcrmse: 0.2472 - val_loss: 0.3945 - val_mcrmse: 0.2163\n",
            "Epoch 29/200\n",
            "60/60 - 5s - loss: 0.2506 - mcrmse: 0.2455 - val_loss: 0.3962 - val_mcrmse: 0.2163\n",
            "Epoch 30/200\n",
            "60/60 - 5s - loss: 0.2491 - mcrmse: 0.2446 - val_loss: 0.3916 - val_mcrmse: 0.2154\n",
            "Epoch 31/200\n",
            "60/60 - 5s - loss: 0.2454 - mcrmse: 0.2423 - val_loss: 0.3917 - val_mcrmse: 0.2148\n",
            "Epoch 32/200\n",
            "60/60 - 5s - loss: 0.2424 - mcrmse: 0.2404 - val_loss: 0.3912 - val_mcrmse: 0.2143\n",
            "Epoch 33/200\n",
            "60/60 - 5s - loss: 0.2415 - mcrmse: 0.2396 - val_loss: 0.3957 - val_mcrmse: 0.2169\n",
            "Epoch 34/200\n",
            "60/60 - 5s - loss: 0.2388 - mcrmse: 0.2378 - val_loss: 0.3882 - val_mcrmse: 0.2130\n",
            "Epoch 35/200\n",
            "60/60 - 5s - loss: 0.2373 - mcrmse: 0.2370 - val_loss: 0.3895 - val_mcrmse: 0.2134\n",
            "Epoch 36/200\n",
            "60/60 - 5s - loss: 0.2343 - mcrmse: 0.2349 - val_loss: 0.3923 - val_mcrmse: 0.2152\n",
            "Epoch 37/200\n",
            "60/60 - 5s - loss: 0.2323 - mcrmse: 0.2336 - val_loss: 0.3899 - val_mcrmse: 0.2137\n",
            "Epoch 38/200\n",
            "60/60 - 5s - loss: 0.2305 - mcrmse: 0.2321 - val_loss: 0.3868 - val_mcrmse: 0.2125\n",
            "Epoch 39/200\n",
            "60/60 - 5s - loss: 0.2287 - mcrmse: 0.2311 - val_loss: 0.3885 - val_mcrmse: 0.2132\n",
            "Epoch 40/200\n",
            "60/60 - 5s - loss: 0.2261 - mcrmse: 0.2296 - val_loss: 0.3865 - val_mcrmse: 0.2124\n",
            "Epoch 41/200\n",
            "60/60 - 5s - loss: 0.2235 - mcrmse: 0.2278 - val_loss: 0.3874 - val_mcrmse: 0.2130\n",
            "Epoch 42/200\n",
            "60/60 - 5s - loss: 0.2228 - mcrmse: 0.2272 - val_loss: 0.3881 - val_mcrmse: 0.2135\n",
            "Epoch 43/200\n",
            "60/60 - 5s - loss: 0.2211 - mcrmse: 0.2260 - val_loss: 0.3893 - val_mcrmse: 0.2137\n",
            "Epoch 44/200\n",
            "60/60 - 5s - loss: 0.2194 - mcrmse: 0.2248 - val_loss: 0.3856 - val_mcrmse: 0.2117\n",
            "Epoch 45/200\n",
            "60/60 - 5s - loss: 0.2173 - mcrmse: 0.2235 - val_loss: 0.3873 - val_mcrmse: 0.2127\n",
            "Epoch 46/200\n",
            "60/60 - 5s - loss: 0.2160 - mcrmse: 0.2226 - val_loss: 0.3862 - val_mcrmse: 0.2119\n",
            "Epoch 47/200\n",
            "60/60 - 5s - loss: 0.2145 - mcrmse: 0.2214 - val_loss: 0.3879 - val_mcrmse: 0.2125\n",
            "Epoch 48/200\n",
            "60/60 - 5s - loss: 0.2130 - mcrmse: 0.2203 - val_loss: 0.3869 - val_mcrmse: 0.2124\n",
            "Epoch 49/200\n",
            "60/60 - 5s - loss: 0.2117 - mcrmse: 0.2194 - val_loss: 0.3897 - val_mcrmse: 0.2144\n",
            "Epoch 50/200\n",
            "60/60 - 5s - loss: 0.2104 - mcrmse: 0.2186 - val_loss: 0.3928 - val_mcrmse: 0.2150\n",
            "Epoch 51/200\n",
            "60/60 - 5s - loss: 0.2098 - mcrmse: 0.2181 - val_loss: 0.3887 - val_mcrmse: 0.2133\n",
            "Epoch 52/200\n",
            "60/60 - 5s - loss: 0.2070 - mcrmse: 0.2161 - val_loss: 0.3876 - val_mcrmse: 0.2128\n",
            "Epoch 53/200\n",
            "60/60 - 5s - loss: 0.2053 - mcrmse: 0.2149 - val_loss: 0.3874 - val_mcrmse: 0.2130\n",
            "Epoch 54/200\n",
            "60/60 - 5s - loss: 0.2035 - mcrmse: 0.2136 - val_loss: 0.3879 - val_mcrmse: 0.2130\n",
            "Epoch 55/200\n",
            "60/60 - 5s - loss: 0.2025 - mcrmse: 0.2130 - val_loss: 0.3907 - val_mcrmse: 0.2146\n",
            "Epoch 56/200\n",
            "60/60 - 5s - loss: 0.2022 - mcrmse: 0.2127 - val_loss: 0.3874 - val_mcrmse: 0.2128\n",
            "Epoch 57/200\n",
            "60/60 - 5s - loss: 0.2004 - mcrmse: 0.2116 - val_loss: 0.3893 - val_mcrmse: 0.2136\n",
            "Epoch 58/200\n",
            "60/60 - 5s - loss: 0.1990 - mcrmse: 0.2105 - val_loss: 0.3895 - val_mcrmse: 0.2137\n",
            "Epoch 59/200\n",
            "60/60 - 5s - loss: 0.1990 - mcrmse: 0.2104 - val_loss: 0.3885 - val_mcrmse: 0.2129\n",
            "Epoch 60/200\n",
            "60/60 - 5s - loss: 0.1973 - mcrmse: 0.2091 - val_loss: 0.3888 - val_mcrmse: 0.2134\n",
            "Epoch 61/200\n",
            "60/60 - 5s - loss: 0.1965 - mcrmse: 0.2086 - val_loss: 0.3874 - val_mcrmse: 0.2125\n",
            "Epoch 62/200\n",
            "60/60 - 5s - loss: 0.1950 - mcrmse: 0.2076 - val_loss: 0.3897 - val_mcrmse: 0.2135\n",
            "Epoch 63/200\n",
            "60/60 - 5s - loss: 0.1942 - mcrmse: 0.2070 - val_loss: 0.3879 - val_mcrmse: 0.2126\n",
            "Epoch 64/200\n",
            "\n",
            "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 5s - loss: 0.1932 - mcrmse: 0.2062 - val_loss: 0.3904 - val_mcrmse: 0.2140\n",
            "Epoch 00064: early stopping\n",
            "##### Type 2 Fold 3 #####\n",
            "Fold 3 validation loss=0.3855770528316498\n",
            "tf.Tensor(0.21165743, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 2 Fold 4 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2344 -0.186\n",
            "Epoch 1/200\n",
            "60/60 - 7s - loss: 0.6096 - mcrmse: 0.5008 - val_loss: 0.5996 - val_mcrmse: 0.3370\n",
            "Epoch 2/200\n",
            "60/60 - 5s - loss: 0.4563 - mcrmse: 0.3882 - val_loss: 0.5021 - val_mcrmse: 0.2760\n",
            "Epoch 3/200\n",
            "60/60 - 5s - loss: 0.4049 - mcrmse: 0.3527 - val_loss: 0.4717 - val_mcrmse: 0.2565\n",
            "Epoch 4/200\n",
            "60/60 - 5s - loss: 0.3814 - mcrmse: 0.3381 - val_loss: 0.4549 - val_mcrmse: 0.2476\n",
            "Epoch 5/200\n",
            "60/60 - 5s - loss: 0.3627 - mcrmse: 0.3265 - val_loss: 0.4411 - val_mcrmse: 0.2401\n",
            "Epoch 6/200\n",
            "60/60 - 5s - loss: 0.3529 - mcrmse: 0.3205 - val_loss: 0.4369 - val_mcrmse: 0.2371\n",
            "Epoch 7/200\n",
            "60/60 - 5s - loss: 0.3392 - mcrmse: 0.3122 - val_loss: 0.4316 - val_mcrmse: 0.2338\n",
            "Epoch 8/200\n",
            "60/60 - 5s - loss: 0.3316 - mcrmse: 0.3073 - val_loss: 0.4236 - val_mcrmse: 0.2288\n",
            "Epoch 9/200\n",
            "60/60 - 5s - loss: 0.3234 - mcrmse: 0.3026 - val_loss: 0.4191 - val_mcrmse: 0.2268\n",
            "Epoch 10/200\n",
            "60/60 - 5s - loss: 0.3173 - mcrmse: 0.2988 - val_loss: 0.4194 - val_mcrmse: 0.2278\n",
            "Epoch 11/200\n",
            "60/60 - 5s - loss: 0.3091 - mcrmse: 0.2938 - val_loss: 0.4160 - val_mcrmse: 0.2259\n",
            "Epoch 12/200\n",
            "60/60 - 5s - loss: 0.3043 - mcrmse: 0.2906 - val_loss: 0.4111 - val_mcrmse: 0.2226\n",
            "Epoch 13/200\n",
            "60/60 - 5s - loss: 0.2998 - mcrmse: 0.2877 - val_loss: 0.4053 - val_mcrmse: 0.2193\n",
            "Epoch 14/200\n",
            "60/60 - 5s - loss: 0.2941 - mcrmse: 0.2842 - val_loss: 0.4039 - val_mcrmse: 0.2185\n",
            "Epoch 15/200\n",
            "60/60 - 5s - loss: 0.2901 - mcrmse: 0.2817 - val_loss: 0.4126 - val_mcrmse: 0.2232\n",
            "Epoch 16/200\n",
            "60/60 - 5s - loss: 0.2854 - mcrmse: 0.2789 - val_loss: 0.4037 - val_mcrmse: 0.2185\n",
            "Epoch 17/200\n",
            "60/60 - 5s - loss: 0.2809 - mcrmse: 0.2760 - val_loss: 0.4025 - val_mcrmse: 0.2178\n",
            "Epoch 18/200\n",
            "60/60 - 5s - loss: 0.2776 - mcrmse: 0.2739 - val_loss: 0.4002 - val_mcrmse: 0.2158\n",
            "Epoch 19/200\n",
            "60/60 - 5s - loss: 0.2732 - mcrmse: 0.2710 - val_loss: 0.4006 - val_mcrmse: 0.2165\n",
            "Epoch 20/200\n",
            "60/60 - 5s - loss: 0.2701 - mcrmse: 0.2690 - val_loss: 0.3940 - val_mcrmse: 0.2129\n",
            "Epoch 21/200\n",
            "60/60 - 5s - loss: 0.2666 - mcrmse: 0.2669 - val_loss: 0.3958 - val_mcrmse: 0.2138\n",
            "Epoch 22/200\n",
            "60/60 - 5s - loss: 0.2635 - mcrmse: 0.2648 - val_loss: 0.3945 - val_mcrmse: 0.2129\n",
            "Epoch 23/200\n",
            "60/60 - 5s - loss: 0.2611 - mcrmse: 0.2634 - val_loss: 0.3950 - val_mcrmse: 0.2132\n",
            "Epoch 24/200\n",
            "60/60 - 5s - loss: 0.2572 - mcrmse: 0.2607 - val_loss: 0.3998 - val_mcrmse: 0.2155\n",
            "Epoch 25/200\n",
            "60/60 - 5s - loss: 0.2540 - mcrmse: 0.2588 - val_loss: 0.3901 - val_mcrmse: 0.2107\n",
            "Epoch 26/200\n",
            "60/60 - 5s - loss: 0.2522 - mcrmse: 0.2575 - val_loss: 0.3912 - val_mcrmse: 0.2114\n",
            "Epoch 27/200\n",
            "60/60 - 5s - loss: 0.2486 - mcrmse: 0.2554 - val_loss: 0.3900 - val_mcrmse: 0.2107\n",
            "Epoch 28/200\n",
            "60/60 - 5s - loss: 0.2469 - mcrmse: 0.2543 - val_loss: 0.3915 - val_mcrmse: 0.2112\n",
            "Epoch 29/200\n",
            "60/60 - 5s - loss: 0.2440 - mcrmse: 0.2524 - val_loss: 0.3935 - val_mcrmse: 0.2127\n",
            "Epoch 30/200\n",
            "60/60 - 5s - loss: 0.2420 - mcrmse: 0.2510 - val_loss: 0.3909 - val_mcrmse: 0.2111\n",
            "Epoch 31/200\n",
            "60/60 - 5s - loss: 0.2394 - mcrmse: 0.2492 - val_loss: 0.3901 - val_mcrmse: 0.2106\n",
            "Epoch 32/200\n",
            "60/60 - 5s - loss: 0.2356 - mcrmse: 0.2471 - val_loss: 0.3860 - val_mcrmse: 0.2084\n",
            "Epoch 33/200\n",
            "60/60 - 5s - loss: 0.2341 - mcrmse: 0.2460 - val_loss: 0.3899 - val_mcrmse: 0.2103\n",
            "Epoch 34/200\n",
            "60/60 - 5s - loss: 0.2321 - mcrmse: 0.2447 - val_loss: 0.3880 - val_mcrmse: 0.2098\n",
            "Epoch 35/200\n",
            "60/60 - 5s - loss: 0.2305 - mcrmse: 0.2435 - val_loss: 0.3865 - val_mcrmse: 0.2092\n",
            "Epoch 36/200\n",
            "60/60 - 5s - loss: 0.2274 - mcrmse: 0.2415 - val_loss: 0.3853 - val_mcrmse: 0.2080\n",
            "Epoch 37/200\n",
            "60/60 - 5s - loss: 0.2258 - mcrmse: 0.2405 - val_loss: 0.3839 - val_mcrmse: 0.2072\n",
            "Epoch 38/200\n",
            "60/60 - 5s - loss: 0.2238 - mcrmse: 0.2393 - val_loss: 0.3908 - val_mcrmse: 0.2109\n",
            "Epoch 39/200\n",
            "60/60 - 5s - loss: 0.2234 - mcrmse: 0.2388 - val_loss: 0.3881 - val_mcrmse: 0.2095\n",
            "Epoch 40/200\n",
            "60/60 - 5s - loss: 0.2207 - mcrmse: 0.2371 - val_loss: 0.3852 - val_mcrmse: 0.2079\n",
            "Epoch 41/200\n",
            "60/60 - 5s - loss: 0.2182 - mcrmse: 0.2355 - val_loss: 0.3869 - val_mcrmse: 0.2088\n",
            "Epoch 42/200\n",
            "60/60 - 5s - loss: 0.2175 - mcrmse: 0.2349 - val_loss: 0.3857 - val_mcrmse: 0.2080\n",
            "Epoch 43/200\n",
            "60/60 - 5s - loss: 0.2162 - mcrmse: 0.2339 - val_loss: 0.3896 - val_mcrmse: 0.2099\n",
            "Epoch 44/200\n",
            "60/60 - 5s - loss: 0.2150 - mcrmse: 0.2331 - val_loss: 0.3886 - val_mcrmse: 0.2097\n",
            "Epoch 45/200\n",
            "60/60 - 5s - loss: 0.2119 - mcrmse: 0.2312 - val_loss: 0.3891 - val_mcrmse: 0.2104\n",
            "Epoch 46/200\n",
            "60/60 - 5s - loss: 0.2105 - mcrmse: 0.2301 - val_loss: 0.3868 - val_mcrmse: 0.2085\n",
            "Epoch 47/200\n",
            "60/60 - 5s - loss: 0.2086 - mcrmse: 0.2289 - val_loss: 0.3879 - val_mcrmse: 0.2095\n",
            "Epoch 48/200\n",
            "60/60 - 5s - loss: 0.2078 - mcrmse: 0.2283 - val_loss: 0.3889 - val_mcrmse: 0.2104\n",
            "Epoch 49/200\n",
            "60/60 - 5s - loss: 0.2055 - mcrmse: 0.2267 - val_loss: 0.3915 - val_mcrmse: 0.2113\n",
            "Epoch 50/200\n",
            "60/60 - 5s - loss: 0.2041 - mcrmse: 0.2257 - val_loss: 0.3904 - val_mcrmse: 0.2106\n",
            "Epoch 51/200\n",
            "60/60 - 5s - loss: 0.2029 - mcrmse: 0.2249 - val_loss: 0.3883 - val_mcrmse: 0.2098\n",
            "Epoch 52/200\n",
            "60/60 - 5s - loss: 0.2027 - mcrmse: 0.2245 - val_loss: 0.3886 - val_mcrmse: 0.2100\n",
            "Epoch 53/200\n",
            "60/60 - 5s - loss: 0.2009 - mcrmse: 0.2233 - val_loss: 0.3860 - val_mcrmse: 0.2091\n",
            "Epoch 54/200\n",
            "60/60 - 5s - loss: 0.1991 - mcrmse: 0.2222 - val_loss: 0.3886 - val_mcrmse: 0.2098\n",
            "Epoch 55/200\n",
            "60/60 - 5s - loss: 0.1981 - mcrmse: 0.2214 - val_loss: 0.3925 - val_mcrmse: 0.2119\n",
            "Epoch 56/200\n",
            "60/60 - 5s - loss: 0.1970 - mcrmse: 0.2205 - val_loss: 0.3877 - val_mcrmse: 0.2092\n",
            "Epoch 57/200\n",
            "\n",
            "Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 5s - loss: 0.1954 - mcrmse: 0.2194 - val_loss: 0.3885 - val_mcrmse: 0.2094\n",
            "Epoch 00057: early stopping\n",
            "##### Type 2 Fold 4 #####\n",
            "Fold 4 validation loss=0.3838690221309662\n",
            "tf.Tensor(0.20721185, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 3 Fold 0 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.1759 -0.1961\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.6596 - mcrmse: 0.5080 - val_loss: 0.5954 - val_mcrmse: 0.3303\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4642 - mcrmse: 0.3734 - val_loss: 0.5322 - val_mcrmse: 0.2902\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4261 - mcrmse: 0.3491 - val_loss: 0.4928 - val_mcrmse: 0.2686\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3999 - mcrmse: 0.3330 - val_loss: 0.4607 - val_mcrmse: 0.2522\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3761 - mcrmse: 0.3185 - val_loss: 0.4480 - val_mcrmse: 0.2444\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3613 - mcrmse: 0.3093 - val_loss: 0.4396 - val_mcrmse: 0.2407\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3496 - mcrmse: 0.3024 - val_loss: 0.4407 - val_mcrmse: 0.2412\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3387 - mcrmse: 0.2958 - val_loss: 0.4332 - val_mcrmse: 0.2345\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3295 - mcrmse: 0.2903 - val_loss: 0.4147 - val_mcrmse: 0.2256\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3226 - mcrmse: 0.2861 - val_loss: 0.4218 - val_mcrmse: 0.2288\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3117 - mcrmse: 0.2792 - val_loss: 0.4122 - val_mcrmse: 0.2249\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3092 - mcrmse: 0.2776 - val_loss: 0.4054 - val_mcrmse: 0.2213\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.3038 - mcrmse: 0.2742 - val_loss: 0.4092 - val_mcrmse: 0.2223\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.2962 - mcrmse: 0.2697 - val_loss: 0.4034 - val_mcrmse: 0.2194\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2893 - mcrmse: 0.2656 - val_loss: 0.3969 - val_mcrmse: 0.2165\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2850 - mcrmse: 0.2629 - val_loss: 0.4064 - val_mcrmse: 0.2208\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2807 - mcrmse: 0.2602 - val_loss: 0.3983 - val_mcrmse: 0.2180\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2764 - mcrmse: 0.2576 - val_loss: 0.3935 - val_mcrmse: 0.2146\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2721 - mcrmse: 0.2547 - val_loss: 0.3959 - val_mcrmse: 0.2167\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2695 - mcrmse: 0.2531 - val_loss: 0.3979 - val_mcrmse: 0.2171\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2639 - mcrmse: 0.2497 - val_loss: 0.3974 - val_mcrmse: 0.2175\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2611 - mcrmse: 0.2479 - val_loss: 0.3924 - val_mcrmse: 0.2150\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2565 - mcrmse: 0.2452 - val_loss: 0.3940 - val_mcrmse: 0.2163\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2552 - mcrmse: 0.2440 - val_loss: 0.3935 - val_mcrmse: 0.2140\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2520 - mcrmse: 0.2423 - val_loss: 0.3919 - val_mcrmse: 0.2132\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2482 - mcrmse: 0.2399 - val_loss: 0.3935 - val_mcrmse: 0.2152\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2461 - mcrmse: 0.2384 - val_loss: 0.3949 - val_mcrmse: 0.2148\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2422 - mcrmse: 0.2361 - val_loss: 0.3936 - val_mcrmse: 0.2143\n",
            "Epoch 29/200\n",
            "60/60 - 7s - loss: 0.2399 - mcrmse: 0.2346 - val_loss: 0.3966 - val_mcrmse: 0.2170\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2378 - mcrmse: 0.2331 - val_loss: 0.3949 - val_mcrmse: 0.2152\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2358 - mcrmse: 0.2317 - val_loss: 0.3948 - val_mcrmse: 0.2145\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2327 - mcrmse: 0.2299 - val_loss: 0.3906 - val_mcrmse: 0.2129\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2302 - mcrmse: 0.2282 - val_loss: 0.3930 - val_mcrmse: 0.2143\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2287 - mcrmse: 0.2270 - val_loss: 0.3926 - val_mcrmse: 0.2132\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2265 - mcrmse: 0.2257 - val_loss: 0.3897 - val_mcrmse: 0.2122\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2237 - mcrmse: 0.2237 - val_loss: 0.3895 - val_mcrmse: 0.2123\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2227 - mcrmse: 0.2228 - val_loss: 0.3902 - val_mcrmse: 0.2133\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2205 - mcrmse: 0.2218 - val_loss: 0.3951 - val_mcrmse: 0.2150\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2194 - mcrmse: 0.2206 - val_loss: 0.3893 - val_mcrmse: 0.2120\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2173 - mcrmse: 0.2192 - val_loss: 0.3904 - val_mcrmse: 0.2129\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2153 - mcrmse: 0.2180 - val_loss: 0.3915 - val_mcrmse: 0.2134\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2137 - mcrmse: 0.2169 - val_loss: 0.3939 - val_mcrmse: 0.2148\n",
            "Epoch 43/200\n",
            "60/60 - 7s - loss: 0.2125 - mcrmse: 0.2160 - val_loss: 0.3913 - val_mcrmse: 0.2137\n",
            "Epoch 44/200\n",
            "60/60 - 7s - loss: 0.2117 - mcrmse: 0.2153 - val_loss: 0.3900 - val_mcrmse: 0.2123\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.2087 - mcrmse: 0.2135 - val_loss: 0.3877 - val_mcrmse: 0.2112\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.2066 - mcrmse: 0.2119 - val_loss: 0.3872 - val_mcrmse: 0.2125\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.2060 - mcrmse: 0.2115 - val_loss: 0.3922 - val_mcrmse: 0.2145\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.2041 - mcrmse: 0.2102 - val_loss: 0.3864 - val_mcrmse: 0.2111\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.2040 - mcrmse: 0.2099 - val_loss: 0.3915 - val_mcrmse: 0.2136\n",
            "Epoch 50/200\n",
            "60/60 - 7s - loss: 0.2028 - mcrmse: 0.2090 - val_loss: 0.3889 - val_mcrmse: 0.2119\n",
            "Epoch 51/200\n",
            "60/60 - 7s - loss: 0.2008 - mcrmse: 0.2076 - val_loss: 0.3883 - val_mcrmse: 0.2127\n",
            "Epoch 52/200\n",
            "60/60 - 7s - loss: 0.1984 - mcrmse: 0.2061 - val_loss: 0.3877 - val_mcrmse: 0.2115\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.1974 - mcrmse: 0.2053 - val_loss: 0.3915 - val_mcrmse: 0.2141\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.1976 - mcrmse: 0.2052 - val_loss: 0.3959 - val_mcrmse: 0.2161\n",
            "Epoch 55/200\n",
            "60/60 - 7s - loss: 0.1958 - mcrmse: 0.2040 - val_loss: 0.3889 - val_mcrmse: 0.2118\n",
            "Epoch 56/200\n",
            "60/60 - 7s - loss: 0.1933 - mcrmse: 0.2023 - val_loss: 0.3902 - val_mcrmse: 0.2133\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.1920 - mcrmse: 0.2013 - val_loss: 0.3880 - val_mcrmse: 0.2118\n",
            "Epoch 58/200\n",
            "60/60 - 7s - loss: 0.1907 - mcrmse: 0.2007 - val_loss: 0.3908 - val_mcrmse: 0.2136\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.1907 - mcrmse: 0.2003 - val_loss: 0.3881 - val_mcrmse: 0.2120\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.1897 - mcrmse: 0.1996 - val_loss: 0.3880 - val_mcrmse: 0.2112\n",
            "Epoch 61/200\n",
            "60/60 - 7s - loss: 0.1888 - mcrmse: 0.1990 - val_loss: 0.3918 - val_mcrmse: 0.2137\n",
            "Epoch 62/200\n",
            "60/60 - 7s - loss: 0.1867 - mcrmse: 0.1976 - val_loss: 0.3872 - val_mcrmse: 0.2108\n",
            "Epoch 63/200\n",
            "60/60 - 7s - loss: 0.1850 - mcrmse: 0.1964 - val_loss: 0.3882 - val_mcrmse: 0.2119\n",
            "Epoch 64/200\n",
            "60/60 - 7s - loss: 0.1844 - mcrmse: 0.1959 - val_loss: 0.3870 - val_mcrmse: 0.2111\n",
            "Epoch 65/200\n",
            "60/60 - 7s - loss: 0.1842 - mcrmse: 0.1953 - val_loss: 0.3883 - val_mcrmse: 0.2114\n",
            "Epoch 66/200\n",
            "60/60 - 7s - loss: 0.1830 - mcrmse: 0.1945 - val_loss: 0.3898 - val_mcrmse: 0.2123\n",
            "Epoch 67/200\n",
            "60/60 - 7s - loss: 0.1813 - mcrmse: 0.1935 - val_loss: 0.3886 - val_mcrmse: 0.2120\n",
            "Epoch 68/200\n",
            "60/60 - 7s - loss: 0.1808 - mcrmse: 0.1930 - val_loss: 0.3873 - val_mcrmse: 0.2122\n",
            "Epoch 69/200\n",
            "60/60 - 7s - loss: 0.1794 - mcrmse: 0.1920 - val_loss: 0.3882 - val_mcrmse: 0.2125\n",
            "Epoch 70/200\n",
            "60/60 - 7s - loss: 0.1794 - mcrmse: 0.1918 - val_loss: 0.3921 - val_mcrmse: 0.2135\n",
            "Epoch 71/200\n",
            "60/60 - 7s - loss: 0.1777 - mcrmse: 0.1907 - val_loss: 0.3922 - val_mcrmse: 0.2139\n",
            "Epoch 72/200\n",
            "60/60 - 7s - loss: 0.1771 - mcrmse: 0.1900 - val_loss: 0.3904 - val_mcrmse: 0.2129\n",
            "Epoch 73/200\n",
            "60/60 - 7s - loss: 0.1763 - mcrmse: 0.1893 - val_loss: 0.3898 - val_mcrmse: 0.2124\n",
            "Epoch 74/200\n",
            "60/60 - 7s - loss: 0.1757 - mcrmse: 0.1886 - val_loss: 0.3919 - val_mcrmse: 0.2143\n",
            "Epoch 75/200\n",
            "60/60 - 7s - loss: 0.1742 - mcrmse: 0.1878 - val_loss: 0.3874 - val_mcrmse: 0.2114\n",
            "Epoch 76/200\n",
            "60/60 - 7s - loss: 0.1733 - mcrmse: 0.1868 - val_loss: 0.3894 - val_mcrmse: 0.2123\n",
            "Epoch 77/200\n",
            "60/60 - 7s - loss: 0.1724 - mcrmse: 0.1860 - val_loss: 0.3898 - val_mcrmse: 0.2126\n",
            "Epoch 78/200\n",
            "60/60 - 7s - loss: 0.1721 - mcrmse: 0.1858 - val_loss: 0.3887 - val_mcrmse: 0.2120\n",
            "Epoch 79/200\n",
            "60/60 - 7s - loss: 0.1709 - mcrmse: 0.1847 - val_loss: 0.3899 - val_mcrmse: 0.2127\n",
            "Epoch 80/200\n",
            "60/60 - 7s - loss: 0.1704 - mcrmse: 0.1846 - val_loss: 0.3901 - val_mcrmse: 0.2133\n",
            "Epoch 81/200\n",
            "60/60 - 7s - loss: 0.1692 - mcrmse: 0.1834 - val_loss: 0.3886 - val_mcrmse: 0.2123\n",
            "Epoch 82/200\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 7s - loss: 0.1680 - mcrmse: 0.1825 - val_loss: 0.3904 - val_mcrmse: 0.2130\n",
            "Epoch 00082: early stopping\n",
            "##### Type 3 Fold 0 #####\n",
            "Fold 0 validation loss=0.3864365518093109\n",
            "tf.Tensor(0.210755, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 3 Fold 1 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.255600400000066 -0.17248410000000003\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.6731 - mcrmse: 0.5287 - val_loss: 0.6109 - val_mcrmse: 0.3408\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4658 - mcrmse: 0.3906 - val_loss: 0.5244 - val_mcrmse: 0.2902\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4183 - mcrmse: 0.3585 - val_loss: 0.4967 - val_mcrmse: 0.2761\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3873 - mcrmse: 0.3388 - val_loss: 0.4660 - val_mcrmse: 0.2591\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3649 - mcrmse: 0.3248 - val_loss: 0.4567 - val_mcrmse: 0.2527\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3503 - mcrmse: 0.3161 - val_loss: 0.4401 - val_mcrmse: 0.2432\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3359 - mcrmse: 0.3070 - val_loss: 0.4280 - val_mcrmse: 0.2385\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3283 - mcrmse: 0.3022 - val_loss: 0.4195 - val_mcrmse: 0.2314\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3158 - mcrmse: 0.2946 - val_loss: 0.4203 - val_mcrmse: 0.2312\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3094 - mcrmse: 0.2909 - val_loss: 0.4136 - val_mcrmse: 0.2299\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3009 - mcrmse: 0.2854 - val_loss: 0.4128 - val_mcrmse: 0.2277\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.2956 - mcrmse: 0.2823 - val_loss: 0.4122 - val_mcrmse: 0.2266\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.2887 - mcrmse: 0.2777 - val_loss: 0.4080 - val_mcrmse: 0.2242\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.2827 - mcrmse: 0.2746 - val_loss: 0.4064 - val_mcrmse: 0.2239\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2783 - mcrmse: 0.2714 - val_loss: 0.4069 - val_mcrmse: 0.2232\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2732 - mcrmse: 0.2686 - val_loss: 0.4033 - val_mcrmse: 0.2216\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2691 - mcrmse: 0.2659 - val_loss: 0.4012 - val_mcrmse: 0.2196\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2647 - mcrmse: 0.2629 - val_loss: 0.3995 - val_mcrmse: 0.2188\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2604 - mcrmse: 0.2604 - val_loss: 0.3941 - val_mcrmse: 0.2174\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2568 - mcrmse: 0.2584 - val_loss: 0.3958 - val_mcrmse: 0.2167\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2536 - mcrmse: 0.2560 - val_loss: 0.3966 - val_mcrmse: 0.2177\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2497 - mcrmse: 0.2535 - val_loss: 0.3932 - val_mcrmse: 0.2162\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2474 - mcrmse: 0.2521 - val_loss: 0.3952 - val_mcrmse: 0.2163\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2433 - mcrmse: 0.2494 - val_loss: 0.3924 - val_mcrmse: 0.2157\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2412 - mcrmse: 0.2483 - val_loss: 0.3939 - val_mcrmse: 0.2158\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2384 - mcrmse: 0.2463 - val_loss: 0.3923 - val_mcrmse: 0.2151\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2347 - mcrmse: 0.2438 - val_loss: 0.3891 - val_mcrmse: 0.2131\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2317 - mcrmse: 0.2422 - val_loss: 0.3911 - val_mcrmse: 0.2141\n",
            "Epoch 29/200\n",
            "60/60 - 7s - loss: 0.2297 - mcrmse: 0.2408 - val_loss: 0.3872 - val_mcrmse: 0.2120\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2277 - mcrmse: 0.2393 - val_loss: 0.3879 - val_mcrmse: 0.2126\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2242 - mcrmse: 0.2370 - val_loss: 0.3873 - val_mcrmse: 0.2127\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2221 - mcrmse: 0.2356 - val_loss: 0.3873 - val_mcrmse: 0.2121\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2201 - mcrmse: 0.2344 - val_loss: 0.3882 - val_mcrmse: 0.2130\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2178 - mcrmse: 0.2327 - val_loss: 0.3882 - val_mcrmse: 0.2122\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2165 - mcrmse: 0.2320 - val_loss: 0.3879 - val_mcrmse: 0.2121\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2147 - mcrmse: 0.2309 - val_loss: 0.3847 - val_mcrmse: 0.2109\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2116 - mcrmse: 0.2288 - val_loss: 0.3884 - val_mcrmse: 0.2135\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2108 - mcrmse: 0.2281 - val_loss: 0.3869 - val_mcrmse: 0.2119\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2089 - mcrmse: 0.2267 - val_loss: 0.3896 - val_mcrmse: 0.2137\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2068 - mcrmse: 0.2253 - val_loss: 0.3869 - val_mcrmse: 0.2116\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2050 - mcrmse: 0.2240 - val_loss: 0.3851 - val_mcrmse: 0.2107\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2039 - mcrmse: 0.2234 - val_loss: 0.3874 - val_mcrmse: 0.2120\n",
            "Epoch 43/200\n",
            "60/60 - 7s - loss: 0.2018 - mcrmse: 0.2219 - val_loss: 0.3884 - val_mcrmse: 0.2126\n",
            "Epoch 44/200\n",
            "60/60 - 7s - loss: 0.2001 - mcrmse: 0.2208 - val_loss: 0.3874 - val_mcrmse: 0.2125\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.1981 - mcrmse: 0.2194 - val_loss: 0.3858 - val_mcrmse: 0.2115\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.1962 - mcrmse: 0.2180 - val_loss: 0.3864 - val_mcrmse: 0.2116\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.1952 - mcrmse: 0.2173 - val_loss: 0.3867 - val_mcrmse: 0.2117\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.1946 - mcrmse: 0.2169 - val_loss: 0.3884 - val_mcrmse: 0.2125\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.1941 - mcrmse: 0.2164 - val_loss: 0.3862 - val_mcrmse: 0.2113\n",
            "Epoch 50/200\n",
            "60/60 - 7s - loss: 0.1922 - mcrmse: 0.2152 - val_loss: 0.3857 - val_mcrmse: 0.2109\n",
            "Epoch 51/200\n",
            "60/60 - 7s - loss: 0.1901 - mcrmse: 0.2136 - val_loss: 0.3870 - val_mcrmse: 0.2116\n",
            "Epoch 52/200\n",
            "60/60 - 7s - loss: 0.1888 - mcrmse: 0.2126 - val_loss: 0.3871 - val_mcrmse: 0.2119\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.1880 - mcrmse: 0.2120 - val_loss: 0.3870 - val_mcrmse: 0.2116\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.1860 - mcrmse: 0.2106 - val_loss: 0.3874 - val_mcrmse: 0.2118\n",
            "Epoch 55/200\n",
            "60/60 - 7s - loss: 0.1851 - mcrmse: 0.2099 - val_loss: 0.3852 - val_mcrmse: 0.2110\n",
            "Epoch 56/200\n",
            "60/60 - 7s - loss: 0.1840 - mcrmse: 0.2092 - val_loss: 0.3875 - val_mcrmse: 0.2125\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.1824 - mcrmse: 0.2079 - val_loss: 0.3889 - val_mcrmse: 0.2135\n",
            "Epoch 58/200\n",
            "60/60 - 7s - loss: 0.1816 - mcrmse: 0.2074 - val_loss: 0.3884 - val_mcrmse: 0.2130\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.1802 - mcrmse: 0.2063 - val_loss: 0.3869 - val_mcrmse: 0.2120\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.1792 - mcrmse: 0.2057 - val_loss: 0.3865 - val_mcrmse: 0.2118\n",
            "Epoch 61/200\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 7s - loss: 0.1782 - mcrmse: 0.2048 - val_loss: 0.3873 - val_mcrmse: 0.2123\n",
            "Epoch 00061: early stopping\n",
            "##### Type 3 Fold 1 #####\n",
            "Fold 1 validation loss=0.38472896814346313\n",
            "tf.Tensor(0.21065739, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 3 Fold 2 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2932 -0.1845\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.6426 - mcrmse: 0.5196 - val_loss: 0.5832 - val_mcrmse: 0.3263\n",
            "Epoch 2/200\n",
            "60/60 - 8s - loss: 0.4529 - mcrmse: 0.3859 - val_loss: 0.5345 - val_mcrmse: 0.2945\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4076 - mcrmse: 0.3571 - val_loss: 0.5022 - val_mcrmse: 0.2777\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3790 - mcrmse: 0.3392 - val_loss: 0.4763 - val_mcrmse: 0.2627\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3595 - mcrmse: 0.3273 - val_loss: 0.4577 - val_mcrmse: 0.2523\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3459 - mcrmse: 0.3190 - val_loss: 0.4546 - val_mcrmse: 0.2472\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3348 - mcrmse: 0.3120 - val_loss: 0.4418 - val_mcrmse: 0.2401\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3215 - mcrmse: 0.3041 - val_loss: 0.4432 - val_mcrmse: 0.2390\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3170 - mcrmse: 0.3012 - val_loss: 0.4247 - val_mcrmse: 0.2314\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3061 - mcrmse: 0.2946 - val_loss: 0.4207 - val_mcrmse: 0.2283\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.2994 - mcrmse: 0.2905 - val_loss: 0.4253 - val_mcrmse: 0.2312\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.2937 - mcrmse: 0.2872 - val_loss: 0.4264 - val_mcrmse: 0.2326\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.2871 - mcrmse: 0.2834 - val_loss: 0.4241 - val_mcrmse: 0.2316\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.2821 - mcrmse: 0.2801 - val_loss: 0.4176 - val_mcrmse: 0.2265\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2773 - mcrmse: 0.2771 - val_loss: 0.4189 - val_mcrmse: 0.2270\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2725 - mcrmse: 0.2745 - val_loss: 0.4204 - val_mcrmse: 0.2270\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2680 - mcrmse: 0.2716 - val_loss: 0.4179 - val_mcrmse: 0.2263\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2632 - mcrmse: 0.2685 - val_loss: 0.4147 - val_mcrmse: 0.2247\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2588 - mcrmse: 0.2657 - val_loss: 0.4139 - val_mcrmse: 0.2229\n",
            "Epoch 20/200\n",
            "60/60 - 8s - loss: 0.2568 - mcrmse: 0.2645 - val_loss: 0.4097 - val_mcrmse: 0.2209\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2522 - mcrmse: 0.2618 - val_loss: 0.4138 - val_mcrmse: 0.2232\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2492 - mcrmse: 0.2598 - val_loss: 0.4180 - val_mcrmse: 0.2270\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2479 - mcrmse: 0.2588 - val_loss: 0.4090 - val_mcrmse: 0.2216\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2432 - mcrmse: 0.2558 - val_loss: 0.4080 - val_mcrmse: 0.2207\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2405 - mcrmse: 0.2541 - val_loss: 0.4128 - val_mcrmse: 0.2249\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2368 - mcrmse: 0.2519 - val_loss: 0.4069 - val_mcrmse: 0.2213\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2343 - mcrmse: 0.2501 - val_loss: 0.4120 - val_mcrmse: 0.2228\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2324 - mcrmse: 0.2490 - val_loss: 0.4098 - val_mcrmse: 0.2222\n",
            "Epoch 29/200\n",
            "60/60 - 7s - loss: 0.2294 - mcrmse: 0.2472 - val_loss: 0.4087 - val_mcrmse: 0.2207\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2278 - mcrmse: 0.2460 - val_loss: 0.4077 - val_mcrmse: 0.2210\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2256 - mcrmse: 0.2443 - val_loss: 0.4073 - val_mcrmse: 0.2210\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2227 - mcrmse: 0.2427 - val_loss: 0.4066 - val_mcrmse: 0.2202\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2205 - mcrmse: 0.2412 - val_loss: 0.4089 - val_mcrmse: 0.2223\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2181 - mcrmse: 0.2396 - val_loss: 0.4102 - val_mcrmse: 0.2227\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2167 - mcrmse: 0.2386 - val_loss: 0.4086 - val_mcrmse: 0.2214\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2147 - mcrmse: 0.2374 - val_loss: 0.4075 - val_mcrmse: 0.2209\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2128 - mcrmse: 0.2361 - val_loss: 0.4057 - val_mcrmse: 0.2198\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2108 - mcrmse: 0.2346 - val_loss: 0.4052 - val_mcrmse: 0.2203\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2094 - mcrmse: 0.2333 - val_loss: 0.4053 - val_mcrmse: 0.2197\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2080 - mcrmse: 0.2325 - val_loss: 0.4054 - val_mcrmse: 0.2194\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2058 - mcrmse: 0.2308 - val_loss: 0.4088 - val_mcrmse: 0.2217\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2040 - mcrmse: 0.2296 - val_loss: 0.4040 - val_mcrmse: 0.2193\n",
            "Epoch 43/200\n",
            "60/60 - 7s - loss: 0.2026 - mcrmse: 0.2286 - val_loss: 0.4034 - val_mcrmse: 0.2189\n",
            "Epoch 44/200\n",
            "60/60 - 7s - loss: 0.2012 - mcrmse: 0.2275 - val_loss: 0.4054 - val_mcrmse: 0.2206\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.1996 - mcrmse: 0.2264 - val_loss: 0.4050 - val_mcrmse: 0.2197\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.1987 - mcrmse: 0.2257 - val_loss: 0.4072 - val_mcrmse: 0.2210\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.1971 - mcrmse: 0.2247 - val_loss: 0.4071 - val_mcrmse: 0.2211\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.1963 - mcrmse: 0.2242 - val_loss: 0.4067 - val_mcrmse: 0.2202\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.1942 - mcrmse: 0.2225 - val_loss: 0.4074 - val_mcrmse: 0.2214\n",
            "Epoch 50/200\n",
            "60/60 - 7s - loss: 0.1928 - mcrmse: 0.2216 - val_loss: 0.4024 - val_mcrmse: 0.2187\n",
            "Epoch 51/200\n",
            "60/60 - 7s - loss: 0.1917 - mcrmse: 0.2206 - val_loss: 0.4049 - val_mcrmse: 0.2200\n",
            "Epoch 52/200\n",
            "60/60 - 7s - loss: 0.1903 - mcrmse: 0.2197 - val_loss: 0.4013 - val_mcrmse: 0.2182\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.1889 - mcrmse: 0.2186 - val_loss: 0.3991 - val_mcrmse: 0.2165\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.1884 - mcrmse: 0.2180 - val_loss: 0.4031 - val_mcrmse: 0.2194\n",
            "Epoch 55/200\n",
            "60/60 - 7s - loss: 0.1869 - mcrmse: 0.2172 - val_loss: 0.4050 - val_mcrmse: 0.2195\n",
            "Epoch 56/200\n",
            "60/60 - 7s - loss: 0.1856 - mcrmse: 0.2159 - val_loss: 0.4050 - val_mcrmse: 0.2202\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.1842 - mcrmse: 0.2151 - val_loss: 0.4039 - val_mcrmse: 0.2192\n",
            "Epoch 58/200\n",
            "60/60 - 7s - loss: 0.1834 - mcrmse: 0.2143 - val_loss: 0.4068 - val_mcrmse: 0.2205\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.1822 - mcrmse: 0.2133 - val_loss: 0.4064 - val_mcrmse: 0.2208\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.1814 - mcrmse: 0.2129 - val_loss: 0.4004 - val_mcrmse: 0.2176\n",
            "Epoch 61/200\n",
            "60/60 - 7s - loss: 0.1800 - mcrmse: 0.2117 - val_loss: 0.4063 - val_mcrmse: 0.2207\n",
            "Epoch 62/200\n",
            "60/60 - 7s - loss: 0.1788 - mcrmse: 0.2108 - val_loss: 0.4055 - val_mcrmse: 0.2203\n",
            "Epoch 63/200\n",
            "60/60 - 7s - loss: 0.1780 - mcrmse: 0.2103 - val_loss: 0.4050 - val_mcrmse: 0.2200\n",
            "Epoch 64/200\n",
            "60/60 - 7s - loss: 0.1765 - mcrmse: 0.2091 - val_loss: 0.4049 - val_mcrmse: 0.2193\n",
            "Epoch 65/200\n",
            "60/60 - 7s - loss: 0.1763 - mcrmse: 0.2088 - val_loss: 0.4024 - val_mcrmse: 0.2187\n",
            "Epoch 66/200\n",
            "60/60 - 7s - loss: 0.1748 - mcrmse: 0.2077 - val_loss: 0.4057 - val_mcrmse: 0.2200\n",
            "Epoch 67/200\n",
            "60/60 - 7s - loss: 0.1745 - mcrmse: 0.2075 - val_loss: 0.4054 - val_mcrmse: 0.2199\n",
            "Epoch 68/200\n",
            "60/60 - 7s - loss: 0.1740 - mcrmse: 0.2069 - val_loss: 0.4061 - val_mcrmse: 0.2211\n",
            "Epoch 69/200\n",
            "60/60 - 7s - loss: 0.1722 - mcrmse: 0.2058 - val_loss: 0.4024 - val_mcrmse: 0.2188\n",
            "Epoch 70/200\n",
            "60/60 - 7s - loss: 0.1711 - mcrmse: 0.2048 - val_loss: 0.4048 - val_mcrmse: 0.2198\n",
            "Epoch 71/200\n",
            "60/60 - 7s - loss: 0.1703 - mcrmse: 0.2041 - val_loss: 0.4053 - val_mcrmse: 0.2204\n",
            "Epoch 72/200\n",
            "60/60 - 7s - loss: 0.1696 - mcrmse: 0.2036 - val_loss: 0.4043 - val_mcrmse: 0.2198\n",
            "Epoch 73/200\n",
            "\n",
            "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 7s - loss: 0.1692 - mcrmse: 0.2032 - val_loss: 0.4063 - val_mcrmse: 0.2213\n",
            "Epoch 00073: early stopping\n",
            "##### Type 3 Fold 2 #####\n",
            "Fold 2 validation loss=0.39914655685424805\n",
            "tf.Tensor(0.21650364, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 3 Fold 3 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.155131100000162 -0.18742859999999997\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.6766 - mcrmse: 0.5281 - val_loss: 0.6098 - val_mcrmse: 0.3480\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4697 - mcrmse: 0.3849 - val_loss: 0.5314 - val_mcrmse: 0.2923\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4205 - mcrmse: 0.3519 - val_loss: 0.4997 - val_mcrmse: 0.2749\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3911 - mcrmse: 0.3331 - val_loss: 0.4606 - val_mcrmse: 0.2533\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3709 - mcrmse: 0.3202 - val_loss: 0.4438 - val_mcrmse: 0.2437\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3545 - mcrmse: 0.3101 - val_loss: 0.4487 - val_mcrmse: 0.2477\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3410 - mcrmse: 0.3019 - val_loss: 0.4194 - val_mcrmse: 0.2302\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3312 - mcrmse: 0.2955 - val_loss: 0.4164 - val_mcrmse: 0.2292\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3219 - mcrmse: 0.2897 - val_loss: 0.4140 - val_mcrmse: 0.2277\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3136 - mcrmse: 0.2845 - val_loss: 0.4051 - val_mcrmse: 0.2219\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3063 - mcrmse: 0.2799 - val_loss: 0.4066 - val_mcrmse: 0.2236\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.3007 - mcrmse: 0.2764 - val_loss: 0.4062 - val_mcrmse: 0.2221\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.2935 - mcrmse: 0.2719 - val_loss: 0.4005 - val_mcrmse: 0.2195\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.2891 - mcrmse: 0.2691 - val_loss: 0.4052 - val_mcrmse: 0.2225\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2837 - mcrmse: 0.2658 - val_loss: 0.4004 - val_mcrmse: 0.2188\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2798 - mcrmse: 0.2633 - val_loss: 0.3990 - val_mcrmse: 0.2183\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2747 - mcrmse: 0.2601 - val_loss: 0.3896 - val_mcrmse: 0.2133\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2709 - mcrmse: 0.2575 - val_loss: 0.3960 - val_mcrmse: 0.2169\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2664 - mcrmse: 0.2548 - val_loss: 0.3954 - val_mcrmse: 0.2161\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2625 - mcrmse: 0.2522 - val_loss: 0.3920 - val_mcrmse: 0.2147\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2583 - mcrmse: 0.2496 - val_loss: 0.3900 - val_mcrmse: 0.2138\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2557 - mcrmse: 0.2479 - val_loss: 0.3915 - val_mcrmse: 0.2145\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2532 - mcrmse: 0.2464 - val_loss: 0.3925 - val_mcrmse: 0.2152\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2482 - mcrmse: 0.2433 - val_loss: 0.3921 - val_mcrmse: 0.2141\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2454 - mcrmse: 0.2415 - val_loss: 0.3944 - val_mcrmse: 0.2156\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2428 - mcrmse: 0.2396 - val_loss: 0.3928 - val_mcrmse: 0.2144\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2398 - mcrmse: 0.2380 - val_loss: 0.3892 - val_mcrmse: 0.2128\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2376 - mcrmse: 0.2362 - val_loss: 0.3890 - val_mcrmse: 0.2131\n",
            "Epoch 29/200\n",
            "60/60 - 7s - loss: 0.2358 - mcrmse: 0.2352 - val_loss: 0.3871 - val_mcrmse: 0.2119\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2327 - mcrmse: 0.2329 - val_loss: 0.3879 - val_mcrmse: 0.2124\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2306 - mcrmse: 0.2316 - val_loss: 0.3897 - val_mcrmse: 0.2128\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2279 - mcrmse: 0.2298 - val_loss: 0.3874 - val_mcrmse: 0.2119\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2264 - mcrmse: 0.2285 - val_loss: 0.3875 - val_mcrmse: 0.2115\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2233 - mcrmse: 0.2266 - val_loss: 0.3887 - val_mcrmse: 0.2122\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2230 - mcrmse: 0.2262 - val_loss: 0.3854 - val_mcrmse: 0.2105\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2202 - mcrmse: 0.2244 - val_loss: 0.3833 - val_mcrmse: 0.2092\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2176 - mcrmse: 0.2227 - val_loss: 0.3874 - val_mcrmse: 0.2117\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2159 - mcrmse: 0.2216 - val_loss: 0.3854 - val_mcrmse: 0.2111\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2147 - mcrmse: 0.2205 - val_loss: 0.3838 - val_mcrmse: 0.2096\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2122 - mcrmse: 0.2190 - val_loss: 0.3884 - val_mcrmse: 0.2119\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2106 - mcrmse: 0.2177 - val_loss: 0.3851 - val_mcrmse: 0.2106\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2099 - mcrmse: 0.2178 - val_loss: 0.3895 - val_mcrmse: 0.2127\n",
            "Epoch 43/200\n",
            "60/60 - 7s - loss: 0.2070 - mcrmse: 0.2154 - val_loss: 0.3848 - val_mcrmse: 0.2104\n",
            "Epoch 44/200\n",
            "60/60 - 7s - loss: 0.2060 - mcrmse: 0.2146 - val_loss: 0.3858 - val_mcrmse: 0.2108\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.2040 - mcrmse: 0.2132 - val_loss: 0.3914 - val_mcrmse: 0.2139\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.2025 - mcrmse: 0.2121 - val_loss: 0.3847 - val_mcrmse: 0.2103\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.2011 - mcrmse: 0.2110 - val_loss: 0.3865 - val_mcrmse: 0.2115\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.1992 - mcrmse: 0.2100 - val_loss: 0.3879 - val_mcrmse: 0.2118\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.1983 - mcrmse: 0.2091 - val_loss: 0.3848 - val_mcrmse: 0.2102\n",
            "Epoch 50/200\n",
            "60/60 - 7s - loss: 0.1976 - mcrmse: 0.2082 - val_loss: 0.3852 - val_mcrmse: 0.2107\n",
            "Epoch 51/200\n",
            "60/60 - 7s - loss: 0.1958 - mcrmse: 0.2074 - val_loss: 0.3888 - val_mcrmse: 0.2119\n",
            "Epoch 52/200\n",
            "60/60 - 7s - loss: 0.1941 - mcrmse: 0.2061 - val_loss: 0.3867 - val_mcrmse: 0.2111\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.1939 - mcrmse: 0.2057 - val_loss: 0.3873 - val_mcrmse: 0.2117\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.1916 - mcrmse: 0.2042 - val_loss: 0.3840 - val_mcrmse: 0.2099\n",
            "Epoch 55/200\n",
            "60/60 - 7s - loss: 0.1899 - mcrmse: 0.2032 - val_loss: 0.3892 - val_mcrmse: 0.2126\n",
            "Epoch 56/200\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 7s - loss: 0.1892 - mcrmse: 0.2024 - val_loss: 0.3871 - val_mcrmse: 0.2119\n",
            "Epoch 00056: early stopping\n",
            "##### Type 3 Fold 3 #####\n",
            "Fold 3 validation loss=0.38325953483581543\n",
            "tf.Tensor(0.20924507, shape=(), dtype=float32)\n",
            "\n",
            "##### Type 3 Fold 4 #####\n",
            "trn.SN_filter == 1 [-0.3851 -0.2853 -0.4997 -0.4982 -0.4965] [ 6.7902  8.1738 10.487   7.2046  5.0096]\n",
            "4.2344 -0.186\n",
            "Epoch 1/200\n",
            "60/60 - 10s - loss: 0.6566 - mcrmse: 0.5228 - val_loss: 0.5856 - val_mcrmse: 0.3262\n",
            "Epoch 2/200\n",
            "60/60 - 7s - loss: 0.4605 - mcrmse: 0.3877 - val_loss: 0.5178 - val_mcrmse: 0.2814\n",
            "Epoch 3/200\n",
            "60/60 - 7s - loss: 0.4161 - mcrmse: 0.3584 - val_loss: 0.4727 - val_mcrmse: 0.2583\n",
            "Epoch 4/200\n",
            "60/60 - 7s - loss: 0.3872 - mcrmse: 0.3409 - val_loss: 0.4517 - val_mcrmse: 0.2464\n",
            "Epoch 5/200\n",
            "60/60 - 7s - loss: 0.3670 - mcrmse: 0.3279 - val_loss: 0.4423 - val_mcrmse: 0.2393\n",
            "Epoch 6/200\n",
            "60/60 - 7s - loss: 0.3514 - mcrmse: 0.3184 - val_loss: 0.4314 - val_mcrmse: 0.2334\n",
            "Epoch 7/200\n",
            "60/60 - 7s - loss: 0.3359 - mcrmse: 0.3087 - val_loss: 0.4188 - val_mcrmse: 0.2267\n",
            "Epoch 8/200\n",
            "60/60 - 7s - loss: 0.3264 - mcrmse: 0.3028 - val_loss: 0.4155 - val_mcrmse: 0.2247\n",
            "Epoch 9/200\n",
            "60/60 - 7s - loss: 0.3156 - mcrmse: 0.2961 - val_loss: 0.4079 - val_mcrmse: 0.2202\n",
            "Epoch 10/200\n",
            "60/60 - 7s - loss: 0.3086 - mcrmse: 0.2920 - val_loss: 0.4088 - val_mcrmse: 0.2212\n",
            "Epoch 11/200\n",
            "60/60 - 7s - loss: 0.3003 - mcrmse: 0.2866 - val_loss: 0.4062 - val_mcrmse: 0.2204\n",
            "Epoch 12/200\n",
            "60/60 - 7s - loss: 0.2964 - mcrmse: 0.2841 - val_loss: 0.4059 - val_mcrmse: 0.2186\n",
            "Epoch 13/200\n",
            "60/60 - 7s - loss: 0.2885 - mcrmse: 0.2794 - val_loss: 0.3994 - val_mcrmse: 0.2155\n",
            "Epoch 14/200\n",
            "60/60 - 7s - loss: 0.2808 - mcrmse: 0.2746 - val_loss: 0.3937 - val_mcrmse: 0.2123\n",
            "Epoch 15/200\n",
            "60/60 - 7s - loss: 0.2771 - mcrmse: 0.2725 - val_loss: 0.3969 - val_mcrmse: 0.2142\n",
            "Epoch 16/200\n",
            "60/60 - 7s - loss: 0.2720 - mcrmse: 0.2694 - val_loss: 0.3933 - val_mcrmse: 0.2124\n",
            "Epoch 17/200\n",
            "60/60 - 7s - loss: 0.2687 - mcrmse: 0.2671 - val_loss: 0.3976 - val_mcrmse: 0.2156\n",
            "Epoch 18/200\n",
            "60/60 - 7s - loss: 0.2639 - mcrmse: 0.2642 - val_loss: 0.3954 - val_mcrmse: 0.2127\n",
            "Epoch 19/200\n",
            "60/60 - 7s - loss: 0.2604 - mcrmse: 0.2617 - val_loss: 0.3892 - val_mcrmse: 0.2095\n",
            "Epoch 20/200\n",
            "60/60 - 7s - loss: 0.2561 - mcrmse: 0.2591 - val_loss: 0.3879 - val_mcrmse: 0.2090\n",
            "Epoch 21/200\n",
            "60/60 - 7s - loss: 0.2524 - mcrmse: 0.2568 - val_loss: 0.3870 - val_mcrmse: 0.2089\n",
            "Epoch 22/200\n",
            "60/60 - 7s - loss: 0.2478 - mcrmse: 0.2537 - val_loss: 0.3870 - val_mcrmse: 0.2089\n",
            "Epoch 23/200\n",
            "60/60 - 7s - loss: 0.2458 - mcrmse: 0.2524 - val_loss: 0.3885 - val_mcrmse: 0.2101\n",
            "Epoch 24/200\n",
            "60/60 - 7s - loss: 0.2425 - mcrmse: 0.2503 - val_loss: 0.3844 - val_mcrmse: 0.2069\n",
            "Epoch 25/200\n",
            "60/60 - 7s - loss: 0.2406 - mcrmse: 0.2494 - val_loss: 0.3889 - val_mcrmse: 0.2101\n",
            "Epoch 26/200\n",
            "60/60 - 7s - loss: 0.2385 - mcrmse: 0.2478 - val_loss: 0.3894 - val_mcrmse: 0.2103\n",
            "Epoch 27/200\n",
            "60/60 - 7s - loss: 0.2337 - mcrmse: 0.2446 - val_loss: 0.3846 - val_mcrmse: 0.2071\n",
            "Epoch 28/200\n",
            "60/60 - 7s - loss: 0.2312 - mcrmse: 0.2429 - val_loss: 0.3897 - val_mcrmse: 0.2098\n",
            "Epoch 29/200\n",
            "60/60 - 7s - loss: 0.2292 - mcrmse: 0.2416 - val_loss: 0.3843 - val_mcrmse: 0.2069\n",
            "Epoch 30/200\n",
            "60/60 - 7s - loss: 0.2257 - mcrmse: 0.2393 - val_loss: 0.3832 - val_mcrmse: 0.2065\n",
            "Epoch 31/200\n",
            "60/60 - 7s - loss: 0.2241 - mcrmse: 0.2382 - val_loss: 0.3833 - val_mcrmse: 0.2062\n",
            "Epoch 32/200\n",
            "60/60 - 7s - loss: 0.2223 - mcrmse: 0.2370 - val_loss: 0.3831 - val_mcrmse: 0.2070\n",
            "Epoch 33/200\n",
            "60/60 - 7s - loss: 0.2204 - mcrmse: 0.2354 - val_loss: 0.3848 - val_mcrmse: 0.2078\n",
            "Epoch 34/200\n",
            "60/60 - 7s - loss: 0.2176 - mcrmse: 0.2335 - val_loss: 0.3860 - val_mcrmse: 0.2074\n",
            "Epoch 35/200\n",
            "60/60 - 7s - loss: 0.2156 - mcrmse: 0.2323 - val_loss: 0.3811 - val_mcrmse: 0.2055\n",
            "Epoch 36/200\n",
            "60/60 - 7s - loss: 0.2131 - mcrmse: 0.2308 - val_loss: 0.3843 - val_mcrmse: 0.2075\n",
            "Epoch 37/200\n",
            "60/60 - 7s - loss: 0.2121 - mcrmse: 0.2299 - val_loss: 0.3837 - val_mcrmse: 0.2069\n",
            "Epoch 38/200\n",
            "60/60 - 7s - loss: 0.2095 - mcrmse: 0.2280 - val_loss: 0.3829 - val_mcrmse: 0.2062\n",
            "Epoch 39/200\n",
            "60/60 - 7s - loss: 0.2080 - mcrmse: 0.2270 - val_loss: 0.3875 - val_mcrmse: 0.2087\n",
            "Epoch 40/200\n",
            "60/60 - 7s - loss: 0.2061 - mcrmse: 0.2257 - val_loss: 0.3851 - val_mcrmse: 0.2076\n",
            "Epoch 41/200\n",
            "60/60 - 7s - loss: 0.2044 - mcrmse: 0.2244 - val_loss: 0.3863 - val_mcrmse: 0.2085\n",
            "Epoch 42/200\n",
            "60/60 - 7s - loss: 0.2036 - mcrmse: 0.2237 - val_loss: 0.3871 - val_mcrmse: 0.2091\n",
            "Epoch 43/200\n",
            "60/60 - 7s - loss: 0.2022 - mcrmse: 0.2226 - val_loss: 0.3799 - val_mcrmse: 0.2047\n",
            "Epoch 44/200\n",
            "60/60 - 7s - loss: 0.1994 - mcrmse: 0.2209 - val_loss: 0.3828 - val_mcrmse: 0.2067\n",
            "Epoch 45/200\n",
            "60/60 - 7s - loss: 0.1980 - mcrmse: 0.2198 - val_loss: 0.3868 - val_mcrmse: 0.2090\n",
            "Epoch 46/200\n",
            "60/60 - 7s - loss: 0.1963 - mcrmse: 0.2185 - val_loss: 0.3814 - val_mcrmse: 0.2059\n",
            "Epoch 47/200\n",
            "60/60 - 7s - loss: 0.1947 - mcrmse: 0.2170 - val_loss: 0.3833 - val_mcrmse: 0.2068\n",
            "Epoch 48/200\n",
            "60/60 - 7s - loss: 0.1939 - mcrmse: 0.2166 - val_loss: 0.3819 - val_mcrmse: 0.2056\n",
            "Epoch 49/200\n",
            "60/60 - 7s - loss: 0.1921 - mcrmse: 0.2153 - val_loss: 0.3825 - val_mcrmse: 0.2061\n",
            "Epoch 50/200\n",
            "60/60 - 7s - loss: 0.1915 - mcrmse: 0.2150 - val_loss: 0.3802 - val_mcrmse: 0.2047\n",
            "Epoch 51/200\n",
            "60/60 - 7s - loss: 0.1898 - mcrmse: 0.2136 - val_loss: 0.3802 - val_mcrmse: 0.2052\n",
            "Epoch 52/200\n",
            "60/60 - 7s - loss: 0.1882 - mcrmse: 0.2124 - val_loss: 0.3800 - val_mcrmse: 0.2048\n",
            "Epoch 53/200\n",
            "60/60 - 7s - loss: 0.1870 - mcrmse: 0.2114 - val_loss: 0.3811 - val_mcrmse: 0.2058\n",
            "Epoch 54/200\n",
            "60/60 - 7s - loss: 0.1861 - mcrmse: 0.2107 - val_loss: 0.3825 - val_mcrmse: 0.2062\n",
            "Epoch 55/200\n",
            "60/60 - 7s - loss: 0.1851 - mcrmse: 0.2103 - val_loss: 0.3800 - val_mcrmse: 0.2055\n",
            "Epoch 56/200\n",
            "60/60 - 7s - loss: 0.1834 - mcrmse: 0.2088 - val_loss: 0.3861 - val_mcrmse: 0.2089\n",
            "Epoch 57/200\n",
            "60/60 - 7s - loss: 0.1826 - mcrmse: 0.2080 - val_loss: 0.3802 - val_mcrmse: 0.2051\n",
            "Epoch 58/200\n",
            "60/60 - 7s - loss: 0.1811 - mcrmse: 0.2072 - val_loss: 0.3813 - val_mcrmse: 0.2060\n",
            "Epoch 59/200\n",
            "60/60 - 7s - loss: 0.1804 - mcrmse: 0.2062 - val_loss: 0.3864 - val_mcrmse: 0.2086\n",
            "Epoch 60/200\n",
            "60/60 - 7s - loss: 0.1795 - mcrmse: 0.2059 - val_loss: 0.3836 - val_mcrmse: 0.2072\n",
            "Epoch 61/200\n",
            "60/60 - 7s - loss: 0.1779 - mcrmse: 0.2047 - val_loss: 0.3799 - val_mcrmse: 0.2051\n",
            "Epoch 62/200\n",
            "60/60 - 7s - loss: 0.1768 - mcrmse: 0.2036 - val_loss: 0.3815 - val_mcrmse: 0.2057\n",
            "Epoch 63/200\n",
            "\n",
            "Epoch 00063: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Restoring model weights from the end of the best epoch.\n",
            "60/60 - 7s - loss: 0.1758 - mcrmse: 0.2032 - val_loss: 0.3845 - val_mcrmse: 0.2079\n",
            "Epoch 00063: early stopping\n",
            "##### Type 3 Fold 4 #####\n",
            "Fold 4 validation loss=0.379853755235672\n",
            "tf.Tensor(0.20468551, shape=(), dtype=float32)\n",
            "wrote to submission.csv\n",
            "reactivity 0.60894770163963 0.37081730333218776\n",
            "deg_Mg_pH10 0.5519786968409955 0.3046804817662837\n",
            "deg_Mg_50C 0.7510876488963586 0.5641326563246596\n",
            "reactivity 0.19864401087657232 0.03945944305713179\n",
            "deg_Mg_pH10 0.24717935394291315 0.06109763301563594\n",
            "deg_Mg_50C 0.2018751405288552 0.04075357236354503\n",
            "0.6373380157923281 0.2158995017827802\n",
            "type 0\n",
            "reactivity 0.20576208858030048 0.042338037096927426\n",
            "deg_Mg_pH10 0.25518483589196633 0.0651193004692098\n",
            "deg_Mg_50C 0.20890224484561423 0.04364014790153696\n",
            "(0.2232830564392937, 0.05036582848922474)\n",
            "type 1\n",
            "reactivity 0.20456598994746 0.041847244243184305\n",
            "deg_Mg_pH10 0.2545047156689399 0.06477265029772794\n",
            "deg_Mg_50C 0.2095974212065526 0.04393107897643702\n",
            "(0.22288937560765085, 0.05018365783911643)\n",
            "type 2\n",
            "reactivity 0.20904460950068432 0.04369964876129359\n",
            "deg_Mg_pH10 0.26016979985583716 0.06768832475702637\n",
            "deg_Mg_50C 0.2153522298947563 0.04637658292064397\n",
            "(0.22818887975042593, 0.05258818547965464)\n",
            "type 3\n",
            "reactivity 0.21034998941328892 0.04424711804617076\n",
            "deg_Mg_pH10 0.25674517968102756 0.06591808728944314\n",
            "deg_Mg_50C 0.21084841308754537 0.044457053301536174\n",
            "(0.2259811940606206, 0.05154075287905002)\n",
            "reactivity 0.6088481458983486 0.3706960647638568\n",
            "deg_Mg_pH10 0.5518492410386604 0.3045375848349455\n",
            "deg_Mg_50C 0.7504197277365042 0.5631297677761291\n",
            "0.6370390382245045\n",
            "reactivity 0.19839503941407644 0.039360591664112946\n",
            "deg_Mg_pH10 0.24709949466462208 0.0610581602635116\n",
            "deg_Mg_50C 0.2017227630376 0.04069207312752372\n",
            "0.2157390990387662\n",
            "          id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C\n",
            "0    id_00073f8be_0    0.736404     0.605843  1.932607    0.512766  0.766763\n",
            "1    id_00073f8be_1    2.321013     3.166493  4.425825    3.250620  2.924317\n",
            "2   id_00073f8be_10    0.285719     0.430095  0.392201    0.309354  0.598485\n",
            "3  id_00073f8be_100    0.238900     0.409432  0.813625    0.511039  0.640120\n",
            "4  id_00073f8be_101    0.420902     0.197097  0.361101    0.246006  0.466889\n",
            "outputs_oof_0.21574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/OpenVaccine/outputs/outputs_oof_0.21574'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBgSo-70pmTU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}